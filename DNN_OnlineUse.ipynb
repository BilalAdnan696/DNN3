{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DNN-OnlineUse",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UgODPcnga61",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F96vowuzdT-e",
        "colab_type": "text"
      },
      "source": [
        "### Loading Libraries and Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okSC-YFN-hRv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7e5be972-86b6-425d-dfc8-30c46666be57"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "from numpy.random import seed\n",
        "seed(2)\n",
        "#from tensorflow import set_random_seed\n",
        "#set_random_seed(2)\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from IPython import display\n",
        "#from missingpy import MissForest\n",
        "from matplotlib import cm\n",
        "from matplotlib import gridspec\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from tensorflow.python.data import Dataset\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "print(tf.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Qxiclo8_Gbc",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 509
        },
        "outputId": "923c3027-9993-4f86-d492-973beaaf6b22"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded= files.upload()\n",
        "pd.options.display.max_rows = 10\n",
        "pd.options.display.float_format = '{:.4f}'.format\n",
        "\n",
        "Online_appoint = pd.read_excel(\"OnlineUse.xlsx\",'OnlineUseData')\n",
        "\n",
        "\n",
        "\n",
        "Online_appoint"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-fea5331c-faff-4f20-95e2-3e78b6de3608\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-fea5331c-faff-4f20-95e2-3e78b6de3608\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving OnlineUse.xlsx to OnlineUse.xlsx\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ClinicID</th>\n",
              "      <th>OnlineAppointmentUse</th>\n",
              "      <th>malepct</th>\n",
              "      <th>unemp</th>\n",
              "      <th>age16to24</th>\n",
              "      <th>age25to34</th>\n",
              "      <th>age35to44</th>\n",
              "      <th>age45to54</th>\n",
              "      <th>age55to64</th>\n",
              "      <th>age65to74</th>\n",
              "      <th>age75to84</th>\n",
              "      <th>age85plus</th>\n",
              "      <th>phoneeasy</th>\n",
              "      <th>onlineasy</th>\n",
              "      <th>race</th>\n",
              "      <th>longstdhealth</th>\n",
              "      <th>canmngownhealth</th>\n",
              "      <th>reducedability</th>\n",
              "      <th>prefgpalways</th>\n",
              "      <th>bcaaware</th>\n",
              "      <th>vendor</th>\n",
              "      <th>numpats</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0.0597</td>\n",
              "      <td>0.4785</td>\n",
              "      <td>0.1083</td>\n",
              "      <td>0.0587</td>\n",
              "      <td>0.1818</td>\n",
              "      <td>0.0992</td>\n",
              "      <td>0.1825</td>\n",
              "      <td>0.1430</td>\n",
              "      <td>0.1547</td>\n",
              "      <td>0.1205</td>\n",
              "      <td>0.0597</td>\n",
              "      <td>0.4655</td>\n",
              "      <td>0.7673</td>\n",
              "      <td>0.8247</td>\n",
              "      <td>0.5260</td>\n",
              "      <td>0.8208</td>\n",
              "      <td>0.6516</td>\n",
              "      <td>0.5347</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>TPP</td>\n",
              "      <td>4088</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0.1338</td>\n",
              "      <td>0.4745</td>\n",
              "      <td>0.1062</td>\n",
              "      <td>0.1468</td>\n",
              "      <td>0.1393</td>\n",
              "      <td>0.0729</td>\n",
              "      <td>0.2185</td>\n",
              "      <td>0.1657</td>\n",
              "      <td>0.1399</td>\n",
              "      <td>0.0717</td>\n",
              "      <td>0.0451</td>\n",
              "      <td>0.7172</td>\n",
              "      <td>0.9420</td>\n",
              "      <td>0.9565</td>\n",
              "      <td>0.5713</td>\n",
              "      <td>0.7960</td>\n",
              "      <td>0.6967</td>\n",
              "      <td>0.7239</td>\n",
              "      <td>0.2978</td>\n",
              "      <td>TPP</td>\n",
              "      <td>19599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>0.0610</td>\n",
              "      <td>0.4045</td>\n",
              "      <td>0.0364</td>\n",
              "      <td>0.0670</td>\n",
              "      <td>0.1258</td>\n",
              "      <td>0.1952</td>\n",
              "      <td>0.1705</td>\n",
              "      <td>0.1817</td>\n",
              "      <td>0.1409</td>\n",
              "      <td>0.0955</td>\n",
              "      <td>0.0233</td>\n",
              "      <td>0.8579</td>\n",
              "      <td>0.9768</td>\n",
              "      <td>0.9225</td>\n",
              "      <td>0.4262</td>\n",
              "      <td>0.9553</td>\n",
              "      <td>0.6520</td>\n",
              "      <td>0.5634</td>\n",
              "      <td>0.2313</td>\n",
              "      <td>TPP</td>\n",
              "      <td>10606</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>0.0990</td>\n",
              "      <td>0.3981</td>\n",
              "      <td>0.0210</td>\n",
              "      <td>0.1016</td>\n",
              "      <td>0.0740</td>\n",
              "      <td>0.2052</td>\n",
              "      <td>0.1438</td>\n",
              "      <td>0.1325</td>\n",
              "      <td>0.2006</td>\n",
              "      <td>0.0982</td>\n",
              "      <td>0.0440</td>\n",
              "      <td>0.6888</td>\n",
              "      <td>0.9419</td>\n",
              "      <td>0.9756</td>\n",
              "      <td>0.4972</td>\n",
              "      <td>0.9128</td>\n",
              "      <td>0.5157</td>\n",
              "      <td>0.5682</td>\n",
              "      <td>0.5476</td>\n",
              "      <td>TPP</td>\n",
              "      <td>8047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0.0998</td>\n",
              "      <td>0.4675</td>\n",
              "      <td>0.0345</td>\n",
              "      <td>0.1080</td>\n",
              "      <td>0.1818</td>\n",
              "      <td>0.1142</td>\n",
              "      <td>0.1202</td>\n",
              "      <td>0.2306</td>\n",
              "      <td>0.1275</td>\n",
              "      <td>0.0894</td>\n",
              "      <td>0.0283</td>\n",
              "      <td>0.6822</td>\n",
              "      <td>0.8482</td>\n",
              "      <td>0.9213</td>\n",
              "      <td>0.6443</td>\n",
              "      <td>0.8621</td>\n",
              "      <td>0.5990</td>\n",
              "      <td>0.5358</td>\n",
              "      <td>0.4056</td>\n",
              "      <td>TPP</td>\n",
              "      <td>14585</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6852</th>\n",
              "      <td>6858</td>\n",
              "      <td>0.1470</td>\n",
              "      <td>0.4372</td>\n",
              "      <td>0.0126</td>\n",
              "      <td>0.0997</td>\n",
              "      <td>0.1584</td>\n",
              "      <td>0.2314</td>\n",
              "      <td>0.1791</td>\n",
              "      <td>0.1352</td>\n",
              "      <td>0.0967</td>\n",
              "      <td>0.0666</td>\n",
              "      <td>0.0329</td>\n",
              "      <td>0.8327</td>\n",
              "      <td>0.7383</td>\n",
              "      <td>0.9272</td>\n",
              "      <td>0.6447</td>\n",
              "      <td>0.7543</td>\n",
              "      <td>0.6988</td>\n",
              "      <td>0.4780</td>\n",
              "      <td>0.4295</td>\n",
              "      <td>TPP</td>\n",
              "      <td>6215</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6853</th>\n",
              "      <td>6859</td>\n",
              "      <td>0.1291</td>\n",
              "      <td>0.5170</td>\n",
              "      <td>0.0741</td>\n",
              "      <td>0.1017</td>\n",
              "      <td>0.1393</td>\n",
              "      <td>0.2065</td>\n",
              "      <td>0.1590</td>\n",
              "      <td>0.2016</td>\n",
              "      <td>0.1005</td>\n",
              "      <td>0.0726</td>\n",
              "      <td>0.0187</td>\n",
              "      <td>0.4563</td>\n",
              "      <td>0.6130</td>\n",
              "      <td>0.8170</td>\n",
              "      <td>0.5522</td>\n",
              "      <td>0.7546</td>\n",
              "      <td>0.8028</td>\n",
              "      <td>0.2355</td>\n",
              "      <td>0.4360</td>\n",
              "      <td>TPP</td>\n",
              "      <td>6032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6854</th>\n",
              "      <td>6860</td>\n",
              "      <td>-97.0000</td>\n",
              "      <td>-97.0000</td>\n",
              "      <td>-97.0000</td>\n",
              "      <td>-97.0000</td>\n",
              "      <td>-97.0000</td>\n",
              "      <td>-97.0000</td>\n",
              "      <td>-97.0000</td>\n",
              "      <td>-97.0000</td>\n",
              "      <td>-97.0000</td>\n",
              "      <td>-97.0000</td>\n",
              "      <td>-97.0000</td>\n",
              "      <td>-97.0000</td>\n",
              "      <td>-97.0000</td>\n",
              "      <td>-97.0000</td>\n",
              "      <td>-97.0000</td>\n",
              "      <td>-97.0000</td>\n",
              "      <td>-97.0000</td>\n",
              "      <td>-97.0000</td>\n",
              "      <td>-97.0000</td>\n",
              "      <td>EMIS</td>\n",
              "      <td>135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6855</th>\n",
              "      <td>6861</td>\n",
              "      <td>0.1420</td>\n",
              "      <td>0.6017</td>\n",
              "      <td>0.2127</td>\n",
              "      <td>0.2118</td>\n",
              "      <td>0.2063</td>\n",
              "      <td>0.3310</td>\n",
              "      <td>0.1857</td>\n",
              "      <td>0.0592</td>\n",
              "      <td>0.0059</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.4118</td>\n",
              "      <td>0.4815</td>\n",
              "      <td>0.0641</td>\n",
              "      <td>0.3335</td>\n",
              "      <td>0.3889</td>\n",
              "      <td>0.9083</td>\n",
              "      <td>0.2268</td>\n",
              "      <td>0.3620</td>\n",
              "      <td>EMIS</td>\n",
              "      <td>5549</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6856</th>\n",
              "      <td>6862</td>\n",
              "      <td>0.0111</td>\n",
              "      <td>0.4746</td>\n",
              "      <td>0.0465</td>\n",
              "      <td>0.0481</td>\n",
              "      <td>0.1957</td>\n",
              "      <td>0.0984</td>\n",
              "      <td>0.2853</td>\n",
              "      <td>0.1582</td>\n",
              "      <td>0.1040</td>\n",
              "      <td>0.0758</td>\n",
              "      <td>0.0345</td>\n",
              "      <td>0.7307</td>\n",
              "      <td>0.7183</td>\n",
              "      <td>0.7561</td>\n",
              "      <td>0.6108</td>\n",
              "      <td>0.8217</td>\n",
              "      <td>0.7468</td>\n",
              "      <td>0.2996</td>\n",
              "      <td>0.2104</td>\n",
              "      <td>TPP</td>\n",
              "      <td>5529</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6857 rows Ã— 22 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      ClinicID  OnlineAppointmentUse  malepct  ...  bcaaware  vendor  numpats\n",
              "0            1                0.0597   0.4785  ...    0.1809     TPP     4088\n",
              "1            2                0.1338   0.4745  ...    0.2978     TPP    19599\n",
              "2            3                0.0610   0.4045  ...    0.2313     TPP    10606\n",
              "3            4                0.0990   0.3981  ...    0.5476     TPP     8047\n",
              "4            5                0.0998   0.4675  ...    0.4056     TPP    14585\n",
              "...        ...                   ...      ...  ...       ...     ...      ...\n",
              "6852      6858                0.1470   0.4372  ...    0.4295     TPP     6215\n",
              "6853      6859                0.1291   0.5170  ...    0.4360     TPP     6032\n",
              "6854      6860              -97.0000 -97.0000  ...  -97.0000    EMIS      135\n",
              "6855      6861                0.1420   0.6017  ...    0.3620    EMIS     5549\n",
              "6856      6862                0.0111   0.4746  ...    0.2104     TPP     5529\n",
              "\n",
              "[6857 rows x 22 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjQpWaenBSfz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "outputId": "30a77518-094d-44b8-bf9b-0b5306d926a3"
      },
      "source": [
        "pd.options.display.max_rows = 30\n",
        "Online_appoint.isnull().sum()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ClinicID                0\n",
              "OnlineAppointmentUse    0\n",
              "malepct                 0\n",
              "unemp                   0\n",
              "age16to24               0\n",
              "age25to34               0\n",
              "age35to44               0\n",
              "age45to54               0\n",
              "age55to64               0\n",
              "age65to74               0\n",
              "age75to84               0\n",
              "age85plus               0\n",
              "phoneeasy               0\n",
              "onlineasy               0\n",
              "race                    0\n",
              "longstdhealth           0\n",
              "canmngownhealth         0\n",
              "reducedability          0\n",
              "prefgpalways            0\n",
              "bcaaware                0\n",
              "vendor                  0\n",
              "numpats                 0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vU055oybddnC",
        "colab_type": "text"
      },
      "source": [
        "### Preprocessing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D41ynRpsBTnm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "1c7bfcd3-f4a7-4c5b-9a52-56535582ad01"
      },
      "source": [
        "Online_appoint.query([Online_appoint.columns]<1)\n",
        "Online_appoint.columns"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['ClinicID', 'OnlineAppointmentUse', 'malepct', 'unemp', 'age16to24',\n",
              "       'age25to34', 'age35to44', 'age45to54', 'age55to64', 'age65to74',\n",
              "       'age75to84', 'age85plus', 'phoneeasy', 'onlineasy', 'race',\n",
              "       'longstdhealth', 'canmngownhealth', 'reducedability', 'prefgpalways',\n",
              "       'bcaaware', 'vendor', 'numpats'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_Rfrcl0VFFW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "outputId": "3431a5ed-a56a-486c-c812-12d38db874ce"
      },
      "source": [
        "Online_appoint.describe()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ClinicID</th>\n",
              "      <th>OnlineAppointmentUse</th>\n",
              "      <th>malepct</th>\n",
              "      <th>unemp</th>\n",
              "      <th>age16to24</th>\n",
              "      <th>age25to34</th>\n",
              "      <th>age35to44</th>\n",
              "      <th>age45to54</th>\n",
              "      <th>age55to64</th>\n",
              "      <th>age65to74</th>\n",
              "      <th>age75to84</th>\n",
              "      <th>age85plus</th>\n",
              "      <th>phoneeasy</th>\n",
              "      <th>onlineasy</th>\n",
              "      <th>race</th>\n",
              "      <th>longstdhealth</th>\n",
              "      <th>canmngownhealth</th>\n",
              "      <th>reducedability</th>\n",
              "      <th>prefgpalways</th>\n",
              "      <th>bcaaware</th>\n",
              "      <th>numpats</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>6857.0000</td>\n",
              "      <td>6857.0000</td>\n",
              "      <td>6857.0000</td>\n",
              "      <td>6857.0000</td>\n",
              "      <td>6857.0000</td>\n",
              "      <td>6857.0000</td>\n",
              "      <td>6857.0000</td>\n",
              "      <td>6857.0000</td>\n",
              "      <td>6857.0000</td>\n",
              "      <td>6857.0000</td>\n",
              "      <td>6857.0000</td>\n",
              "      <td>6857.0000</td>\n",
              "      <td>6857.0000</td>\n",
              "      <td>6857.0000</td>\n",
              "      <td>6857.0000</td>\n",
              "      <td>6857.0000</td>\n",
              "      <td>6857.0000</td>\n",
              "      <td>6857.0000</td>\n",
              "      <td>6857.0000</td>\n",
              "      <td>6857.0000</td>\n",
              "      <td>6857.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>3433.9555</td>\n",
              "      <td>-0.2758</td>\n",
              "      <td>0.2808</td>\n",
              "      <td>-0.7985</td>\n",
              "      <td>-0.1072</td>\n",
              "      <td>-0.0472</td>\n",
              "      <td>-0.0461</td>\n",
              "      <td>-0.0349</td>\n",
              "      <td>-0.0544</td>\n",
              "      <td>-0.1152</td>\n",
              "      <td>-0.5397</td>\n",
              "      <td>-2.1991</td>\n",
              "      <td>0.5054</td>\n",
              "      <td>0.5561</td>\n",
              "      <td>0.4994</td>\n",
              "      <td>0.2947</td>\n",
              "      <td>-0.6971</td>\n",
              "      <td>-0.9105</td>\n",
              "      <td>-2.8189</td>\n",
              "      <td>0.1816</td>\n",
              "      <td>8602.8997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1979.6661</td>\n",
              "      <td>6.3362</td>\n",
              "      <td>4.5556</td>\n",
              "      <td>9.0294</td>\n",
              "      <td>4.5375</td>\n",
              "      <td>4.5405</td>\n",
              "      <td>4.5404</td>\n",
              "      <td>4.5407</td>\n",
              "      <td>4.5398</td>\n",
              "      <td>4.8365</td>\n",
              "      <td>7.7152</td>\n",
              "      <td>14.6036</td>\n",
              "      <td>4.5698</td>\n",
              "      <td>4.5698</td>\n",
              "      <td>4.8740</td>\n",
              "      <td>4.7065</td>\n",
              "      <td>12.1261</td>\n",
              "      <td>12.0418</td>\n",
              "      <td>17.6659</td>\n",
              "      <td>4.7060</td>\n",
              "      <td>5505.2618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.0000</td>\n",
              "      <td>-98.0000</td>\n",
              "      <td>-97.0000</td>\n",
              "      <td>-98.0000</td>\n",
              "      <td>-97.0000</td>\n",
              "      <td>-97.0000</td>\n",
              "      <td>-97.0000</td>\n",
              "      <td>-97.0000</td>\n",
              "      <td>-97.0000</td>\n",
              "      <td>-98.0000</td>\n",
              "      <td>-98.0000</td>\n",
              "      <td>-98.0000</td>\n",
              "      <td>-97.0000</td>\n",
              "      <td>-97.0000</td>\n",
              "      <td>-98.0000</td>\n",
              "      <td>-97.0000</td>\n",
              "      <td>-97.0000</td>\n",
              "      <td>-97.0000</td>\n",
              "      <td>-97.0000</td>\n",
              "      <td>-98.0000</td>\n",
              "      <td>52.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1720.0000</td>\n",
              "      <td>0.0713</td>\n",
              "      <td>0.4560</td>\n",
              "      <td>0.0131</td>\n",
              "      <td>0.0677</td>\n",
              "      <td>0.1109</td>\n",
              "      <td>0.1223</td>\n",
              "      <td>0.1494</td>\n",
              "      <td>0.1306</td>\n",
              "      <td>0.0933</td>\n",
              "      <td>0.0493</td>\n",
              "      <td>0.0165</td>\n",
              "      <td>0.5875</td>\n",
              "      <td>0.6961</td>\n",
              "      <td>0.6026</td>\n",
              "      <td>0.4663</td>\n",
              "      <td>0.7774</td>\n",
              "      <td>0.5263</td>\n",
              "      <td>0.3412</td>\n",
              "      <td>0.2924</td>\n",
              "      <td>4789.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>3434.0000</td>\n",
              "      <td>0.1214</td>\n",
              "      <td>0.4918</td>\n",
              "      <td>0.0315</td>\n",
              "      <td>0.0974</td>\n",
              "      <td>0.1548</td>\n",
              "      <td>0.1596</td>\n",
              "      <td>0.1772</td>\n",
              "      <td>0.1605</td>\n",
              "      <td>0.1266</td>\n",
              "      <td>0.0726</td>\n",
              "      <td>0.0265</td>\n",
              "      <td>0.7475</td>\n",
              "      <td>0.7865</td>\n",
              "      <td>0.8572</td>\n",
              "      <td>0.5268</td>\n",
              "      <td>0.8390</td>\n",
              "      <td>0.5941</td>\n",
              "      <td>0.4761</td>\n",
              "      <td>0.4086</td>\n",
              "      <td>7561.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>5148.0000</td>\n",
              "      <td>0.1855</td>\n",
              "      <td>0.5286</td>\n",
              "      <td>0.0606</td>\n",
              "      <td>0.1309</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>0.2043</td>\n",
              "      <td>0.2046</td>\n",
              "      <td>0.1863</td>\n",
              "      <td>0.1585</td>\n",
              "      <td>0.0946</td>\n",
              "      <td>0.0383</td>\n",
              "      <td>0.8756</td>\n",
              "      <td>0.8597</td>\n",
              "      <td>0.9415</td>\n",
              "      <td>0.5816</td>\n",
              "      <td>0.8901</td>\n",
              "      <td>0.6635</td>\n",
              "      <td>0.6186</td>\n",
              "      <td>0.5236</td>\n",
              "      <td>11106.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>6862.0000</td>\n",
              "      <td>0.6674</td>\n",
              "      <td>0.8427</td>\n",
              "      <td>0.5951</td>\n",
              "      <td>0.9073</td>\n",
              "      <td>0.6937</td>\n",
              "      <td>0.4442</td>\n",
              "      <td>0.3514</td>\n",
              "      <td>0.3203</td>\n",
              "      <td>0.3095</td>\n",
              "      <td>0.2333</td>\n",
              "      <td>0.5815</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.9802</td>\n",
              "      <td>0.9586</td>\n",
              "      <td>73488.0000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       ClinicID  OnlineAppointmentUse  ...  bcaaware    numpats\n",
              "count 6857.0000             6857.0000  ... 6857.0000  6857.0000\n",
              "mean  3433.9555               -0.2758  ...    0.1816  8602.8997\n",
              "std   1979.6661                6.3362  ...    4.7060  5505.2618\n",
              "min      1.0000              -98.0000  ...  -98.0000    52.0000\n",
              "25%   1720.0000                0.0713  ...    0.2924  4789.0000\n",
              "50%   3434.0000                0.1214  ...    0.4086  7561.0000\n",
              "75%   5148.0000                0.1855  ...    0.5236 11106.0000\n",
              "max   6862.0000                0.6674  ...    0.9586 73488.0000\n",
              "\n",
              "[8 rows x 21 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoEUwpnaVtKv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "outputId": "fb03a355-f8b9-47f5-eaf3-831cbcf9e96e"
      },
      "source": [
        "# making all the values positive\n",
        "Online_appoint['OnlineAppointmentUse']=np.absolute(Online_appoint['OnlineAppointmentUse'])\n",
        "Online_appoint['malepct']=np.absolute(Online_appoint['malepct'])\n",
        "Online_appoint['unemp']=np.absolute(Online_appoint['unemp'])\n",
        "Online_appoint['age16to24']=np.absolute(Online_appoint['age16to24'])\n",
        "Online_appoint['age25to34']=np.absolute(Online_appoint['age25to34'])\n",
        "Online_appoint['age35to44']=np.absolute(Online_appoint['age35to44'])\n",
        "Online_appoint['age45to54']=np.absolute(Online_appoint['age45to54'])\n",
        "Online_appoint['age55to64']=np.absolute(Online_appoint['age55to64'])\n",
        "Online_appoint['age65to74']=np.absolute(Online_appoint['age65to74'])\n",
        "Online_appoint['age75to84']=np.absolute(Online_appoint['age75to84'])\n",
        "Online_appoint['age85plus']=np.absolute(Online_appoint['age85plus'])\n",
        "Online_appoint['phoneeasy']=np.absolute(Online_appoint['phoneeasy'])\n",
        "Online_appoint['onlineasy']=np.absolute(Online_appoint['onlineasy'])\n",
        "Online_appoint['canmngownhealth']=np.absolute(Online_appoint['canmngownhealth'])\n",
        "Online_appoint['race']=np.absolute(Online_appoint['race'])\n",
        "Online_appoint['longstdhealth']=np.absolute(Online_appoint['longstdhealth'])\n",
        "Online_appoint['reducedability']=np.absolute(Online_appoint['reducedability'])\n",
        "Online_appoint['prefgpalways']=np.absolute(Online_appoint['prefgpalways'])\n",
        "Online_appoint['bcaaware']=np.absolute(Online_appoint['bcaaware'])\n",
        "\n",
        "\n",
        "Online_appoint.describe()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ClinicID</th>\n",
              "      <th>OnlineAppointmentUse</th>\n",
              "      <th>malepct</th>\n",
              "      <th>unemp</th>\n",
              "      <th>age16to24</th>\n",
              "      <th>age25to34</th>\n",
              "      <th>age35to44</th>\n",
              "      <th>age45to54</th>\n",
              "      <th>age55to64</th>\n",
              "      <th>age65to74</th>\n",
              "      <th>age75to84</th>\n",
              "      <th>age85plus</th>\n",
              "      <th>phoneeasy</th>\n",
              "      <th>onlineasy</th>\n",
              "      <th>race</th>\n",
              "      <th>longstdhealth</th>\n",
              "      <th>canmngownhealth</th>\n",
              "      <th>reducedability</th>\n",
              "      <th>prefgpalways</th>\n",
              "      <th>bcaaware</th>\n",
              "      <th>numpats</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>6857.0000</td>\n",
              "      <td>6857.0000</td>\n",
              "      <td>6857.0000</td>\n",
              "      <td>6857.0000</td>\n",
              "      <td>6857.0000</td>\n",
              "      <td>6857.0000</td>\n",
              "      <td>6857.0000</td>\n",
              "      <td>6857.0000</td>\n",
              "      <td>6857.0000</td>\n",
              "      <td>6857.0000</td>\n",
              "      <td>6857.0000</td>\n",
              "      <td>6857.0000</td>\n",
              "      <td>6857.0000</td>\n",
              "      <td>6857.0000</td>\n",
              "      <td>6857.0000</td>\n",
              "      <td>6857.0000</td>\n",
              "      <td>6857.0000</td>\n",
              "      <td>6857.0000</td>\n",
              "      <td>6857.0000</td>\n",
              "      <td>6857.0000</td>\n",
              "      <td>6857.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>3433.9555</td>\n",
              "      <td>0.5488</td>\n",
              "      <td>0.7052</td>\n",
              "      <td>0.8830</td>\n",
              "      <td>0.3172</td>\n",
              "      <td>0.3772</td>\n",
              "      <td>0.3783</td>\n",
              "      <td>0.3895</td>\n",
              "      <td>0.3700</td>\n",
              "      <td>0.3664</td>\n",
              "      <td>0.6851</td>\n",
              "      <td>2.2556</td>\n",
              "      <td>0.9298</td>\n",
              "      <td>0.9805</td>\n",
              "      <td>0.9810</td>\n",
              "      <td>0.7474</td>\n",
              "      <td>2.3302</td>\n",
              "      <td>2.0885</td>\n",
              "      <td>3.7732</td>\n",
              "      <td>0.6346</td>\n",
              "      <td>8602.8997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1979.6661</td>\n",
              "      <td>6.3184</td>\n",
              "      <td>4.5095</td>\n",
              "      <td>9.0216</td>\n",
              "      <td>4.5277</td>\n",
              "      <td>4.5251</td>\n",
              "      <td>4.5248</td>\n",
              "      <td>4.5241</td>\n",
              "      <td>4.5250</td>\n",
              "      <td>4.8240</td>\n",
              "      <td>7.7036</td>\n",
              "      <td>14.5950</td>\n",
              "      <td>4.5026</td>\n",
              "      <td>4.4979</td>\n",
              "      <td>4.8003</td>\n",
              "      <td>4.6561</td>\n",
              "      <td>11.9205</td>\n",
              "      <td>11.8942</td>\n",
              "      <td>17.4870</td>\n",
              "      <td>4.6665</td>\n",
              "      <td>5505.2618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.2478</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0615</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.1591</td>\n",
              "      <td>0.2031</td>\n",
              "      <td>0.2624</td>\n",
              "      <td>0.0214</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>52.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1720.0000</td>\n",
              "      <td>0.0720</td>\n",
              "      <td>0.4564</td>\n",
              "      <td>0.0137</td>\n",
              "      <td>0.0678</td>\n",
              "      <td>0.1113</td>\n",
              "      <td>0.1226</td>\n",
              "      <td>0.1497</td>\n",
              "      <td>0.1310</td>\n",
              "      <td>0.0937</td>\n",
              "      <td>0.0500</td>\n",
              "      <td>0.0175</td>\n",
              "      <td>0.5893</td>\n",
              "      <td>0.6972</td>\n",
              "      <td>0.6045</td>\n",
              "      <td>0.4670</td>\n",
              "      <td>0.7829</td>\n",
              "      <td>0.5319</td>\n",
              "      <td>0.3628</td>\n",
              "      <td>0.2931</td>\n",
              "      <td>4789.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>3434.0000</td>\n",
              "      <td>0.1222</td>\n",
              "      <td>0.4922</td>\n",
              "      <td>0.0324</td>\n",
              "      <td>0.0976</td>\n",
              "      <td>0.1551</td>\n",
              "      <td>0.1598</td>\n",
              "      <td>0.1774</td>\n",
              "      <td>0.1608</td>\n",
              "      <td>0.1268</td>\n",
              "      <td>0.0732</td>\n",
              "      <td>0.0275</td>\n",
              "      <td>0.7485</td>\n",
              "      <td>0.7869</td>\n",
              "      <td>0.8585</td>\n",
              "      <td>0.5271</td>\n",
              "      <td>0.8425</td>\n",
              "      <td>0.5990</td>\n",
              "      <td>0.4920</td>\n",
              "      <td>0.4097</td>\n",
              "      <td>7561.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>5148.0000</td>\n",
              "      <td>0.1866</td>\n",
              "      <td>0.5289</td>\n",
              "      <td>0.0619</td>\n",
              "      <td>0.1313</td>\n",
              "      <td>0.2073</td>\n",
              "      <td>0.2047</td>\n",
              "      <td>0.2051</td>\n",
              "      <td>0.1866</td>\n",
              "      <td>0.1588</td>\n",
              "      <td>0.0952</td>\n",
              "      <td>0.0396</td>\n",
              "      <td>0.8767</td>\n",
              "      <td>0.8604</td>\n",
              "      <td>0.9423</td>\n",
              "      <td>0.5820</td>\n",
              "      <td>0.8934</td>\n",
              "      <td>0.6677</td>\n",
              "      <td>0.6438</td>\n",
              "      <td>0.5254</td>\n",
              "      <td>11106.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>6862.0000</td>\n",
              "      <td>98.0000</td>\n",
              "      <td>97.0000</td>\n",
              "      <td>98.0000</td>\n",
              "      <td>97.0000</td>\n",
              "      <td>97.0000</td>\n",
              "      <td>97.0000</td>\n",
              "      <td>97.0000</td>\n",
              "      <td>97.0000</td>\n",
              "      <td>98.0000</td>\n",
              "      <td>98.0000</td>\n",
              "      <td>98.0000</td>\n",
              "      <td>97.0000</td>\n",
              "      <td>97.0000</td>\n",
              "      <td>98.0000</td>\n",
              "      <td>97.0000</td>\n",
              "      <td>97.0000</td>\n",
              "      <td>97.0000</td>\n",
              "      <td>97.0000</td>\n",
              "      <td>98.0000</td>\n",
              "      <td>73488.0000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       ClinicID  OnlineAppointmentUse  ...  bcaaware    numpats\n",
              "count 6857.0000             6857.0000  ... 6857.0000  6857.0000\n",
              "mean  3433.9555                0.5488  ...    0.6346  8602.8997\n",
              "std   1979.6661                6.3184  ...    4.6665  5505.2618\n",
              "min      1.0000                0.0000  ...    0.0052    52.0000\n",
              "25%   1720.0000                0.0720  ...    0.2931  4789.0000\n",
              "50%   3434.0000                0.1222  ...    0.4097  7561.0000\n",
              "75%   5148.0000                0.1866  ...    0.5254 11106.0000\n",
              "max   6862.0000               98.0000  ...   98.0000 73488.0000\n",
              "\n",
              "[8 rows x 21 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOAmYEg8baxW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "outputId": "67798c07-51ad-480f-aa0a-68eeea380a8a"
      },
      "source": [
        "Ohe=OneHotEncoder(sparse=False)\n",
        "Online_appoint_vendor=pd.DataFrame(Ohe.fit_transform(Online_appoint[['vendor']]))\n",
        "Online_appoint_vendor.rename(columns={0:'EMIS',1:'EMIS (I)',2:'MICROTEST',3:'TPP',4:'VISION',5:'VISION (I)'},inplace=True)\n",
        "Online_appoint_vendor"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>EMIS</th>\n",
              "      <th>EMIS (I)</th>\n",
              "      <th>MICROTEST</th>\n",
              "      <th>TPP</th>\n",
              "      <th>VISION</th>\n",
              "      <th>VISION (I)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6852</th>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6853</th>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6854</th>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6855</th>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6856</th>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6857 rows Ã— 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       EMIS  EMIS (I)  MICROTEST    TPP  VISION  VISION (I)\n",
              "0    0.0000    0.0000     0.0000 1.0000  0.0000      0.0000\n",
              "1    0.0000    0.0000     0.0000 1.0000  0.0000      0.0000\n",
              "2    0.0000    0.0000     0.0000 1.0000  0.0000      0.0000\n",
              "3    0.0000    0.0000     0.0000 1.0000  0.0000      0.0000\n",
              "4    0.0000    0.0000     0.0000 1.0000  0.0000      0.0000\n",
              "...     ...       ...        ...    ...     ...         ...\n",
              "6852 0.0000    0.0000     0.0000 1.0000  0.0000      0.0000\n",
              "6853 0.0000    0.0000     0.0000 1.0000  0.0000      0.0000\n",
              "6854 1.0000    0.0000     0.0000 0.0000  0.0000      0.0000\n",
              "6855 1.0000    0.0000     0.0000 0.0000  0.0000      0.0000\n",
              "6856 0.0000    0.0000     0.0000 1.0000  0.0000      0.0000\n",
              "\n",
              "[6857 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WGJ8Nmfq5Pl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# seperating the numpats columns\n",
        "Online_appoint_numpats=Online_appoint['numpats']"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wxc-tfsTfLEq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Online_appoint.drop(columns=['vendor','numpats'],axis=1,inplace=True)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_TzsS8dgdWz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "outputId": "5cd00401-b332-448d-de49-a349d363ee21"
      },
      "source": [
        "Online_appoint_final=Online_appoint.join(Online_appoint_vendor)\n",
        "Online_appoint_final"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ClinicID</th>\n",
              "      <th>OnlineAppointmentUse</th>\n",
              "      <th>malepct</th>\n",
              "      <th>unemp</th>\n",
              "      <th>age16to24</th>\n",
              "      <th>age25to34</th>\n",
              "      <th>age35to44</th>\n",
              "      <th>age45to54</th>\n",
              "      <th>age55to64</th>\n",
              "      <th>age65to74</th>\n",
              "      <th>age75to84</th>\n",
              "      <th>age85plus</th>\n",
              "      <th>phoneeasy</th>\n",
              "      <th>onlineasy</th>\n",
              "      <th>race</th>\n",
              "      <th>longstdhealth</th>\n",
              "      <th>canmngownhealth</th>\n",
              "      <th>reducedability</th>\n",
              "      <th>prefgpalways</th>\n",
              "      <th>bcaaware</th>\n",
              "      <th>EMIS</th>\n",
              "      <th>EMIS (I)</th>\n",
              "      <th>MICROTEST</th>\n",
              "      <th>TPP</th>\n",
              "      <th>VISION</th>\n",
              "      <th>VISION (I)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0.0597</td>\n",
              "      <td>0.4785</td>\n",
              "      <td>0.1083</td>\n",
              "      <td>0.0587</td>\n",
              "      <td>0.1818</td>\n",
              "      <td>0.0992</td>\n",
              "      <td>0.1825</td>\n",
              "      <td>0.1430</td>\n",
              "      <td>0.1547</td>\n",
              "      <td>0.1205</td>\n",
              "      <td>0.0597</td>\n",
              "      <td>0.4655</td>\n",
              "      <td>0.7673</td>\n",
              "      <td>0.8247</td>\n",
              "      <td>0.5260</td>\n",
              "      <td>0.8208</td>\n",
              "      <td>0.6516</td>\n",
              "      <td>0.5347</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0.1338</td>\n",
              "      <td>0.4745</td>\n",
              "      <td>0.1062</td>\n",
              "      <td>0.1468</td>\n",
              "      <td>0.1393</td>\n",
              "      <td>0.0729</td>\n",
              "      <td>0.2185</td>\n",
              "      <td>0.1657</td>\n",
              "      <td>0.1399</td>\n",
              "      <td>0.0717</td>\n",
              "      <td>0.0451</td>\n",
              "      <td>0.7172</td>\n",
              "      <td>0.9420</td>\n",
              "      <td>0.9565</td>\n",
              "      <td>0.5713</td>\n",
              "      <td>0.7960</td>\n",
              "      <td>0.6967</td>\n",
              "      <td>0.7239</td>\n",
              "      <td>0.2978</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>0.0610</td>\n",
              "      <td>0.4045</td>\n",
              "      <td>0.0364</td>\n",
              "      <td>0.0670</td>\n",
              "      <td>0.1258</td>\n",
              "      <td>0.1952</td>\n",
              "      <td>0.1705</td>\n",
              "      <td>0.1817</td>\n",
              "      <td>0.1409</td>\n",
              "      <td>0.0955</td>\n",
              "      <td>0.0233</td>\n",
              "      <td>0.8579</td>\n",
              "      <td>0.9768</td>\n",
              "      <td>0.9225</td>\n",
              "      <td>0.4262</td>\n",
              "      <td>0.9553</td>\n",
              "      <td>0.6520</td>\n",
              "      <td>0.5634</td>\n",
              "      <td>0.2313</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>0.0990</td>\n",
              "      <td>0.3981</td>\n",
              "      <td>0.0210</td>\n",
              "      <td>0.1016</td>\n",
              "      <td>0.0740</td>\n",
              "      <td>0.2052</td>\n",
              "      <td>0.1438</td>\n",
              "      <td>0.1325</td>\n",
              "      <td>0.2006</td>\n",
              "      <td>0.0982</td>\n",
              "      <td>0.0440</td>\n",
              "      <td>0.6888</td>\n",
              "      <td>0.9419</td>\n",
              "      <td>0.9756</td>\n",
              "      <td>0.4972</td>\n",
              "      <td>0.9128</td>\n",
              "      <td>0.5157</td>\n",
              "      <td>0.5682</td>\n",
              "      <td>0.5476</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0.0998</td>\n",
              "      <td>0.4675</td>\n",
              "      <td>0.0345</td>\n",
              "      <td>0.1080</td>\n",
              "      <td>0.1818</td>\n",
              "      <td>0.1142</td>\n",
              "      <td>0.1202</td>\n",
              "      <td>0.2306</td>\n",
              "      <td>0.1275</td>\n",
              "      <td>0.0894</td>\n",
              "      <td>0.0283</td>\n",
              "      <td>0.6822</td>\n",
              "      <td>0.8482</td>\n",
              "      <td>0.9213</td>\n",
              "      <td>0.6443</td>\n",
              "      <td>0.8621</td>\n",
              "      <td>0.5990</td>\n",
              "      <td>0.5358</td>\n",
              "      <td>0.4056</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6852</th>\n",
              "      <td>6858</td>\n",
              "      <td>0.1470</td>\n",
              "      <td>0.4372</td>\n",
              "      <td>0.0126</td>\n",
              "      <td>0.0997</td>\n",
              "      <td>0.1584</td>\n",
              "      <td>0.2314</td>\n",
              "      <td>0.1791</td>\n",
              "      <td>0.1352</td>\n",
              "      <td>0.0967</td>\n",
              "      <td>0.0666</td>\n",
              "      <td>0.0329</td>\n",
              "      <td>0.8327</td>\n",
              "      <td>0.7383</td>\n",
              "      <td>0.9272</td>\n",
              "      <td>0.6447</td>\n",
              "      <td>0.7543</td>\n",
              "      <td>0.6988</td>\n",
              "      <td>0.4780</td>\n",
              "      <td>0.4295</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6853</th>\n",
              "      <td>6859</td>\n",
              "      <td>0.1291</td>\n",
              "      <td>0.5170</td>\n",
              "      <td>0.0741</td>\n",
              "      <td>0.1017</td>\n",
              "      <td>0.1393</td>\n",
              "      <td>0.2065</td>\n",
              "      <td>0.1590</td>\n",
              "      <td>0.2016</td>\n",
              "      <td>0.1005</td>\n",
              "      <td>0.0726</td>\n",
              "      <td>0.0187</td>\n",
              "      <td>0.4563</td>\n",
              "      <td>0.6130</td>\n",
              "      <td>0.8170</td>\n",
              "      <td>0.5522</td>\n",
              "      <td>0.7546</td>\n",
              "      <td>0.8028</td>\n",
              "      <td>0.2355</td>\n",
              "      <td>0.4360</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6854</th>\n",
              "      <td>6860</td>\n",
              "      <td>97.0000</td>\n",
              "      <td>97.0000</td>\n",
              "      <td>97.0000</td>\n",
              "      <td>97.0000</td>\n",
              "      <td>97.0000</td>\n",
              "      <td>97.0000</td>\n",
              "      <td>97.0000</td>\n",
              "      <td>97.0000</td>\n",
              "      <td>97.0000</td>\n",
              "      <td>97.0000</td>\n",
              "      <td>97.0000</td>\n",
              "      <td>97.0000</td>\n",
              "      <td>97.0000</td>\n",
              "      <td>97.0000</td>\n",
              "      <td>97.0000</td>\n",
              "      <td>97.0000</td>\n",
              "      <td>97.0000</td>\n",
              "      <td>97.0000</td>\n",
              "      <td>97.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6855</th>\n",
              "      <td>6861</td>\n",
              "      <td>0.1420</td>\n",
              "      <td>0.6017</td>\n",
              "      <td>0.2127</td>\n",
              "      <td>0.2118</td>\n",
              "      <td>0.2063</td>\n",
              "      <td>0.3310</td>\n",
              "      <td>0.1857</td>\n",
              "      <td>0.0592</td>\n",
              "      <td>0.0059</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.4118</td>\n",
              "      <td>0.4815</td>\n",
              "      <td>0.0641</td>\n",
              "      <td>0.3335</td>\n",
              "      <td>0.3889</td>\n",
              "      <td>0.9083</td>\n",
              "      <td>0.2268</td>\n",
              "      <td>0.3620</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6856</th>\n",
              "      <td>6862</td>\n",
              "      <td>0.0111</td>\n",
              "      <td>0.4746</td>\n",
              "      <td>0.0465</td>\n",
              "      <td>0.0481</td>\n",
              "      <td>0.1957</td>\n",
              "      <td>0.0984</td>\n",
              "      <td>0.2853</td>\n",
              "      <td>0.1582</td>\n",
              "      <td>0.1040</td>\n",
              "      <td>0.0758</td>\n",
              "      <td>0.0345</td>\n",
              "      <td>0.7307</td>\n",
              "      <td>0.7183</td>\n",
              "      <td>0.7561</td>\n",
              "      <td>0.6108</td>\n",
              "      <td>0.8217</td>\n",
              "      <td>0.7468</td>\n",
              "      <td>0.2996</td>\n",
              "      <td>0.2104</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6857 rows Ã— 26 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      ClinicID  OnlineAppointmentUse  malepct  ...    TPP  VISION  VISION (I)\n",
              "0            1                0.0597   0.4785  ... 1.0000  0.0000      0.0000\n",
              "1            2                0.1338   0.4745  ... 1.0000  0.0000      0.0000\n",
              "2            3                0.0610   0.4045  ... 1.0000  0.0000      0.0000\n",
              "3            4                0.0990   0.3981  ... 1.0000  0.0000      0.0000\n",
              "4            5                0.0998   0.4675  ... 1.0000  0.0000      0.0000\n",
              "...        ...                   ...      ...  ...    ...     ...         ...\n",
              "6852      6858                0.1470   0.4372  ... 1.0000  0.0000      0.0000\n",
              "6853      6859                0.1291   0.5170  ... 1.0000  0.0000      0.0000\n",
              "6854      6860               97.0000  97.0000  ... 0.0000  0.0000      0.0000\n",
              "6855      6861                0.1420   0.6017  ... 0.0000  0.0000      0.0000\n",
              "6856      6862                0.0111   0.4746  ... 1.0000  0.0000      0.0000\n",
              "\n",
              "[6857 rows x 26 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsHYzAzRY3ZI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# removing anamolies\n",
        "for x in Online_appoint_final.columns:\n",
        "  for y in Online_appoint_final[x]:\n",
        "    if int(y)>1:\n",
        "      Online_appoint_final[x].replace([y, y/100],inplace=True)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zT6UYmbRg63b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "finale_df=Online_appoint_final.join(Online_appoint_numpats)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nJcB7WmS30_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "outputId": "44757163-f484-4588-e05b-8e3d00a117c7"
      },
      "source": [
        "finale_df.drop(columns='ClinicID',axis=1,inplace=True)\n",
        "finale_df"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>OnlineAppointmentUse</th>\n",
              "      <th>malepct</th>\n",
              "      <th>unemp</th>\n",
              "      <th>age16to24</th>\n",
              "      <th>age25to34</th>\n",
              "      <th>age35to44</th>\n",
              "      <th>age45to54</th>\n",
              "      <th>age55to64</th>\n",
              "      <th>age65to74</th>\n",
              "      <th>age75to84</th>\n",
              "      <th>age85plus</th>\n",
              "      <th>phoneeasy</th>\n",
              "      <th>onlineasy</th>\n",
              "      <th>race</th>\n",
              "      <th>longstdhealth</th>\n",
              "      <th>canmngownhealth</th>\n",
              "      <th>reducedability</th>\n",
              "      <th>prefgpalways</th>\n",
              "      <th>bcaaware</th>\n",
              "      <th>EMIS</th>\n",
              "      <th>EMIS (I)</th>\n",
              "      <th>MICROTEST</th>\n",
              "      <th>TPP</th>\n",
              "      <th>VISION</th>\n",
              "      <th>VISION (I)</th>\n",
              "      <th>numpats</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0597</td>\n",
              "      <td>0.4785</td>\n",
              "      <td>0.1083</td>\n",
              "      <td>0.0587</td>\n",
              "      <td>0.1818</td>\n",
              "      <td>0.0992</td>\n",
              "      <td>0.1825</td>\n",
              "      <td>0.1430</td>\n",
              "      <td>0.1547</td>\n",
              "      <td>0.1205</td>\n",
              "      <td>0.0597</td>\n",
              "      <td>0.4655</td>\n",
              "      <td>0.7673</td>\n",
              "      <td>0.8247</td>\n",
              "      <td>0.5260</td>\n",
              "      <td>0.8208</td>\n",
              "      <td>0.6516</td>\n",
              "      <td>0.5347</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>4088</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.1338</td>\n",
              "      <td>0.4745</td>\n",
              "      <td>0.1062</td>\n",
              "      <td>0.1468</td>\n",
              "      <td>0.1393</td>\n",
              "      <td>0.0729</td>\n",
              "      <td>0.2185</td>\n",
              "      <td>0.1657</td>\n",
              "      <td>0.1399</td>\n",
              "      <td>0.0717</td>\n",
              "      <td>0.0451</td>\n",
              "      <td>0.7172</td>\n",
              "      <td>0.9420</td>\n",
              "      <td>0.9565</td>\n",
              "      <td>0.5713</td>\n",
              "      <td>0.7960</td>\n",
              "      <td>0.6967</td>\n",
              "      <td>0.7239</td>\n",
              "      <td>0.2978</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>19599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0610</td>\n",
              "      <td>0.4045</td>\n",
              "      <td>0.0364</td>\n",
              "      <td>0.0670</td>\n",
              "      <td>0.1258</td>\n",
              "      <td>0.1952</td>\n",
              "      <td>0.1705</td>\n",
              "      <td>0.1817</td>\n",
              "      <td>0.1409</td>\n",
              "      <td>0.0955</td>\n",
              "      <td>0.0233</td>\n",
              "      <td>0.8579</td>\n",
              "      <td>0.9768</td>\n",
              "      <td>0.9225</td>\n",
              "      <td>0.4262</td>\n",
              "      <td>0.9553</td>\n",
              "      <td>0.6520</td>\n",
              "      <td>0.5634</td>\n",
              "      <td>0.2313</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>10606</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0990</td>\n",
              "      <td>0.3981</td>\n",
              "      <td>0.0210</td>\n",
              "      <td>0.1016</td>\n",
              "      <td>0.0740</td>\n",
              "      <td>0.2052</td>\n",
              "      <td>0.1438</td>\n",
              "      <td>0.1325</td>\n",
              "      <td>0.2006</td>\n",
              "      <td>0.0982</td>\n",
              "      <td>0.0440</td>\n",
              "      <td>0.6888</td>\n",
              "      <td>0.9419</td>\n",
              "      <td>0.9756</td>\n",
              "      <td>0.4972</td>\n",
              "      <td>0.9128</td>\n",
              "      <td>0.5157</td>\n",
              "      <td>0.5682</td>\n",
              "      <td>0.5476</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>8047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0998</td>\n",
              "      <td>0.4675</td>\n",
              "      <td>0.0345</td>\n",
              "      <td>0.1080</td>\n",
              "      <td>0.1818</td>\n",
              "      <td>0.1142</td>\n",
              "      <td>0.1202</td>\n",
              "      <td>0.2306</td>\n",
              "      <td>0.1275</td>\n",
              "      <td>0.0894</td>\n",
              "      <td>0.0283</td>\n",
              "      <td>0.6822</td>\n",
              "      <td>0.8482</td>\n",
              "      <td>0.9213</td>\n",
              "      <td>0.6443</td>\n",
              "      <td>0.8621</td>\n",
              "      <td>0.5990</td>\n",
              "      <td>0.5358</td>\n",
              "      <td>0.4056</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>14585</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6852</th>\n",
              "      <td>0.1470</td>\n",
              "      <td>0.4372</td>\n",
              "      <td>0.0126</td>\n",
              "      <td>0.0997</td>\n",
              "      <td>0.1584</td>\n",
              "      <td>0.2314</td>\n",
              "      <td>0.1791</td>\n",
              "      <td>0.1352</td>\n",
              "      <td>0.0967</td>\n",
              "      <td>0.0666</td>\n",
              "      <td>0.0329</td>\n",
              "      <td>0.8327</td>\n",
              "      <td>0.7383</td>\n",
              "      <td>0.9272</td>\n",
              "      <td>0.6447</td>\n",
              "      <td>0.7543</td>\n",
              "      <td>0.6988</td>\n",
              "      <td>0.4780</td>\n",
              "      <td>0.4295</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>6215</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6853</th>\n",
              "      <td>0.1291</td>\n",
              "      <td>0.5170</td>\n",
              "      <td>0.0741</td>\n",
              "      <td>0.1017</td>\n",
              "      <td>0.1393</td>\n",
              "      <td>0.2065</td>\n",
              "      <td>0.1590</td>\n",
              "      <td>0.2016</td>\n",
              "      <td>0.1005</td>\n",
              "      <td>0.0726</td>\n",
              "      <td>0.0187</td>\n",
              "      <td>0.4563</td>\n",
              "      <td>0.6130</td>\n",
              "      <td>0.8170</td>\n",
              "      <td>0.5522</td>\n",
              "      <td>0.7546</td>\n",
              "      <td>0.8028</td>\n",
              "      <td>0.2355</td>\n",
              "      <td>0.4360</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>6032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6854</th>\n",
              "      <td>0.1291</td>\n",
              "      <td>0.5170</td>\n",
              "      <td>0.0741</td>\n",
              "      <td>0.1017</td>\n",
              "      <td>0.1393</td>\n",
              "      <td>0.2065</td>\n",
              "      <td>0.1590</td>\n",
              "      <td>0.2016</td>\n",
              "      <td>0.1005</td>\n",
              "      <td>0.0726</td>\n",
              "      <td>0.0187</td>\n",
              "      <td>0.4563</td>\n",
              "      <td>0.6130</td>\n",
              "      <td>0.8170</td>\n",
              "      <td>0.5522</td>\n",
              "      <td>0.7546</td>\n",
              "      <td>0.8028</td>\n",
              "      <td>0.2355</td>\n",
              "      <td>0.4360</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6855</th>\n",
              "      <td>0.1420</td>\n",
              "      <td>0.6017</td>\n",
              "      <td>0.2127</td>\n",
              "      <td>0.2118</td>\n",
              "      <td>0.2063</td>\n",
              "      <td>0.3310</td>\n",
              "      <td>0.1857</td>\n",
              "      <td>0.0592</td>\n",
              "      <td>0.0059</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.4118</td>\n",
              "      <td>0.4815</td>\n",
              "      <td>0.0641</td>\n",
              "      <td>0.3335</td>\n",
              "      <td>0.3889</td>\n",
              "      <td>0.9083</td>\n",
              "      <td>0.2268</td>\n",
              "      <td>0.3620</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>5549</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6856</th>\n",
              "      <td>0.0111</td>\n",
              "      <td>0.4746</td>\n",
              "      <td>0.0465</td>\n",
              "      <td>0.0481</td>\n",
              "      <td>0.1957</td>\n",
              "      <td>0.0984</td>\n",
              "      <td>0.2853</td>\n",
              "      <td>0.1582</td>\n",
              "      <td>0.1040</td>\n",
              "      <td>0.0758</td>\n",
              "      <td>0.0345</td>\n",
              "      <td>0.7307</td>\n",
              "      <td>0.7183</td>\n",
              "      <td>0.7561</td>\n",
              "      <td>0.6108</td>\n",
              "      <td>0.8217</td>\n",
              "      <td>0.7468</td>\n",
              "      <td>0.2996</td>\n",
              "      <td>0.2104</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>5529</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6857 rows Ã— 26 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      OnlineAppointmentUse  malepct  unemp  ...  VISION  VISION (I)  numpats\n",
              "0                   0.0597   0.4785 0.1083  ...  0.0000      0.0000     4088\n",
              "1                   0.1338   0.4745 0.1062  ...  0.0000      0.0000    19599\n",
              "2                   0.0610   0.4045 0.0364  ...  0.0000      0.0000    10606\n",
              "3                   0.0990   0.3981 0.0210  ...  0.0000      0.0000     8047\n",
              "4                   0.0998   0.4675 0.0345  ...  0.0000      0.0000    14585\n",
              "...                    ...      ...    ...  ...     ...         ...      ...\n",
              "6852                0.1470   0.4372 0.0126  ...  0.0000      0.0000     6215\n",
              "6853                0.1291   0.5170 0.0741  ...  0.0000      0.0000     6032\n",
              "6854                0.1291   0.5170 0.0741  ...  0.0000      0.0000      135\n",
              "6855                0.1420   0.6017 0.2127  ...  0.0000      0.0000     5549\n",
              "6856                0.0111   0.4746 0.0465  ...  0.0000      0.0000     5529\n",
              "\n",
              "[6857 rows x 26 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QcM1Rrwrk2H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "afa5c29b-9e46-4051-a31d-59eb564b1a3b"
      },
      "source": [
        "df_target=finale_df['OnlineAppointmentUse']\n",
        "df_target"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      0.0597\n",
              "1      0.1338\n",
              "2      0.0610\n",
              "3      0.0990\n",
              "4      0.0998\n",
              "        ...  \n",
              "6852   0.1470\n",
              "6853   0.1291\n",
              "6854   0.1291\n",
              "6855   0.1420\n",
              "6856   0.0111\n",
              "Name: OnlineAppointmentUse, Length: 6857, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YBTdns7sle1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "outputId": "675bf972-6b0e-4629-a631-f23bd9d1b001"
      },
      "source": [
        "df_features=finale_df.drop(columns='OnlineAppointmentUse',axis=1)\n",
        "df_features\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>malepct</th>\n",
              "      <th>unemp</th>\n",
              "      <th>age16to24</th>\n",
              "      <th>age25to34</th>\n",
              "      <th>age35to44</th>\n",
              "      <th>age45to54</th>\n",
              "      <th>age55to64</th>\n",
              "      <th>age65to74</th>\n",
              "      <th>age75to84</th>\n",
              "      <th>age85plus</th>\n",
              "      <th>phoneeasy</th>\n",
              "      <th>onlineasy</th>\n",
              "      <th>race</th>\n",
              "      <th>longstdhealth</th>\n",
              "      <th>canmngownhealth</th>\n",
              "      <th>reducedability</th>\n",
              "      <th>prefgpalways</th>\n",
              "      <th>bcaaware</th>\n",
              "      <th>EMIS</th>\n",
              "      <th>EMIS (I)</th>\n",
              "      <th>MICROTEST</th>\n",
              "      <th>TPP</th>\n",
              "      <th>VISION</th>\n",
              "      <th>VISION (I)</th>\n",
              "      <th>numpats</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.4785</td>\n",
              "      <td>0.1083</td>\n",
              "      <td>0.0587</td>\n",
              "      <td>0.1818</td>\n",
              "      <td>0.0992</td>\n",
              "      <td>0.1825</td>\n",
              "      <td>0.1430</td>\n",
              "      <td>0.1547</td>\n",
              "      <td>0.1205</td>\n",
              "      <td>0.0597</td>\n",
              "      <td>0.4655</td>\n",
              "      <td>0.7673</td>\n",
              "      <td>0.8247</td>\n",
              "      <td>0.5260</td>\n",
              "      <td>0.8208</td>\n",
              "      <td>0.6516</td>\n",
              "      <td>0.5347</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>4088</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.4745</td>\n",
              "      <td>0.1062</td>\n",
              "      <td>0.1468</td>\n",
              "      <td>0.1393</td>\n",
              "      <td>0.0729</td>\n",
              "      <td>0.2185</td>\n",
              "      <td>0.1657</td>\n",
              "      <td>0.1399</td>\n",
              "      <td>0.0717</td>\n",
              "      <td>0.0451</td>\n",
              "      <td>0.7172</td>\n",
              "      <td>0.9420</td>\n",
              "      <td>0.9565</td>\n",
              "      <td>0.5713</td>\n",
              "      <td>0.7960</td>\n",
              "      <td>0.6967</td>\n",
              "      <td>0.7239</td>\n",
              "      <td>0.2978</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>19599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.4045</td>\n",
              "      <td>0.0364</td>\n",
              "      <td>0.0670</td>\n",
              "      <td>0.1258</td>\n",
              "      <td>0.1952</td>\n",
              "      <td>0.1705</td>\n",
              "      <td>0.1817</td>\n",
              "      <td>0.1409</td>\n",
              "      <td>0.0955</td>\n",
              "      <td>0.0233</td>\n",
              "      <td>0.8579</td>\n",
              "      <td>0.9768</td>\n",
              "      <td>0.9225</td>\n",
              "      <td>0.4262</td>\n",
              "      <td>0.9553</td>\n",
              "      <td>0.6520</td>\n",
              "      <td>0.5634</td>\n",
              "      <td>0.2313</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>10606</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.3981</td>\n",
              "      <td>0.0210</td>\n",
              "      <td>0.1016</td>\n",
              "      <td>0.0740</td>\n",
              "      <td>0.2052</td>\n",
              "      <td>0.1438</td>\n",
              "      <td>0.1325</td>\n",
              "      <td>0.2006</td>\n",
              "      <td>0.0982</td>\n",
              "      <td>0.0440</td>\n",
              "      <td>0.6888</td>\n",
              "      <td>0.9419</td>\n",
              "      <td>0.9756</td>\n",
              "      <td>0.4972</td>\n",
              "      <td>0.9128</td>\n",
              "      <td>0.5157</td>\n",
              "      <td>0.5682</td>\n",
              "      <td>0.5476</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>8047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.4675</td>\n",
              "      <td>0.0345</td>\n",
              "      <td>0.1080</td>\n",
              "      <td>0.1818</td>\n",
              "      <td>0.1142</td>\n",
              "      <td>0.1202</td>\n",
              "      <td>0.2306</td>\n",
              "      <td>0.1275</td>\n",
              "      <td>0.0894</td>\n",
              "      <td>0.0283</td>\n",
              "      <td>0.6822</td>\n",
              "      <td>0.8482</td>\n",
              "      <td>0.9213</td>\n",
              "      <td>0.6443</td>\n",
              "      <td>0.8621</td>\n",
              "      <td>0.5990</td>\n",
              "      <td>0.5358</td>\n",
              "      <td>0.4056</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>14585</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6852</th>\n",
              "      <td>0.4372</td>\n",
              "      <td>0.0126</td>\n",
              "      <td>0.0997</td>\n",
              "      <td>0.1584</td>\n",
              "      <td>0.2314</td>\n",
              "      <td>0.1791</td>\n",
              "      <td>0.1352</td>\n",
              "      <td>0.0967</td>\n",
              "      <td>0.0666</td>\n",
              "      <td>0.0329</td>\n",
              "      <td>0.8327</td>\n",
              "      <td>0.7383</td>\n",
              "      <td>0.9272</td>\n",
              "      <td>0.6447</td>\n",
              "      <td>0.7543</td>\n",
              "      <td>0.6988</td>\n",
              "      <td>0.4780</td>\n",
              "      <td>0.4295</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>6215</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6853</th>\n",
              "      <td>0.5170</td>\n",
              "      <td>0.0741</td>\n",
              "      <td>0.1017</td>\n",
              "      <td>0.1393</td>\n",
              "      <td>0.2065</td>\n",
              "      <td>0.1590</td>\n",
              "      <td>0.2016</td>\n",
              "      <td>0.1005</td>\n",
              "      <td>0.0726</td>\n",
              "      <td>0.0187</td>\n",
              "      <td>0.4563</td>\n",
              "      <td>0.6130</td>\n",
              "      <td>0.8170</td>\n",
              "      <td>0.5522</td>\n",
              "      <td>0.7546</td>\n",
              "      <td>0.8028</td>\n",
              "      <td>0.2355</td>\n",
              "      <td>0.4360</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>6032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6854</th>\n",
              "      <td>0.5170</td>\n",
              "      <td>0.0741</td>\n",
              "      <td>0.1017</td>\n",
              "      <td>0.1393</td>\n",
              "      <td>0.2065</td>\n",
              "      <td>0.1590</td>\n",
              "      <td>0.2016</td>\n",
              "      <td>0.1005</td>\n",
              "      <td>0.0726</td>\n",
              "      <td>0.0187</td>\n",
              "      <td>0.4563</td>\n",
              "      <td>0.6130</td>\n",
              "      <td>0.8170</td>\n",
              "      <td>0.5522</td>\n",
              "      <td>0.7546</td>\n",
              "      <td>0.8028</td>\n",
              "      <td>0.2355</td>\n",
              "      <td>0.4360</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6855</th>\n",
              "      <td>0.6017</td>\n",
              "      <td>0.2127</td>\n",
              "      <td>0.2118</td>\n",
              "      <td>0.2063</td>\n",
              "      <td>0.3310</td>\n",
              "      <td>0.1857</td>\n",
              "      <td>0.0592</td>\n",
              "      <td>0.0059</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.4118</td>\n",
              "      <td>0.4815</td>\n",
              "      <td>0.0641</td>\n",
              "      <td>0.3335</td>\n",
              "      <td>0.3889</td>\n",
              "      <td>0.9083</td>\n",
              "      <td>0.2268</td>\n",
              "      <td>0.3620</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>5549</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6856</th>\n",
              "      <td>0.4746</td>\n",
              "      <td>0.0465</td>\n",
              "      <td>0.0481</td>\n",
              "      <td>0.1957</td>\n",
              "      <td>0.0984</td>\n",
              "      <td>0.2853</td>\n",
              "      <td>0.1582</td>\n",
              "      <td>0.1040</td>\n",
              "      <td>0.0758</td>\n",
              "      <td>0.0345</td>\n",
              "      <td>0.7307</td>\n",
              "      <td>0.7183</td>\n",
              "      <td>0.7561</td>\n",
              "      <td>0.6108</td>\n",
              "      <td>0.8217</td>\n",
              "      <td>0.7468</td>\n",
              "      <td>0.2996</td>\n",
              "      <td>0.2104</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>5529</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6857 rows Ã— 25 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      malepct  unemp  age16to24  age25to34  ...    TPP  VISION  VISION (I)  numpats\n",
              "0      0.4785 0.1083     0.0587     0.1818  ... 1.0000  0.0000      0.0000     4088\n",
              "1      0.4745 0.1062     0.1468     0.1393  ... 1.0000  0.0000      0.0000    19599\n",
              "2      0.4045 0.0364     0.0670     0.1258  ... 1.0000  0.0000      0.0000    10606\n",
              "3      0.3981 0.0210     0.1016     0.0740  ... 1.0000  0.0000      0.0000     8047\n",
              "4      0.4675 0.0345     0.1080     0.1818  ... 1.0000  0.0000      0.0000    14585\n",
              "...       ...    ...        ...        ...  ...    ...     ...         ...      ...\n",
              "6852   0.4372 0.0126     0.0997     0.1584  ... 1.0000  0.0000      0.0000     6215\n",
              "6853   0.5170 0.0741     0.1017     0.1393  ... 1.0000  0.0000      0.0000     6032\n",
              "6854   0.5170 0.0741     0.1017     0.1393  ... 0.0000  0.0000      0.0000      135\n",
              "6855   0.6017 0.2127     0.2118     0.2063  ... 0.0000  0.0000      0.0000     5549\n",
              "6856   0.4746 0.0465     0.0481     0.1957  ... 1.0000  0.0000      0.0000     5529\n",
              "\n",
              "[6857 rows x 25 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0M6uci6s1le",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "outputId": "23b4f2c1-2589-4e48-b579-7496be46cf5e"
      },
      "source": [
        "df_features.describe()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>malepct</th>\n",
              "      <th>unemp</th>\n",
              "      <th>age16to24</th>\n",
              "      <th>age25to34</th>\n",
              "      <th>age35to44</th>\n",
              "      <th>age45to54</th>\n",
              "      <th>age55to64</th>\n",
              "      <th>age65to74</th>\n",
              "      <th>age75to84</th>\n",
              "      <th>age85plus</th>\n",
              "      <th>phoneeasy</th>\n",
              "      <th>onlineasy</th>\n",
              "      <th>race</th>\n",
              "      <th>longstdhealth</th>\n",
              "      <th>canmngownhealth</th>\n",
              "      <th>reducedability</th>\n",
              "      <th>prefgpalways</th>\n",
              "      <th>bcaaware</th>\n",
              "      <th>EMIS</th>\n",
              "      <th>EMIS (I)</th>\n",
              "      <th>MICROTEST</th>\n",
              "      <th>TPP</th>\n",
              "      <th>VISION</th>\n",
              "      <th>VISION (I)</th>\n",
              "      <th>numpats</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>6857.0000</td>\n",
              "      <td>6857.0000</td>\n",
              "      <td>6857.0000</td>\n",
              "      <td>6857.0000</td>\n",
              "      <td>6857.0000</td>\n",
              "      <td>6857.0000</td>\n",
              "      <td>6857.0000</td>\n",
              "      <td>6857.0000</td>\n",
              "      <td>6857.0000</td>\n",
              "      <td>6857.0000</td>\n",
              "      <td>6857.0000</td>\n",
              "      <td>6857.0000</td>\n",
              "      <td>6857.0000</td>\n",
              "      <td>6857.0000</td>\n",
              "      <td>6857.0000</td>\n",
              "      <td>6857.0000</td>\n",
              "      <td>6857.0000</td>\n",
              "      <td>6857.0000</td>\n",
              "      <td>6857.0000</td>\n",
              "      <td>6857.0000</td>\n",
              "      <td>6857.0000</td>\n",
              "      <td>6857.0000</td>\n",
              "      <td>6857.0000</td>\n",
              "      <td>6857.0000</td>\n",
              "      <td>6857.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.4940</td>\n",
              "      <td>0.0426</td>\n",
              "      <td>0.1052</td>\n",
              "      <td>0.1655</td>\n",
              "      <td>0.1665</td>\n",
              "      <td>0.1777</td>\n",
              "      <td>0.1581</td>\n",
              "      <td>0.1259</td>\n",
              "      <td>0.0730</td>\n",
              "      <td>0.0288</td>\n",
              "      <td>0.7192</td>\n",
              "      <td>0.7700</td>\n",
              "      <td>0.7417</td>\n",
              "      <td>0.5222</td>\n",
              "      <td>0.8292</td>\n",
              "      <td>0.5984</td>\n",
              "      <td>0.4958</td>\n",
              "      <td>0.4091</td>\n",
              "      <td>0.5762</td>\n",
              "      <td>0.0074</td>\n",
              "      <td>0.0064</td>\n",
              "      <td>0.3675</td>\n",
              "      <td>0.0420</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>8602.8997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.0580</td>\n",
              "      <td>0.0409</td>\n",
              "      <td>0.0604</td>\n",
              "      <td>0.0765</td>\n",
              "      <td>0.0614</td>\n",
              "      <td>0.0432</td>\n",
              "      <td>0.0441</td>\n",
              "      <td>0.0482</td>\n",
              "      <td>0.0330</td>\n",
              "      <td>0.0176</td>\n",
              "      <td>0.1917</td>\n",
              "      <td>0.1238</td>\n",
              "      <td>0.2623</td>\n",
              "      <td>0.0868</td>\n",
              "      <td>0.0837</td>\n",
              "      <td>0.0972</td>\n",
              "      <td>0.1866</td>\n",
              "      <td>0.1618</td>\n",
              "      <td>0.4942</td>\n",
              "      <td>0.0859</td>\n",
              "      <td>0.0799</td>\n",
              "      <td>0.4822</td>\n",
              "      <td>0.2006</td>\n",
              "      <td>0.0209</td>\n",
              "      <td>5505.2618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.2478</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0615</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.1591</td>\n",
              "      <td>0.2031</td>\n",
              "      <td>0.2624</td>\n",
              "      <td>0.0214</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>52.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.4563</td>\n",
              "      <td>0.0136</td>\n",
              "      <td>0.0678</td>\n",
              "      <td>0.1113</td>\n",
              "      <td>0.1225</td>\n",
              "      <td>0.1496</td>\n",
              "      <td>0.1309</td>\n",
              "      <td>0.0935</td>\n",
              "      <td>0.0496</td>\n",
              "      <td>0.0170</td>\n",
              "      <td>0.5886</td>\n",
              "      <td>0.6969</td>\n",
              "      <td>0.6037</td>\n",
              "      <td>0.4667</td>\n",
              "      <td>0.7809</td>\n",
              "      <td>0.5309</td>\n",
              "      <td>0.3590</td>\n",
              "      <td>0.2928</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>4789.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.4920</td>\n",
              "      <td>0.0319</td>\n",
              "      <td>0.0975</td>\n",
              "      <td>0.1550</td>\n",
              "      <td>0.1597</td>\n",
              "      <td>0.1772</td>\n",
              "      <td>0.1606</td>\n",
              "      <td>0.1267</td>\n",
              "      <td>0.0727</td>\n",
              "      <td>0.0269</td>\n",
              "      <td>0.7480</td>\n",
              "      <td>0.7867</td>\n",
              "      <td>0.8577</td>\n",
              "      <td>0.5268</td>\n",
              "      <td>0.8404</td>\n",
              "      <td>0.5969</td>\n",
              "      <td>0.4855</td>\n",
              "      <td>0.4092</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>7561.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.5286</td>\n",
              "      <td>0.0609</td>\n",
              "      <td>0.1310</td>\n",
              "      <td>0.2071</td>\n",
              "      <td>0.2045</td>\n",
              "      <td>0.2047</td>\n",
              "      <td>0.1864</td>\n",
              "      <td>0.1585</td>\n",
              "      <td>0.0947</td>\n",
              "      <td>0.0385</td>\n",
              "      <td>0.8760</td>\n",
              "      <td>0.8599</td>\n",
              "      <td>0.9417</td>\n",
              "      <td>0.5816</td>\n",
              "      <td>0.8904</td>\n",
              "      <td>0.6646</td>\n",
              "      <td>0.6276</td>\n",
              "      <td>0.5243</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>11106.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.8427</td>\n",
              "      <td>0.5951</td>\n",
              "      <td>0.9073</td>\n",
              "      <td>0.6937</td>\n",
              "      <td>0.4442</td>\n",
              "      <td>0.3514</td>\n",
              "      <td>0.3203</td>\n",
              "      <td>0.3095</td>\n",
              "      <td>0.2333</td>\n",
              "      <td>0.5815</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.9802</td>\n",
              "      <td>0.9586</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>73488.0000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        malepct     unemp  age16to24  ...    VISION  VISION (I)    numpats\n",
              "count 6857.0000 6857.0000  6857.0000  ... 6857.0000   6857.0000  6857.0000\n",
              "mean     0.4940    0.0426     0.1052  ...    0.0420      0.0004  8602.8997\n",
              "std      0.0580    0.0409     0.0604  ...    0.2006      0.0209  5505.2618\n",
              "min      0.2478    0.0000     0.0000  ...    0.0000      0.0000    52.0000\n",
              "25%      0.4563    0.0136     0.0678  ...    0.0000      0.0000  4789.0000\n",
              "50%      0.4920    0.0319     0.0975  ...    0.0000      0.0000  7561.0000\n",
              "75%      0.5286    0.0609     0.1310  ...    0.0000      0.0000 11106.0000\n",
              "max      0.8427    0.5951     0.9073  ...    1.0000      1.0000 73488.0000\n",
              "\n",
              "[8 rows x 25 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLn9pg12tFVP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "b3eb791f-33d4-4919-9ef8-fafd752e1d6b"
      },
      "source": [
        "# Splitiing the data\n",
        "train_x,valid_x,train_y,valid_y=train_test_split(df_features,df_target,test_size=0.4,random_state=1)\n",
        "print('train x shape :', train_x.shape)\n",
        "print('train y shape :', train_y.shape)\n",
        "print('valid x shape :', valid_x.shape)\n",
        "print('valid y shape :', valid_y.shape)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train x shape : (4114, 25)\n",
            "train y shape : (4114,)\n",
            "valid x shape : (2743, 25)\n",
            "valid y shape : (2743,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4O-vnGbE1bJy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Standardizing the data\n",
        "scaler = StandardScaler().fit(train_x.values)\n",
        "scaledf = scaler.transform(train_x.values)\n",
        "train_x = pd.DataFrame(scaledf, index=train_x.index, columns=train_x.columns)\n",
        "\n",
        "\n",
        "scaler2 = StandardScaler().fit(valid_x.values)\n",
        "vscaled = scaler2.transform(valid_x.values)\n",
        "valid_x = pd.DataFrame(vscaled, index=valid_x.index, columns=valid_x.columns)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kYh0Q0F1pyv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        },
        "outputId": "ad212595-1593-43d3-806b-9cc79f1052f3"
      },
      "source": [
        "print('Training sample summary')\n",
        "display.display(train_x.describe())\n",
        "print('Valid sample summary')\n",
        "display.display(valid_x.describe())"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training sample summary\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>malepct</th>\n",
              "      <th>unemp</th>\n",
              "      <th>age16to24</th>\n",
              "      <th>age25to34</th>\n",
              "      <th>age35to44</th>\n",
              "      <th>age45to54</th>\n",
              "      <th>age55to64</th>\n",
              "      <th>age65to74</th>\n",
              "      <th>age75to84</th>\n",
              "      <th>age85plus</th>\n",
              "      <th>phoneeasy</th>\n",
              "      <th>onlineasy</th>\n",
              "      <th>race</th>\n",
              "      <th>longstdhealth</th>\n",
              "      <th>canmngownhealth</th>\n",
              "      <th>reducedability</th>\n",
              "      <th>prefgpalways</th>\n",
              "      <th>bcaaware</th>\n",
              "      <th>EMIS</th>\n",
              "      <th>EMIS (I)</th>\n",
              "      <th>MICROTEST</th>\n",
              "      <th>TPP</th>\n",
              "      <th>VISION</th>\n",
              "      <th>VISION (I)</th>\n",
              "      <th>numpats</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>4114.0000</td>\n",
              "      <td>4114.0000</td>\n",
              "      <td>4114.0000</td>\n",
              "      <td>4114.0000</td>\n",
              "      <td>4114.0000</td>\n",
              "      <td>4114.0000</td>\n",
              "      <td>4114.0000</td>\n",
              "      <td>4114.0000</td>\n",
              "      <td>4114.0000</td>\n",
              "      <td>4114.0000</td>\n",
              "      <td>4114.0000</td>\n",
              "      <td>4114.0000</td>\n",
              "      <td>4114.0000</td>\n",
              "      <td>4114.0000</td>\n",
              "      <td>4114.0000</td>\n",
              "      <td>4114.0000</td>\n",
              "      <td>4114.0000</td>\n",
              "      <td>4114.0000</td>\n",
              "      <td>4114.0000</td>\n",
              "      <td>4114.0000</td>\n",
              "      <td>4114.0000</td>\n",
              "      <td>4114.0000</td>\n",
              "      <td>4114.0000</td>\n",
              "      <td>4114.0000</td>\n",
              "      <td>4114.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>-0.0000</td>\n",
              "      <td>-0.0000</td>\n",
              "      <td>-0.0000</td>\n",
              "      <td>-0.0000</td>\n",
              "      <td>-0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>-0.0000</td>\n",
              "      <td>-0.0000</td>\n",
              "      <td>-0.0000</td>\n",
              "      <td>-0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>-0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>-0.0000</td>\n",
              "      <td>-0.0000</td>\n",
              "      <td>-0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>-0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>-0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.0001</td>\n",
              "      <td>1.0001</td>\n",
              "      <td>1.0001</td>\n",
              "      <td>1.0001</td>\n",
              "      <td>1.0001</td>\n",
              "      <td>1.0001</td>\n",
              "      <td>1.0001</td>\n",
              "      <td>1.0001</td>\n",
              "      <td>1.0001</td>\n",
              "      <td>1.0001</td>\n",
              "      <td>1.0001</td>\n",
              "      <td>1.0001</td>\n",
              "      <td>1.0001</td>\n",
              "      <td>1.0001</td>\n",
              "      <td>1.0001</td>\n",
              "      <td>1.0001</td>\n",
              "      <td>1.0001</td>\n",
              "      <td>1.0001</td>\n",
              "      <td>1.0001</td>\n",
              "      <td>1.0001</td>\n",
              "      <td>1.0001</td>\n",
              "      <td>1.0001</td>\n",
              "      <td>1.0001</td>\n",
              "      <td>1.0001</td>\n",
              "      <td>1.0001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-4.2183</td>\n",
              "      <td>-1.0090</td>\n",
              "      <td>-1.7347</td>\n",
              "      <td>-2.1335</td>\n",
              "      <td>-2.7345</td>\n",
              "      <td>-4.0969</td>\n",
              "      <td>-3.5833</td>\n",
              "      <td>-2.5913</td>\n",
              "      <td>-2.1995</td>\n",
              "      <td>-1.5462</td>\n",
              "      <td>-3.2164</td>\n",
              "      <td>-6.2258</td>\n",
              "      <td>-2.8214</td>\n",
              "      <td>-4.1702</td>\n",
              "      <td>-7.3560</td>\n",
              "      <td>-3.1134</td>\n",
              "      <td>-2.5484</td>\n",
              "      <td>-2.4538</td>\n",
              "      <td>-1.1815</td>\n",
              "      <td>-0.0843</td>\n",
              "      <td>-0.0828</td>\n",
              "      <td>-0.7539</td>\n",
              "      <td>-0.2063</td>\n",
              "      <td>-0.0156</td>\n",
              "      <td>-1.5847</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>-0.6646</td>\n",
              "      <td>-0.6871</td>\n",
              "      <td>-0.6102</td>\n",
              "      <td>-0.7136</td>\n",
              "      <td>-0.7160</td>\n",
              "      <td>-0.6486</td>\n",
              "      <td>-0.6080</td>\n",
              "      <td>-0.6658</td>\n",
              "      <td>-0.7071</td>\n",
              "      <td>-0.6356</td>\n",
              "      <td>-0.6687</td>\n",
              "      <td>-0.5897</td>\n",
              "      <td>-0.5126</td>\n",
              "      <td>-0.6346</td>\n",
              "      <td>-0.5534</td>\n",
              "      <td>-0.7045</td>\n",
              "      <td>-0.7251</td>\n",
              "      <td>-0.7198</td>\n",
              "      <td>-1.1815</td>\n",
              "      <td>-0.0843</td>\n",
              "      <td>-0.0828</td>\n",
              "      <td>-0.7539</td>\n",
              "      <td>-0.2063</td>\n",
              "      <td>-0.0156</td>\n",
              "      <td>-0.6946</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>-0.0387</td>\n",
              "      <td>-0.2603</td>\n",
              "      <td>-0.1298</td>\n",
              "      <td>-0.1442</td>\n",
              "      <td>-0.1164</td>\n",
              "      <td>-0.0040</td>\n",
              "      <td>0.0627</td>\n",
              "      <td>0.0218</td>\n",
              "      <td>0.0021</td>\n",
              "      <td>-0.1058</td>\n",
              "      <td>0.1504</td>\n",
              "      <td>0.1268</td>\n",
              "      <td>0.4386</td>\n",
              "      <td>0.0495</td>\n",
              "      <td>0.1360</td>\n",
              "      <td>-0.0237</td>\n",
              "      <td>-0.0541</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.8464</td>\n",
              "      <td>-0.0843</td>\n",
              "      <td>-0.0828</td>\n",
              "      <td>-0.7539</td>\n",
              "      <td>-0.2063</td>\n",
              "      <td>-0.0156</td>\n",
              "      <td>-0.1911</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.5919</td>\n",
              "      <td>0.4208</td>\n",
              "      <td>0.4212</td>\n",
              "      <td>0.5398</td>\n",
              "      <td>0.6308</td>\n",
              "      <td>0.6254</td>\n",
              "      <td>0.6381</td>\n",
              "      <td>0.6752</td>\n",
              "      <td>0.6606</td>\n",
              "      <td>0.5240</td>\n",
              "      <td>0.8126</td>\n",
              "      <td>0.7300</td>\n",
              "      <td>0.7589</td>\n",
              "      <td>0.6786</td>\n",
              "      <td>0.7315</td>\n",
              "      <td>0.6756</td>\n",
              "      <td>0.6982</td>\n",
              "      <td>0.7176</td>\n",
              "      <td>0.8464</td>\n",
              "      <td>-0.0843</td>\n",
              "      <td>-0.0828</td>\n",
              "      <td>1.3264</td>\n",
              "      <td>-0.2063</td>\n",
              "      <td>-0.0156</td>\n",
              "      <td>0.4706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>5.9848</td>\n",
              "      <td>12.9647</td>\n",
              "      <td>13.0894</td>\n",
              "      <td>6.8490</td>\n",
              "      <td>4.5458</td>\n",
              "      <td>4.0124</td>\n",
              "      <td>3.6854</td>\n",
              "      <td>3.7869</td>\n",
              "      <td>4.8306</td>\n",
              "      <td>29.8189</td>\n",
              "      <td>1.4654</td>\n",
              "      <td>1.8504</td>\n",
              "      <td>0.9779</td>\n",
              "      <td>5.4863</td>\n",
              "      <td>2.0249</td>\n",
              "      <td>4.0823</td>\n",
              "      <td>2.5768</td>\n",
              "      <td>2.9102</td>\n",
              "      <td>0.8464</td>\n",
              "      <td>11.8685</td>\n",
              "      <td>12.0801</td>\n",
              "      <td>1.3264</td>\n",
              "      <td>4.8465</td>\n",
              "      <td>64.1327</td>\n",
              "      <td>9.6764</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        malepct     unemp  age16to24  ...    VISION  VISION (I)   numpats\n",
              "count 4114.0000 4114.0000  4114.0000  ... 4114.0000   4114.0000 4114.0000\n",
              "mean    -0.0000   -0.0000    -0.0000  ...    0.0000     -0.0000    0.0000\n",
              "std      1.0001    1.0001     1.0001  ...    1.0001      1.0001    1.0001\n",
              "min     -4.2183   -1.0090    -1.7347  ...   -0.2063     -0.0156   -1.5847\n",
              "25%     -0.6646   -0.6871    -0.6102  ...   -0.2063     -0.0156   -0.6946\n",
              "50%     -0.0387   -0.2603    -0.1298  ...   -0.2063     -0.0156   -0.1911\n",
              "75%      0.5919    0.4208     0.4212  ...   -0.2063     -0.0156    0.4706\n",
              "max      5.9848   12.9647    13.0894  ...    4.8465     64.1327    9.6764\n",
              "\n",
              "[8 rows x 25 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Valid sample summary\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>malepct</th>\n",
              "      <th>unemp</th>\n",
              "      <th>age16to24</th>\n",
              "      <th>age25to34</th>\n",
              "      <th>age35to44</th>\n",
              "      <th>age45to54</th>\n",
              "      <th>age55to64</th>\n",
              "      <th>age65to74</th>\n",
              "      <th>age75to84</th>\n",
              "      <th>age85plus</th>\n",
              "      <th>phoneeasy</th>\n",
              "      <th>onlineasy</th>\n",
              "      <th>race</th>\n",
              "      <th>longstdhealth</th>\n",
              "      <th>canmngownhealth</th>\n",
              "      <th>reducedability</th>\n",
              "      <th>prefgpalways</th>\n",
              "      <th>bcaaware</th>\n",
              "      <th>EMIS</th>\n",
              "      <th>EMIS (I)</th>\n",
              "      <th>MICROTEST</th>\n",
              "      <th>TPP</th>\n",
              "      <th>VISION</th>\n",
              "      <th>VISION (I)</th>\n",
              "      <th>numpats</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2743.0000</td>\n",
              "      <td>2743.0000</td>\n",
              "      <td>2743.0000</td>\n",
              "      <td>2743.0000</td>\n",
              "      <td>2743.0000</td>\n",
              "      <td>2743.0000</td>\n",
              "      <td>2743.0000</td>\n",
              "      <td>2743.0000</td>\n",
              "      <td>2743.0000</td>\n",
              "      <td>2743.0000</td>\n",
              "      <td>2743.0000</td>\n",
              "      <td>2743.0000</td>\n",
              "      <td>2743.0000</td>\n",
              "      <td>2743.0000</td>\n",
              "      <td>2743.0000</td>\n",
              "      <td>2743.0000</td>\n",
              "      <td>2743.0000</td>\n",
              "      <td>2743.0000</td>\n",
              "      <td>2743.0000</td>\n",
              "      <td>2743.0000</td>\n",
              "      <td>2743.0000</td>\n",
              "      <td>2743.0000</td>\n",
              "      <td>2743.0000</td>\n",
              "      <td>2743.0000</td>\n",
              "      <td>2743.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>-0.0000</td>\n",
              "      <td>-0.0000</td>\n",
              "      <td>-0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>-0.0000</td>\n",
              "      <td>-0.0000</td>\n",
              "      <td>-0.0000</td>\n",
              "      <td>-0.0000</td>\n",
              "      <td>-0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>-0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>-0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>-0.0000</td>\n",
              "      <td>-0.0000</td>\n",
              "      <td>-0.0000</td>\n",
              "      <td>-0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.0002</td>\n",
              "      <td>1.0002</td>\n",
              "      <td>1.0002</td>\n",
              "      <td>1.0002</td>\n",
              "      <td>1.0002</td>\n",
              "      <td>1.0002</td>\n",
              "      <td>1.0002</td>\n",
              "      <td>1.0002</td>\n",
              "      <td>1.0002</td>\n",
              "      <td>1.0002</td>\n",
              "      <td>1.0002</td>\n",
              "      <td>1.0002</td>\n",
              "      <td>1.0002</td>\n",
              "      <td>1.0002</td>\n",
              "      <td>1.0002</td>\n",
              "      <td>1.0002</td>\n",
              "      <td>1.0002</td>\n",
              "      <td>1.0002</td>\n",
              "      <td>1.0002</td>\n",
              "      <td>1.0002</td>\n",
              "      <td>1.0002</td>\n",
              "      <td>1.0002</td>\n",
              "      <td>1.0002</td>\n",
              "      <td>1.0002</td>\n",
              "      <td>1.0002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-3.4497</td>\n",
              "      <td>-1.0952</td>\n",
              "      <td>-1.7579</td>\n",
              "      <td>-2.0377</td>\n",
              "      <td>-2.6858</td>\n",
              "      <td>-4.1297</td>\n",
              "      <td>-3.5877</td>\n",
              "      <td>-2.6397</td>\n",
              "      <td>-2.2357</td>\n",
              "      <td>-1.7808</td>\n",
              "      <td>-3.4480</td>\n",
              "      <td>-6.2129</td>\n",
              "      <td>-2.8153</td>\n",
              "      <td>-3.8895</td>\n",
              "      <td>-4.7009</td>\n",
              "      <td>-3.5253</td>\n",
              "      <td>-2.4592</td>\n",
              "      <td>-2.5109</td>\n",
              "      <td>-1.1432</td>\n",
              "      <td>-0.0899</td>\n",
              "      <td>-0.0766</td>\n",
              "      <td>-0.7748</td>\n",
              "      <td>-0.2139</td>\n",
              "      <td>-0.0270</td>\n",
              "      <td>-1.4715</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>-0.6248</td>\n",
              "      <td>-0.7425</td>\n",
              "      <td>-0.6245</td>\n",
              "      <td>-0.6904</td>\n",
              "      <td>-0.7118</td>\n",
              "      <td>-0.6475</td>\n",
              "      <td>-0.6316</td>\n",
              "      <td>-0.6770</td>\n",
              "      <td>-0.7118</td>\n",
              "      <td>-0.7079</td>\n",
              "      <td>-0.6933</td>\n",
              "      <td>-0.6056</td>\n",
              "      <td>-0.5432</td>\n",
              "      <td>-0.6554</td>\n",
              "      <td>-0.6159</td>\n",
              "      <td>-0.6747</td>\n",
              "      <td>-0.7393</td>\n",
              "      <td>-0.7133</td>\n",
              "      <td>-1.1432</td>\n",
              "      <td>-0.0899</td>\n",
              "      <td>-0.0766</td>\n",
              "      <td>-0.7748</td>\n",
              "      <td>-0.2139</td>\n",
              "      <td>-0.0270</td>\n",
              "      <td>-0.6917</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>-0.0285</td>\n",
              "      <td>-0.2632</td>\n",
              "      <td>-0.1391</td>\n",
              "      <td>-0.1222</td>\n",
              "      <td>-0.1027</td>\n",
              "      <td>-0.0180</td>\n",
              "      <td>0.0477</td>\n",
              "      <td>0.0080</td>\n",
              "      <td>-0.0160</td>\n",
              "      <td>-0.1025</td>\n",
              "      <td>0.1549</td>\n",
              "      <td>0.1474</td>\n",
              "      <td>0.4428</td>\n",
              "      <td>0.0596</td>\n",
              "      <td>0.1261</td>\n",
              "      <td>-0.0005</td>\n",
              "      <td>-0.0612</td>\n",
              "      <td>-0.0009</td>\n",
              "      <td>0.8747</td>\n",
              "      <td>-0.0899</td>\n",
              "      <td>-0.0766</td>\n",
              "      <td>-0.7748</td>\n",
              "      <td>-0.2139</td>\n",
              "      <td>-0.0270</td>\n",
              "      <td>-0.1865</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.6082</td>\n",
              "      <td>0.4985</td>\n",
              "      <td>0.4300</td>\n",
              "      <td>0.5466</td>\n",
              "      <td>0.6026</td>\n",
              "      <td>0.6302</td>\n",
              "      <td>0.6541</td>\n",
              "      <td>0.6836</td>\n",
              "      <td>0.6504</td>\n",
              "      <td>0.6009</td>\n",
              "      <td>0.8319</td>\n",
              "      <td>0.7128</td>\n",
              "      <td>0.7672</td>\n",
              "      <td>0.6935</td>\n",
              "      <td>0.7308</td>\n",
              "      <td>0.6847</td>\n",
              "      <td>0.7182</td>\n",
              "      <td>0.6984</td>\n",
              "      <td>0.8747</td>\n",
              "      <td>-0.0899</td>\n",
              "      <td>-0.0766</td>\n",
              "      <td>1.2906</td>\n",
              "      <td>-0.2139</td>\n",
              "      <td>-0.0270</td>\n",
              "      <td>0.4357</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>4.2397</td>\n",
              "      <td>9.1829</td>\n",
              "      <td>10.6173</td>\n",
              "      <td>6.1174</td>\n",
              "      <td>4.2074</td>\n",
              "      <td>3.9852</td>\n",
              "      <td>3.5084</td>\n",
              "      <td>3.2716</td>\n",
              "      <td>3.7835</td>\n",
              "      <td>4.6007</td>\n",
              "      <td>1.4642</td>\n",
              "      <td>1.8699</td>\n",
              "      <td>0.9951</td>\n",
              "      <td>4.5458</td>\n",
              "      <td>2.0691</td>\n",
              "      <td>4.2141</td>\n",
              "      <td>2.6081</td>\n",
              "      <td>3.4222</td>\n",
              "      <td>0.8747</td>\n",
              "      <td>11.1212</td>\n",
              "      <td>13.0552</td>\n",
              "      <td>1.2906</td>\n",
              "      <td>4.6753</td>\n",
              "      <td>37.0203</td>\n",
              "      <td>11.4262</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        malepct     unemp  age16to24  ...    VISION  VISION (I)   numpats\n",
              "count 2743.0000 2743.0000  2743.0000  ... 2743.0000   2743.0000 2743.0000\n",
              "mean     0.0000    0.0000     0.0000  ...   -0.0000     -0.0000    0.0000\n",
              "std      1.0002    1.0002     1.0002  ...    1.0002      1.0002    1.0002\n",
              "min     -3.4497   -1.0952    -1.7579  ...   -0.2139     -0.0270   -1.4715\n",
              "25%     -0.6248   -0.7425    -0.6245  ...   -0.2139     -0.0270   -0.6917\n",
              "50%     -0.0285   -0.2632    -0.1391  ...   -0.2139     -0.0270   -0.1865\n",
              "75%      0.6082    0.4985     0.4300  ...   -0.2139     -0.0270    0.4357\n",
              "max      4.2397    9.1829    10.6173  ...    4.6753     37.0203   11.4262\n",
              "\n",
              "[8 rows x 25 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNHfuD-91qyc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "b4e7e1dd-e66a-46e4-ca7c-7465592f6dfe"
      },
      "source": [
        "print('Training target sample summary')\n",
        "display.display(train_y.describe())\n",
        "print('Validation target sample summary')\n",
        "display.display(valid_y.describe())"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training target sample summary\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "count   4114.0000\n",
              "mean       0.1369\n",
              "std        0.0875\n",
              "min        0.0000\n",
              "25%        0.0716\n",
              "50%        0.1213\n",
              "75%        0.1864\n",
              "max        0.6521\n",
              "Name: OnlineAppointmentUse, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Validation target sample summary\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "count   2743.0000\n",
              "mean       0.1373\n",
              "std        0.0884\n",
              "min        0.0000\n",
              "25%        0.0720\n",
              "50%        0.1221\n",
              "75%        0.1849\n",
              "max        0.6674\n",
              "Name: OnlineAppointmentUse, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwaU3vI6Fym8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 540
        },
        "outputId": "7ec15546-39db-4e64-af59-1cb58ba7f322"
      },
      "source": [
        "!pip install -U keras-tuner"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras-tuner\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a7/f7/4b41b6832abf4c9bef71a664dc563adb25afc5812831667c6db572b1a261/keras-tuner-1.0.1.tar.gz (54kB)\n",
            "\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                          | 10kB 15.7MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 20kB 2.1MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ              | 30kB 2.7MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        | 40kB 3.0MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 51kB 2.5MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61kB 2.1MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: tabulate in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (0.8.7)\n",
            "Collecting terminaltables\n",
            "  Downloading https://files.pythonhosted.org/packages/9b/c4/4a21174f32f8a7e1104798c445dacdc1d4df86f2f26722767034e4de4bff/terminaltables-3.1.0.tar.gz\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/c9/dc/45cdef1b4d119eb96316b3117e6d5708a08029992b2fee2c143c7a0a5cc5/colorama-0.4.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (0.22.2.post1)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->keras-tuner) (0.16.0)\n",
            "Building wheels for collected packages: keras-tuner, terminaltables\n",
            "  Building wheel for keras-tuner (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-tuner: filename=keras_tuner-1.0.1-cp36-none-any.whl size=73200 sha256=e44950dd200bd903f5aab0e620fbd4d5092b69377546f0583b2566587b95b978\n",
            "  Stored in directory: /root/.cache/pip/wheels/b9/cc/62/52716b70dd90f3db12519233c3a93a5360bc672da1a10ded43\n",
            "  Building wheel for terminaltables (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for terminaltables: filename=terminaltables-3.1.0-cp36-none-any.whl size=15356 sha256=34417fa3355c6aa5f27bffab59d73aec5231106df23f7f4ca51973a298fb7ab4\n",
            "  Stored in directory: /root/.cache/pip/wheels/30/6b/50/6c75775b681fb36cdfac7f19799888ef9d8813aff9e379663e\n",
            "Successfully built keras-tuner terminaltables\n",
            "Installing collected packages: terminaltables, colorama, keras-tuner\n",
            "Successfully installed colorama-0.4.3 keras-tuner-1.0.1 terminaltables-3.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccXolzUfeQyw",
        "colab_type": "text"
      },
      "source": [
        "### Build Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLZaXwKOFxJW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Using keras tuner to build a model\n",
        "from tensorflow.keras import layers\n",
        "from kerastuner.tuners import RandomSearch\n",
        "\n",
        "def build_model(hp):\n",
        "    model = keras.Sequential()\n",
        "    for i in range(hp.Int('num_layers', 1, 4)):\n",
        "        model.add(layers.Dense(units=hp.Int('units_' + str(i),\n",
        "                                            min_value=8,\n",
        "                                            max_value=32,\n",
        "                                            step=8),\n",
        "                               activation='relu'))\n",
        "    model.add(layers.Dense(1))\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(\n",
        "            hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),\n",
        "        loss='mse',\n",
        "        metrics=['mae'])\n",
        "    return model\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdDKJdKXHPLg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tuner = RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_mae',\n",
        "    max_trials=4,\n",
        "    executions_per_trial=2,\n",
        "    directory='my_dir',\n",
        "    project_name='Online_appointment')"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQ_s_GWxekFw",
        "colab_type": "text"
      },
      "source": [
        "### Fit Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhWAEwPqHfDj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d364aac8-bfd3-4dcc-a486-1531863f7554"
      },
      "source": [
        "tuner.search(train_x.values, train_y.values,\n",
        "             epochs=100,\n",
        "             validation_data= (valid_x, valid_y))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "129/129 [==============================] - 0s 3ms/step - loss: 0.0300 - mae: 0.0849 - val_loss: 0.0223 - val_mae: 0.0610\n",
            "Epoch 2/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0199 - mae: 0.0527 - val_loss: 0.0039 - val_mae: 0.0436\n",
            "Epoch 3/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0032 - mae: 0.0427 - val_loss: 0.0059 - val_mae: 0.0422\n",
            "Epoch 4/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0081 - mae: 0.0449 - val_loss: 0.0050 - val_mae: 0.0430\n",
            "Epoch 5/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0047 - mae: 0.0430 - val_loss: 0.0031 - val_mae: 0.0428\n",
            "Epoch 6/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0029 - mae: 0.0404 - val_loss: 0.0029 - val_mae: 0.0411\n",
            "Epoch 7/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0028 - mae: 0.0402 - val_loss: 0.0026 - val_mae: 0.0389\n",
            "Epoch 8/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0027 - mae: 0.0395 - val_loss: 0.0026 - val_mae: 0.0388\n",
            "Epoch 9/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0395 - val_loss: 0.0025 - val_mae: 0.0387\n",
            "Epoch 10/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0392 - val_loss: 0.0026 - val_mae: 0.0389\n",
            "Epoch 11/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0027 - mae: 0.0397 - val_loss: 0.0025 - val_mae: 0.0387\n",
            "Epoch 12/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0394 - val_loss: 0.0025 - val_mae: 0.0386\n",
            "Epoch 13/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0027 - mae: 0.0396 - val_loss: 0.0027 - val_mae: 0.0400\n",
            "Epoch 14/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0393 - val_loss: 0.0026 - val_mae: 0.0383\n",
            "Epoch 15/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0391 - val_loss: 0.0026 - val_mae: 0.0390\n",
            "Epoch 16/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0390 - val_loss: 0.0025 - val_mae: 0.0382\n",
            "Epoch 17/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0387 - val_loss: 0.0029 - val_mae: 0.0409\n",
            "Epoch 18/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0389 - val_loss: 0.0030 - val_mae: 0.0414\n",
            "Epoch 19/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0389 - val_loss: 0.0027 - val_mae: 0.0391\n",
            "Epoch 20/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0388 - val_loss: 0.0025 - val_mae: 0.0382\n",
            "Epoch 21/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0387 - val_loss: 0.0025 - val_mae: 0.0385\n",
            "Epoch 22/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0388 - val_loss: 0.0025 - val_mae: 0.0379\n",
            "Epoch 23/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0389 - val_loss: 0.0027 - val_mae: 0.0391\n",
            "Epoch 24/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0388 - val_loss: 0.0028 - val_mae: 0.0409\n",
            "Epoch 25/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0388 - val_loss: 0.0026 - val_mae: 0.0388\n",
            "Epoch 26/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0384 - val_loss: 0.0026 - val_mae: 0.0387\n",
            "Epoch 27/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0383 - val_loss: 0.0026 - val_mae: 0.0389\n",
            "Epoch 28/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0384 - val_loss: 0.0025 - val_mae: 0.0383\n",
            "Epoch 29/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0387 - val_loss: 0.0026 - val_mae: 0.0392\n",
            "Epoch 30/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0384 - val_loss: 0.0027 - val_mae: 0.0394\n",
            "Epoch 31/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0384 - val_loss: 0.0027 - val_mae: 0.0403\n",
            "Epoch 32/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0382 - val_loss: 0.0028 - val_mae: 0.0403\n",
            "Epoch 33/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0390 - val_loss: 0.0025 - val_mae: 0.0384\n",
            "Epoch 34/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0392 - val_loss: 0.0025 - val_mae: 0.0384\n",
            "Epoch 35/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0386 - val_loss: 0.0027 - val_mae: 0.0400\n",
            "Epoch 36/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0385 - val_loss: 0.0025 - val_mae: 0.0384\n",
            "Epoch 37/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0386 - val_loss: 0.0025 - val_mae: 0.0380\n",
            "Epoch 38/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0388 - val_loss: 0.0026 - val_mae: 0.0397\n",
            "Epoch 39/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0393 - val_loss: 0.0026 - val_mae: 0.0386\n",
            "Epoch 40/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0389 - val_loss: 0.0025 - val_mae: 0.0388\n",
            "Epoch 41/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0389 - val_loss: 0.0026 - val_mae: 0.0386\n",
            "Epoch 42/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0384 - val_loss: 0.0027 - val_mae: 0.0404\n",
            "Epoch 43/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0387 - val_loss: 0.0026 - val_mae: 0.0384\n",
            "Epoch 44/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0031 - mae: 0.0412 - val_loss: 0.0026 - val_mae: 0.0391\n",
            "Epoch 45/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0385 - val_loss: 0.0025 - val_mae: 0.0384\n",
            "Epoch 46/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0389 - val_loss: 0.0028 - val_mae: 0.0395\n",
            "Epoch 47/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0382 - val_loss: 0.0027 - val_mae: 0.0406\n",
            "Epoch 48/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0383 - val_loss: 0.0027 - val_mae: 0.0401\n",
            "Epoch 49/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0381 - val_loss: 0.0027 - val_mae: 0.0393\n",
            "Epoch 50/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0387 - val_loss: 0.0028 - val_mae: 0.0417\n",
            "Epoch 51/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0380 - val_loss: 0.0026 - val_mae: 0.0393\n",
            "Epoch 52/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0388 - val_loss: 0.0026 - val_mae: 0.0392\n",
            "Epoch 53/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0383 - val_loss: 0.0027 - val_mae: 0.0391\n",
            "Epoch 54/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0385 - val_loss: 0.0029 - val_mae: 0.0400\n",
            "Epoch 55/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0024 - mae: 0.0379 - val_loss: 0.0028 - val_mae: 0.0409\n",
            "Epoch 56/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0384 - val_loss: 0.0029 - val_mae: 0.0410\n",
            "Epoch 57/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0024 - mae: 0.0376 - val_loss: 0.0027 - val_mae: 0.0394\n",
            "Epoch 58/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0382 - val_loss: 0.0030 - val_mae: 0.0407\n",
            "Epoch 59/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0382 - val_loss: 0.0028 - val_mae: 0.0410\n",
            "Epoch 60/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0388 - val_loss: 0.0027 - val_mae: 0.0397\n",
            "Epoch 61/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0383 - val_loss: 0.0029 - val_mae: 0.0411\n",
            "Epoch 62/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0024 - mae: 0.0377 - val_loss: 0.0027 - val_mae: 0.0402\n",
            "Epoch 63/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0380 - val_loss: 0.0039 - val_mae: 0.0446\n",
            "Epoch 64/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0038 - mae: 0.0425 - val_loss: 0.0044 - val_mae: 0.0489\n",
            "Epoch 65/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0037 - mae: 0.0464 - val_loss: 0.0033 - val_mae: 0.0437\n",
            "Epoch 66/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0029 - mae: 0.0417 - val_loss: 0.0028 - val_mae: 0.0402\n",
            "Epoch 67/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0028 - mae: 0.0406 - val_loss: 0.0028 - val_mae: 0.0401\n",
            "Epoch 68/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0027 - mae: 0.0402 - val_loss: 0.0030 - val_mae: 0.0424\n",
            "Epoch 69/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0027 - mae: 0.0397 - val_loss: 0.0036 - val_mae: 0.0474\n",
            "Epoch 70/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0027 - mae: 0.0405 - val_loss: 0.0029 - val_mae: 0.0407\n",
            "Epoch 71/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0395 - val_loss: 0.0028 - val_mae: 0.0403\n",
            "Epoch 72/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0391 - val_loss: 0.0027 - val_mae: 0.0394\n",
            "Epoch 73/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0027 - mae: 0.0405 - val_loss: 0.0027 - val_mae: 0.0390\n",
            "Epoch 74/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0389 - val_loss: 0.0027 - val_mae: 0.0392\n",
            "Epoch 75/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0385 - val_loss: 0.0026 - val_mae: 0.0393\n",
            "Epoch 76/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0392 - val_loss: 0.0027 - val_mae: 0.0393\n",
            "Epoch 77/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0390 - val_loss: 0.0027 - val_mae: 0.0400\n",
            "Epoch 78/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0390 - val_loss: 0.0029 - val_mae: 0.0422\n",
            "Epoch 79/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0384 - val_loss: 0.0029 - val_mae: 0.0415\n",
            "Epoch 80/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0401 - val_loss: 0.0029 - val_mae: 0.0405\n",
            "Epoch 81/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0389 - val_loss: 0.0027 - val_mae: 0.0394\n",
            "Epoch 82/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0388 - val_loss: 0.0028 - val_mae: 0.0395\n",
            "Epoch 83/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0389 - val_loss: 0.0027 - val_mae: 0.0395\n",
            "Epoch 84/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0391 - val_loss: 0.0028 - val_mae: 0.0396\n",
            "Epoch 85/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0386 - val_loss: 0.0026 - val_mae: 0.0389\n",
            "Epoch 86/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0390 - val_loss: 0.0027 - val_mae: 0.0393\n",
            "Epoch 87/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0381 - val_loss: 0.0028 - val_mae: 0.0400\n",
            "Epoch 88/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0385 - val_loss: 0.0027 - val_mae: 0.0388\n",
            "Epoch 89/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0385 - val_loss: 0.0027 - val_mae: 0.0392\n",
            "Epoch 90/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0383 - val_loss: 0.0029 - val_mae: 0.0421\n",
            "Epoch 91/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0381 - val_loss: 0.0028 - val_mae: 0.0407\n",
            "Epoch 92/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0024 - mae: 0.0375 - val_loss: 0.0027 - val_mae: 0.0390\n",
            "Epoch 93/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0024 - mae: 0.0379 - val_loss: 0.0026 - val_mae: 0.0386\n",
            "Epoch 94/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0393 - val_loss: 0.0026 - val_mae: 0.0385\n",
            "Epoch 95/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0024 - mae: 0.0377 - val_loss: 0.0027 - val_mae: 0.0399\n",
            "Epoch 96/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0024 - mae: 0.0377 - val_loss: 0.0027 - val_mae: 0.0397\n",
            "Epoch 97/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0024 - mae: 0.0383 - val_loss: 0.0028 - val_mae: 0.0403\n",
            "Epoch 98/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0384 - val_loss: 0.0030 - val_mae: 0.0425\n",
            "Epoch 99/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0385 - val_loss: 0.0033 - val_mae: 0.0434\n",
            "Epoch 100/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0383 - val_loss: 0.0028 - val_mae: 0.0404\n",
            "Epoch 1/100\n",
            "129/129 [==============================] - 0s 3ms/step - loss: 0.0191 - mae: 0.0763 - val_loss: 0.0083 - val_mae: 0.0483\n",
            "Epoch 2/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0078 - mae: 0.0450 - val_loss: 0.0029 - val_mae: 0.0415\n",
            "Epoch 3/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0030 - mae: 0.0418 - val_loss: 0.0030 - val_mae: 0.0424\n",
            "Epoch 4/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0028 - mae: 0.0410 - val_loss: 0.0028 - val_mae: 0.0407\n",
            "Epoch 5/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0028 - mae: 0.0408 - val_loss: 0.0027 - val_mae: 0.0399\n",
            "Epoch 6/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0027 - mae: 0.0402 - val_loss: 0.0027 - val_mae: 0.0399\n",
            "Epoch 7/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0027 - mae: 0.0402 - val_loss: 0.0027 - val_mae: 0.0396\n",
            "Epoch 8/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0027 - mae: 0.0398 - val_loss: 0.0027 - val_mae: 0.0395\n",
            "Epoch 9/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0028 - mae: 0.0404 - val_loss: 0.0028 - val_mae: 0.0402\n",
            "Epoch 10/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0027 - mae: 0.0397 - val_loss: 0.0029 - val_mae: 0.0409\n",
            "Epoch 11/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0028 - mae: 0.0405 - val_loss: 0.0027 - val_mae: 0.0404\n",
            "Epoch 12/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0027 - mae: 0.0398 - val_loss: 0.0029 - val_mae: 0.0414\n",
            "Epoch 13/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0027 - mae: 0.0393 - val_loss: 0.0027 - val_mae: 0.0395\n",
            "Epoch 14/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0028 - mae: 0.0401 - val_loss: 0.0028 - val_mae: 0.0414\n",
            "Epoch 15/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0027 - mae: 0.0394 - val_loss: 0.0030 - val_mae: 0.0413\n",
            "Epoch 16/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0027 - mae: 0.0398 - val_loss: 0.0026 - val_mae: 0.0392\n",
            "Epoch 17/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0395 - val_loss: 0.0028 - val_mae: 0.0401\n",
            "Epoch 18/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0027 - mae: 0.0402 - val_loss: 0.0026 - val_mae: 0.0399\n",
            "Epoch 19/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0391 - val_loss: 0.0028 - val_mae: 0.0408\n",
            "Epoch 20/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0027 - mae: 0.0396 - val_loss: 0.0029 - val_mae: 0.0416\n",
            "Epoch 21/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0027 - mae: 0.0398 - val_loss: 0.0028 - val_mae: 0.0403\n",
            "Epoch 22/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0390 - val_loss: 0.0027 - val_mae: 0.0394\n",
            "Epoch 23/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0389 - val_loss: 0.0027 - val_mae: 0.0401\n",
            "Epoch 24/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0027 - mae: 0.0396 - val_loss: 0.0027 - val_mae: 0.0402\n",
            "Epoch 25/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0391 - val_loss: 0.0027 - val_mae: 0.0405\n",
            "Epoch 26/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0027 - mae: 0.0398 - val_loss: 0.0027 - val_mae: 0.0397\n",
            "Epoch 27/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0395 - val_loss: 0.0028 - val_mae: 0.0397\n",
            "Epoch 28/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0392 - val_loss: 0.0027 - val_mae: 0.0401\n",
            "Epoch 29/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0391 - val_loss: 0.0026 - val_mae: 0.0392\n",
            "Epoch 30/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0390 - val_loss: 0.0035 - val_mae: 0.0408\n",
            "Epoch 31/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0052 - mae: 0.0501 - val_loss: 0.0050 - val_mae: 0.0504\n",
            "Epoch 32/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0038 - mae: 0.0460 - val_loss: 0.0033 - val_mae: 0.0432\n",
            "Epoch 33/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0030 - mae: 0.0417 - val_loss: 0.0029 - val_mae: 0.0401\n",
            "Epoch 34/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0029 - mae: 0.0413 - val_loss: 0.0031 - val_mae: 0.0427\n",
            "Epoch 35/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0029 - mae: 0.0417 - val_loss: 0.0028 - val_mae: 0.0405\n",
            "Epoch 36/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0027 - mae: 0.0400 - val_loss: 0.0030 - val_mae: 0.0415\n",
            "Epoch 37/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0028 - mae: 0.0406 - val_loss: 0.0041 - val_mae: 0.0516\n",
            "Epoch 38/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0027 - mae: 0.0399 - val_loss: 0.0027 - val_mae: 0.0391\n",
            "Epoch 39/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0394 - val_loss: 0.0028 - val_mae: 0.0406\n",
            "Epoch 40/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0389 - val_loss: 0.0028 - val_mae: 0.0398\n",
            "Epoch 41/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0394 - val_loss: 0.0033 - val_mae: 0.0449\n",
            "Epoch 42/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0393 - val_loss: 0.0027 - val_mae: 0.0388\n",
            "Epoch 43/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0388 - val_loss: 0.0027 - val_mae: 0.0387\n",
            "Epoch 44/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0386 - val_loss: 0.0028 - val_mae: 0.0397\n",
            "Epoch 45/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0392 - val_loss: 0.0028 - val_mae: 0.0398\n",
            "Epoch 46/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0383 - val_loss: 0.0027 - val_mae: 0.0388\n",
            "Epoch 47/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0388 - val_loss: 0.0026 - val_mae: 0.0387\n",
            "Epoch 48/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0388 - val_loss: 0.0028 - val_mae: 0.0396\n",
            "Epoch 49/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0388 - val_loss: 0.0027 - val_mae: 0.0387\n",
            "Epoch 50/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0389 - val_loss: 0.0026 - val_mae: 0.0382\n",
            "Epoch 51/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0384 - val_loss: 0.0027 - val_mae: 0.0391\n",
            "Epoch 52/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0387 - val_loss: 0.0033 - val_mae: 0.0433\n",
            "Epoch 53/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0393 - val_loss: 0.0027 - val_mae: 0.0393\n",
            "Epoch 54/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0386 - val_loss: 0.0029 - val_mae: 0.0410\n",
            "Epoch 55/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0391 - val_loss: 0.0026 - val_mae: 0.0384\n",
            "Epoch 56/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0384 - val_loss: 0.0031 - val_mae: 0.0424\n",
            "Epoch 57/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0389 - val_loss: 0.0028 - val_mae: 0.0408\n",
            "Epoch 58/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0384 - val_loss: 0.0026 - val_mae: 0.0390\n",
            "Epoch 59/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0386 - val_loss: 0.0027 - val_mae: 0.0394\n",
            "Epoch 60/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0389 - val_loss: 0.0028 - val_mae: 0.0398\n",
            "Epoch 61/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0382 - val_loss: 0.0026 - val_mae: 0.0388\n",
            "Epoch 62/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0394 - val_loss: 0.0028 - val_mae: 0.0401\n",
            "Epoch 63/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0386 - val_loss: 0.0027 - val_mae: 0.0394\n",
            "Epoch 64/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0389 - val_loss: 0.0027 - val_mae: 0.0394\n",
            "Epoch 65/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0383 - val_loss: 0.0028 - val_mae: 0.0394\n",
            "Epoch 66/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0385 - val_loss: 0.0027 - val_mae: 0.0398\n",
            "Epoch 67/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0383 - val_loss: 0.0027 - val_mae: 0.0399\n",
            "Epoch 68/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0386 - val_loss: 0.0028 - val_mae: 0.0396\n",
            "Epoch 69/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0383 - val_loss: 0.0026 - val_mae: 0.0388\n",
            "Epoch 70/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0383 - val_loss: 0.0026 - val_mae: 0.0386\n",
            "Epoch 71/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0383 - val_loss: 0.0029 - val_mae: 0.0416\n",
            "Epoch 72/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0383 - val_loss: 0.0028 - val_mae: 0.0402\n",
            "Epoch 73/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0389 - val_loss: 0.0027 - val_mae: 0.0396\n",
            "Epoch 74/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0384 - val_loss: 0.0028 - val_mae: 0.0394\n",
            "Epoch 75/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0384 - val_loss: 0.0028 - val_mae: 0.0414\n",
            "Epoch 76/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0383 - val_loss: 0.0028 - val_mae: 0.0395\n",
            "Epoch 77/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0024 - mae: 0.0376 - val_loss: 0.0027 - val_mae: 0.0395\n",
            "Epoch 78/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0390 - val_loss: 0.0027 - val_mae: 0.0395\n",
            "Epoch 79/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0390 - val_loss: 0.0026 - val_mae: 0.0385\n",
            "Epoch 80/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0024 - mae: 0.0377 - val_loss: 0.0028 - val_mae: 0.0401\n",
            "Epoch 81/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0024 - mae: 0.0377 - val_loss: 0.0028 - val_mae: 0.0405\n",
            "Epoch 82/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0024 - mae: 0.0377 - val_loss: 0.0027 - val_mae: 0.0395\n",
            "Epoch 83/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0024 - mae: 0.0379 - val_loss: 0.0027 - val_mae: 0.0391\n",
            "Epoch 84/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0381 - val_loss: 0.0029 - val_mae: 0.0413\n",
            "Epoch 85/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0389 - val_loss: 0.0027 - val_mae: 0.0392\n",
            "Epoch 86/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0024 - mae: 0.0378 - val_loss: 0.0029 - val_mae: 0.0419\n",
            "Epoch 87/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0024 - mae: 0.0376 - val_loss: 0.0029 - val_mae: 0.0405\n",
            "Epoch 88/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0024 - mae: 0.0380 - val_loss: 0.0027 - val_mae: 0.0395\n",
            "Epoch 89/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0024 - mae: 0.0375 - val_loss: 0.0027 - val_mae: 0.0387\n",
            "Epoch 90/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0382 - val_loss: 0.0027 - val_mae: 0.0386\n",
            "Epoch 91/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0024 - mae: 0.0375 - val_loss: 0.0029 - val_mae: 0.0419\n",
            "Epoch 92/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0024 - mae: 0.0381 - val_loss: 0.0028 - val_mae: 0.0403\n",
            "Epoch 93/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0024 - mae: 0.0380 - val_loss: 0.0028 - val_mae: 0.0407\n",
            "Epoch 94/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0024 - mae: 0.0380 - val_loss: 0.0027 - val_mae: 0.0392\n",
            "Epoch 95/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0024 - mae: 0.0372 - val_loss: 0.0029 - val_mae: 0.0403\n",
            "Epoch 96/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0024 - mae: 0.0380 - val_loss: 0.0031 - val_mae: 0.0429\n",
            "Epoch 97/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0024 - mae: 0.0374 - val_loss: 0.0028 - val_mae: 0.0400\n",
            "Epoch 98/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0024 - mae: 0.0381 - val_loss: 0.0028 - val_mae: 0.0401\n",
            "Epoch 99/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0023 - mae: 0.0370 - val_loss: 0.0027 - val_mae: 0.0394\n",
            "Epoch 100/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0386 - val_loss: 0.0031 - val_mae: 0.0422\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span style=\"color:cyan\"> |-Trial ID: e2dd84cbe5d142e1e3b0a8e4aad94797</span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span style=\"color:cyan\"> |-Score: 0.038086067885160446</span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span style=\"color:cyan\"> |-Best step: 0</span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span style=\"color:cyan\"> |-learning_rate: 0.01</span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span style=\"color:blue\"> |-num_layers: 2</span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span style=\"color:cyan\"> |-units_0: 24</span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span style=\"color:blue\"> |-units_1: 8</span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "129/129 [==============================] - 0s 3ms/step - loss: 0.8430 - mae: 0.6669 - val_loss: 0.7442 - val_mae: 0.6121\n",
            "Epoch 2/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.5600 - mae: 0.5356 - val_loss: 0.5384 - val_mae: 0.5222\n",
            "Epoch 3/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.4201 - mae: 0.4657 - val_loss: 0.4202 - val_mae: 0.4657\n",
            "Epoch 4/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.3346 - mae: 0.4190 - val_loss: 0.3403 - val_mae: 0.4219\n",
            "Epoch 5/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.2744 - mae: 0.3824 - val_loss: 0.2817 - val_mae: 0.3859\n",
            "Epoch 6/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.2291 - mae: 0.3514 - val_loss: 0.2361 - val_mae: 0.3550\n",
            "Epoch 7/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.1937 - mae: 0.3249 - val_loss: 0.2005 - val_mae: 0.3284\n",
            "Epoch 8/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.1656 - mae: 0.3024 - val_loss: 0.1724 - val_mae: 0.3056\n",
            "Epoch 9/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.1431 - mae: 0.2820 - val_loss: 0.1489 - val_mae: 0.2853\n",
            "Epoch 10/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.1246 - mae: 0.2642 - val_loss: 0.1300 - val_mae: 0.2679\n",
            "Epoch 11/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.1093 - mae: 0.2489 - val_loss: 0.1143 - val_mae: 0.2525\n",
            "Epoch 12/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0966 - mae: 0.2346 - val_loss: 0.1017 - val_mae: 0.2386\n",
            "Epoch 13/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0858 - mae: 0.2217 - val_loss: 0.0906 - val_mae: 0.2263\n",
            "Epoch 14/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0767 - mae: 0.2102 - val_loss: 0.0814 - val_mae: 0.2152\n",
            "Epoch 15/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0691 - mae: 0.1998 - val_loss: 0.0737 - val_mae: 0.2053\n",
            "Epoch 16/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0626 - mae: 0.1905 - val_loss: 0.0672 - val_mae: 0.1964\n",
            "Epoch 17/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0571 - mae: 0.1820 - val_loss: 0.0613 - val_mae: 0.1880\n",
            "Epoch 18/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0523 - mae: 0.1745 - val_loss: 0.0563 - val_mae: 0.1804\n",
            "Epoch 19/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0481 - mae: 0.1674 - val_loss: 0.0521 - val_mae: 0.1735\n",
            "Epoch 20/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0443 - mae: 0.1609 - val_loss: 0.0483 - val_mae: 0.1671\n",
            "Epoch 21/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0410 - mae: 0.1549 - val_loss: 0.0449 - val_mae: 0.1611\n",
            "Epoch 22/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0381 - mae: 0.1491 - val_loss: 0.0418 - val_mae: 0.1554\n",
            "Epoch 23/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0354 - mae: 0.1440 - val_loss: 0.0390 - val_mae: 0.1500\n",
            "Epoch 24/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0330 - mae: 0.1392 - val_loss: 0.0365 - val_mae: 0.1450\n",
            "Epoch 25/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0309 - mae: 0.1346 - val_loss: 0.0342 - val_mae: 0.1402\n",
            "Epoch 26/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0289 - mae: 0.1303 - val_loss: 0.0320 - val_mae: 0.1358\n",
            "Epoch 27/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0271 - mae: 0.1262 - val_loss: 0.0301 - val_mae: 0.1316\n",
            "Epoch 28/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0254 - mae: 0.1223 - val_loss: 0.0284 - val_mae: 0.1277\n",
            "Epoch 29/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0239 - mae: 0.1186 - val_loss: 0.0268 - val_mae: 0.1240\n",
            "Epoch 30/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0226 - mae: 0.1152 - val_loss: 0.0253 - val_mae: 0.1204\n",
            "Epoch 31/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0213 - mae: 0.1119 - val_loss: 0.0239 - val_mae: 0.1170\n",
            "Epoch 32/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0202 - mae: 0.1088 - val_loss: 0.0227 - val_mae: 0.1138\n",
            "Epoch 33/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0191 - mae: 0.1059 - val_loss: 0.0215 - val_mae: 0.1107\n",
            "Epoch 34/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0181 - mae: 0.1031 - val_loss: 0.0204 - val_mae: 0.1077\n",
            "Epoch 35/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0172 - mae: 0.1003 - val_loss: 0.0194 - val_mae: 0.1049\n",
            "Epoch 36/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0163 - mae: 0.0977 - val_loss: 0.0184 - val_mae: 0.1022\n",
            "Epoch 37/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0155 - mae: 0.0952 - val_loss: 0.0176 - val_mae: 0.0997\n",
            "Epoch 38/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0148 - mae: 0.0930 - val_loss: 0.0168 - val_mae: 0.0973\n",
            "Epoch 39/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0141 - mae: 0.0908 - val_loss: 0.0160 - val_mae: 0.0951\n",
            "Epoch 40/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0134 - mae: 0.0887 - val_loss: 0.0153 - val_mae: 0.0929\n",
            "Epoch 41/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0128 - mae: 0.0866 - val_loss: 0.0147 - val_mae: 0.0908\n",
            "Epoch 42/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0123 - mae: 0.0847 - val_loss: 0.0141 - val_mae: 0.0887\n",
            "Epoch 43/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0117 - mae: 0.0828 - val_loss: 0.0135 - val_mae: 0.0868\n",
            "Epoch 44/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0112 - mae: 0.0810 - val_loss: 0.0129 - val_mae: 0.0849\n",
            "Epoch 45/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0107 - mae: 0.0793 - val_loss: 0.0124 - val_mae: 0.0831\n",
            "Epoch 46/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0103 - mae: 0.0777 - val_loss: 0.0119 - val_mae: 0.0813\n",
            "Epoch 47/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0099 - mae: 0.0762 - val_loss: 0.0114 - val_mae: 0.0797\n",
            "Epoch 48/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0095 - mae: 0.0748 - val_loss: 0.0110 - val_mae: 0.0782\n",
            "Epoch 49/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0092 - mae: 0.0735 - val_loss: 0.0106 - val_mae: 0.0767\n",
            "Epoch 50/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0089 - mae: 0.0721 - val_loss: 0.0102 - val_mae: 0.0755\n",
            "Epoch 51/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0086 - mae: 0.0710 - val_loss: 0.0099 - val_mae: 0.0742\n",
            "Epoch 52/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0083 - mae: 0.0697 - val_loss: 0.0095 - val_mae: 0.0730\n",
            "Epoch 53/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0080 - mae: 0.0687 - val_loss: 0.0092 - val_mae: 0.0719\n",
            "Epoch 54/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0078 - mae: 0.0676 - val_loss: 0.0090 - val_mae: 0.0708\n",
            "Epoch 55/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0075 - mae: 0.0666 - val_loss: 0.0087 - val_mae: 0.0698\n",
            "Epoch 56/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0073 - mae: 0.0656 - val_loss: 0.0085 - val_mae: 0.0688\n",
            "Epoch 57/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0071 - mae: 0.0647 - val_loss: 0.0082 - val_mae: 0.0679\n",
            "Epoch 58/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0069 - mae: 0.0638 - val_loss: 0.0080 - val_mae: 0.0670\n",
            "Epoch 59/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0067 - mae: 0.0630 - val_loss: 0.0078 - val_mae: 0.0662\n",
            "Epoch 60/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0065 - mae: 0.0621 - val_loss: 0.0076 - val_mae: 0.0656\n",
            "Epoch 61/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0064 - mae: 0.0613 - val_loss: 0.0074 - val_mae: 0.0648\n",
            "Epoch 62/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0062 - mae: 0.0607 - val_loss: 0.0072 - val_mae: 0.0639\n",
            "Epoch 63/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0061 - mae: 0.0599 - val_loss: 0.0070 - val_mae: 0.0632\n",
            "Epoch 64/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0059 - mae: 0.0592 - val_loss: 0.0069 - val_mae: 0.0625\n",
            "Epoch 65/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0058 - mae: 0.0585 - val_loss: 0.0067 - val_mae: 0.0618\n",
            "Epoch 66/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0056 - mae: 0.0578 - val_loss: 0.0066 - val_mae: 0.0611\n",
            "Epoch 67/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0055 - mae: 0.0572 - val_loss: 0.0064 - val_mae: 0.0605\n",
            "Epoch 68/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0054 - mae: 0.0567 - val_loss: 0.0063 - val_mae: 0.0599\n",
            "Epoch 69/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0053 - mae: 0.0560 - val_loss: 0.0062 - val_mae: 0.0592\n",
            "Epoch 70/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0052 - mae: 0.0555 - val_loss: 0.0060 - val_mae: 0.0588\n",
            "Epoch 71/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0051 - mae: 0.0550 - val_loss: 0.0060 - val_mae: 0.0581\n",
            "Epoch 72/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0050 - mae: 0.0545 - val_loss: 0.0058 - val_mae: 0.0576\n",
            "Epoch 73/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0049 - mae: 0.0540 - val_loss: 0.0057 - val_mae: 0.0571\n",
            "Epoch 74/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0048 - mae: 0.0536 - val_loss: 0.0056 - val_mae: 0.0566\n",
            "Epoch 75/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0047 - mae: 0.0530 - val_loss: 0.0055 - val_mae: 0.0562\n",
            "Epoch 76/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0046 - mae: 0.0527 - val_loss: 0.0054 - val_mae: 0.0555\n",
            "Epoch 77/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0046 - mae: 0.0522 - val_loss: 0.0053 - val_mae: 0.0551\n",
            "Epoch 78/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0045 - mae: 0.0518 - val_loss: 0.0053 - val_mae: 0.0547\n",
            "Epoch 79/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0044 - mae: 0.0515 - val_loss: 0.0052 - val_mae: 0.0542\n",
            "Epoch 80/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0043 - mae: 0.0511 - val_loss: 0.0051 - val_mae: 0.0538\n",
            "Epoch 81/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0043 - mae: 0.0506 - val_loss: 0.0051 - val_mae: 0.0535\n",
            "Epoch 82/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0042 - mae: 0.0503 - val_loss: 0.0050 - val_mae: 0.0531\n",
            "Epoch 83/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0041 - mae: 0.0499 - val_loss: 0.0049 - val_mae: 0.0527\n",
            "Epoch 84/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0041 - mae: 0.0496 - val_loss: 0.0048 - val_mae: 0.0522\n",
            "Epoch 85/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0040 - mae: 0.0492 - val_loss: 0.0048 - val_mae: 0.0521\n",
            "Epoch 86/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0040 - mae: 0.0490 - val_loss: 0.0047 - val_mae: 0.0515\n",
            "Epoch 87/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0039 - mae: 0.0486 - val_loss: 0.0047 - val_mae: 0.0513\n",
            "Epoch 88/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0039 - mae: 0.0484 - val_loss: 0.0046 - val_mae: 0.0510\n",
            "Epoch 89/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0038 - mae: 0.0480 - val_loss: 0.0045 - val_mae: 0.0506\n",
            "Epoch 90/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0038 - mae: 0.0477 - val_loss: 0.0045 - val_mae: 0.0503\n",
            "Epoch 91/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0037 - mae: 0.0475 - val_loss: 0.0044 - val_mae: 0.0500\n",
            "Epoch 92/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0037 - mae: 0.0472 - val_loss: 0.0044 - val_mae: 0.0497\n",
            "Epoch 93/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0037 - mae: 0.0470 - val_loss: 0.0043 - val_mae: 0.0495\n",
            "Epoch 94/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0036 - mae: 0.0467 - val_loss: 0.0043 - val_mae: 0.0492\n",
            "Epoch 95/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0036 - mae: 0.0464 - val_loss: 0.0042 - val_mae: 0.0489\n",
            "Epoch 96/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0035 - mae: 0.0462 - val_loss: 0.0042 - val_mae: 0.0488\n",
            "Epoch 97/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0035 - mae: 0.0460 - val_loss: 0.0041 - val_mae: 0.0485\n",
            "Epoch 98/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0035 - mae: 0.0457 - val_loss: 0.0042 - val_mae: 0.0482\n",
            "Epoch 99/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0034 - mae: 0.0455 - val_loss: 0.0041 - val_mae: 0.0481\n",
            "Epoch 100/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0034 - mae: 0.0453 - val_loss: 0.0041 - val_mae: 0.0479\n",
            "Epoch 1/100\n",
            "129/129 [==============================] - 0s 3ms/step - loss: 0.1403 - mae: 0.2686 - val_loss: 0.1187 - val_mae: 0.2555\n",
            "Epoch 2/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.1171 - mae: 0.2442 - val_loss: 0.0994 - val_mae: 0.2344\n",
            "Epoch 3/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0985 - mae: 0.2244 - val_loss: 0.0842 - val_mae: 0.2162\n",
            "Epoch 4/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0841 - mae: 0.2075 - val_loss: 0.0726 - val_mae: 0.2006\n",
            "Epoch 5/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0729 - mae: 0.1931 - val_loss: 0.0633 - val_mae: 0.1869\n",
            "Epoch 6/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0639 - mae: 0.1805 - val_loss: 0.0558 - val_mae: 0.1750\n",
            "Epoch 7/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0563 - mae: 0.1692 - val_loss: 0.0495 - val_mae: 0.1644\n",
            "Epoch 8/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0502 - mae: 0.1590 - val_loss: 0.0445 - val_mae: 0.1553\n",
            "Epoch 9/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0449 - mae: 0.1504 - val_loss: 0.0401 - val_mae: 0.1474\n",
            "Epoch 10/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0403 - mae: 0.1422 - val_loss: 0.0365 - val_mae: 0.1403\n",
            "Epoch 11/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0365 - mae: 0.1348 - val_loss: 0.0334 - val_mae: 0.1340\n",
            "Epoch 12/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0332 - mae: 0.1284 - val_loss: 0.0308 - val_mae: 0.1283\n",
            "Epoch 13/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0304 - mae: 0.1227 - val_loss: 0.0285 - val_mae: 0.1230\n",
            "Epoch 14/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0280 - mae: 0.1175 - val_loss: 0.0265 - val_mae: 0.1183\n",
            "Epoch 15/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0258 - mae: 0.1127 - val_loss: 0.0247 - val_mae: 0.1140\n",
            "Epoch 16/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0240 - mae: 0.1086 - val_loss: 0.0232 - val_mae: 0.1103\n",
            "Epoch 17/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0224 - mae: 0.1047 - val_loss: 0.0218 - val_mae: 0.1067\n",
            "Epoch 18/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0209 - mae: 0.1013 - val_loss: 0.0205 - val_mae: 0.1034\n",
            "Epoch 19/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0196 - mae: 0.0981 - val_loss: 0.0193 - val_mae: 0.1005\n",
            "Epoch 20/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0184 - mae: 0.0951 - val_loss: 0.0183 - val_mae: 0.0978\n",
            "Epoch 21/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0173 - mae: 0.0923 - val_loss: 0.0173 - val_mae: 0.0952\n",
            "Epoch 22/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0162 - mae: 0.0898 - val_loss: 0.0164 - val_mae: 0.0930\n",
            "Epoch 23/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0153 - mae: 0.0873 - val_loss: 0.0157 - val_mae: 0.0910\n",
            "Epoch 24/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0144 - mae: 0.0850 - val_loss: 0.0150 - val_mae: 0.0892\n",
            "Epoch 25/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0137 - mae: 0.0830 - val_loss: 0.0144 - val_mae: 0.0876\n",
            "Epoch 26/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0130 - mae: 0.0812 - val_loss: 0.0139 - val_mae: 0.0862\n",
            "Epoch 27/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0124 - mae: 0.0795 - val_loss: 0.0134 - val_mae: 0.0849\n",
            "Epoch 28/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0118 - mae: 0.0780 - val_loss: 0.0130 - val_mae: 0.0837\n",
            "Epoch 29/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0113 - mae: 0.0767 - val_loss: 0.0126 - val_mae: 0.0825\n",
            "Epoch 30/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0109 - mae: 0.0755 - val_loss: 0.0122 - val_mae: 0.0813\n",
            "Epoch 31/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0105 - mae: 0.0744 - val_loss: 0.0119 - val_mae: 0.0803\n",
            "Epoch 32/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0101 - mae: 0.0733 - val_loss: 0.0116 - val_mae: 0.0794\n",
            "Epoch 33/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0098 - mae: 0.0725 - val_loss: 0.0113 - val_mae: 0.0786\n",
            "Epoch 34/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0095 - mae: 0.0717 - val_loss: 0.0110 - val_mae: 0.0777\n",
            "Epoch 35/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0092 - mae: 0.0708 - val_loss: 0.0107 - val_mae: 0.0769\n",
            "Epoch 36/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0090 - mae: 0.0699 - val_loss: 0.0105 - val_mae: 0.0761\n",
            "Epoch 37/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0087 - mae: 0.0691 - val_loss: 0.0103 - val_mae: 0.0753\n",
            "Epoch 38/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0084 - mae: 0.0683 - val_loss: 0.0100 - val_mae: 0.0745\n",
            "Epoch 39/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0082 - mae: 0.0675 - val_loss: 0.0098 - val_mae: 0.0738\n",
            "Epoch 40/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0080 - mae: 0.0669 - val_loss: 0.0096 - val_mae: 0.0730\n",
            "Epoch 41/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0078 - mae: 0.0661 - val_loss: 0.0094 - val_mae: 0.0722\n",
            "Epoch 42/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0076 - mae: 0.0654 - val_loss: 0.0092 - val_mae: 0.0715\n",
            "Epoch 43/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0074 - mae: 0.0648 - val_loss: 0.0090 - val_mae: 0.0708\n",
            "Epoch 44/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0072 - mae: 0.0640 - val_loss: 0.0088 - val_mae: 0.0701\n",
            "Epoch 45/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0071 - mae: 0.0633 - val_loss: 0.0087 - val_mae: 0.0693\n",
            "Epoch 46/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0069 - mae: 0.0627 - val_loss: 0.0085 - val_mae: 0.0686\n",
            "Epoch 47/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0068 - mae: 0.0620 - val_loss: 0.0083 - val_mae: 0.0680\n",
            "Epoch 48/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0066 - mae: 0.0615 - val_loss: 0.0082 - val_mae: 0.0674\n",
            "Epoch 49/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0065 - mae: 0.0610 - val_loss: 0.0081 - val_mae: 0.0668\n",
            "Epoch 50/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0064 - mae: 0.0604 - val_loss: 0.0079 - val_mae: 0.0661\n",
            "Epoch 51/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0063 - mae: 0.0599 - val_loss: 0.0078 - val_mae: 0.0655\n",
            "Epoch 52/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0062 - mae: 0.0594 - val_loss: 0.0077 - val_mae: 0.0650\n",
            "Epoch 53/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0061 - mae: 0.0589 - val_loss: 0.0075 - val_mae: 0.0644\n",
            "Epoch 54/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0060 - mae: 0.0585 - val_loss: 0.0074 - val_mae: 0.0638\n",
            "Epoch 55/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0059 - mae: 0.0581 - val_loss: 0.0073 - val_mae: 0.0632\n",
            "Epoch 56/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0058 - mae: 0.0576 - val_loss: 0.0072 - val_mae: 0.0626\n",
            "Epoch 57/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0057 - mae: 0.0572 - val_loss: 0.0070 - val_mae: 0.0620\n",
            "Epoch 58/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0056 - mae: 0.0568 - val_loss: 0.0069 - val_mae: 0.0614\n",
            "Epoch 59/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0055 - mae: 0.0563 - val_loss: 0.0067 - val_mae: 0.0609\n",
            "Epoch 60/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0054 - mae: 0.0559 - val_loss: 0.0066 - val_mae: 0.0604\n",
            "Epoch 61/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0053 - mae: 0.0555 - val_loss: 0.0065 - val_mae: 0.0598\n",
            "Epoch 62/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0052 - mae: 0.0550 - val_loss: 0.0064 - val_mae: 0.0592\n",
            "Epoch 63/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0051 - mae: 0.0545 - val_loss: 0.0063 - val_mae: 0.0587\n",
            "Epoch 64/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0050 - mae: 0.0541 - val_loss: 0.0062 - val_mae: 0.0582\n",
            "Epoch 65/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0050 - mae: 0.0536 - val_loss: 0.0061 - val_mae: 0.0578\n",
            "Epoch 66/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0049 - mae: 0.0533 - val_loss: 0.0060 - val_mae: 0.0573\n",
            "Epoch 67/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0048 - mae: 0.0528 - val_loss: 0.0059 - val_mae: 0.0567\n",
            "Epoch 68/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0047 - mae: 0.0524 - val_loss: 0.0058 - val_mae: 0.0563\n",
            "Epoch 69/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0047 - mae: 0.0521 - val_loss: 0.0057 - val_mae: 0.0558\n",
            "Epoch 70/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0046 - mae: 0.0516 - val_loss: 0.0056 - val_mae: 0.0554\n",
            "Epoch 71/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0045 - mae: 0.0513 - val_loss: 0.0055 - val_mae: 0.0550\n",
            "Epoch 72/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0045 - mae: 0.0510 - val_loss: 0.0054 - val_mae: 0.0545\n",
            "Epoch 73/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0044 - mae: 0.0505 - val_loss: 0.0054 - val_mae: 0.0542\n",
            "Epoch 74/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0043 - mae: 0.0503 - val_loss: 0.0053 - val_mae: 0.0537\n",
            "Epoch 75/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0043 - mae: 0.0499 - val_loss: 0.0052 - val_mae: 0.0534\n",
            "Epoch 76/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0042 - mae: 0.0496 - val_loss: 0.0051 - val_mae: 0.0530\n",
            "Epoch 77/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0042 - mae: 0.0494 - val_loss: 0.0051 - val_mae: 0.0527\n",
            "Epoch 78/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0041 - mae: 0.0490 - val_loss: 0.0050 - val_mae: 0.0524\n",
            "Epoch 79/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0041 - mae: 0.0487 - val_loss: 0.0050 - val_mae: 0.0522\n",
            "Epoch 80/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0040 - mae: 0.0485 - val_loss: 0.0049 - val_mae: 0.0519\n",
            "Epoch 81/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0040 - mae: 0.0483 - val_loss: 0.0049 - val_mae: 0.0516\n",
            "Epoch 82/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0039 - mae: 0.0480 - val_loss: 0.0048 - val_mae: 0.0513\n",
            "Epoch 83/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0039 - mae: 0.0478 - val_loss: 0.0048 - val_mae: 0.0512\n",
            "Epoch 84/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0039 - mae: 0.0476 - val_loss: 0.0047 - val_mae: 0.0509\n",
            "Epoch 85/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0038 - mae: 0.0473 - val_loss: 0.0047 - val_mae: 0.0507\n",
            "Epoch 86/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0038 - mae: 0.0471 - val_loss: 0.0046 - val_mae: 0.0504\n",
            "Epoch 87/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0038 - mae: 0.0469 - val_loss: 0.0046 - val_mae: 0.0503\n",
            "Epoch 88/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0037 - mae: 0.0468 - val_loss: 0.0045 - val_mae: 0.0500\n",
            "Epoch 89/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0037 - mae: 0.0465 - val_loss: 0.0045 - val_mae: 0.0498\n",
            "Epoch 90/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0037 - mae: 0.0463 - val_loss: 0.0044 - val_mae: 0.0496\n",
            "Epoch 91/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0036 - mae: 0.0462 - val_loss: 0.0044 - val_mae: 0.0495\n",
            "Epoch 92/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0036 - mae: 0.0459 - val_loss: 0.0044 - val_mae: 0.0493\n",
            "Epoch 93/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0036 - mae: 0.0458 - val_loss: 0.0044 - val_mae: 0.0491\n",
            "Epoch 94/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0036 - mae: 0.0457 - val_loss: 0.0043 - val_mae: 0.0489\n",
            "Epoch 95/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0035 - mae: 0.0455 - val_loss: 0.0043 - val_mae: 0.0488\n",
            "Epoch 96/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0035 - mae: 0.0453 - val_loss: 0.0042 - val_mae: 0.0486\n",
            "Epoch 97/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0035 - mae: 0.0452 - val_loss: 0.0042 - val_mae: 0.0484\n",
            "Epoch 98/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0035 - mae: 0.0451 - val_loss: 0.0042 - val_mae: 0.0482\n",
            "Epoch 99/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0034 - mae: 0.0449 - val_loss: 0.0041 - val_mae: 0.0480\n",
            "Epoch 100/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0034 - mae: 0.0447 - val_loss: 0.0041 - val_mae: 0.0479\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span style=\"color:cyan\"> |-Trial ID: 995f65c750733d973197cb12bb3923a9</span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span style=\"color:cyan\"> |-Score: 0.04786086082458496</span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span style=\"color:cyan\"> |-Best step: 0</span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span style=\"color:cyan\"> |-learning_rate: 0.0001</span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span style=\"color:blue\"> |-num_layers: 2</span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span style=\"color:cyan\"> |-units_0: 16</span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span style=\"color:blue\"> |-units_1: 8</span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "129/129 [==============================] - 0s 3ms/step - loss: 0.0176 - mae: 0.0811 - val_loss: 0.0079 - val_mae: 0.0664\n",
            "Epoch 2/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0066 - mae: 0.0621 - val_loss: 0.0052 - val_mae: 0.0531\n",
            "Epoch 3/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0042 - mae: 0.0487 - val_loss: 0.0035 - val_mae: 0.0434\n",
            "Epoch 4/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0034 - mae: 0.0441 - val_loss: 0.0032 - val_mae: 0.0414\n",
            "Epoch 5/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0032 - mae: 0.0430 - val_loss: 0.0034 - val_mae: 0.0438\n",
            "Epoch 6/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0029 - mae: 0.0414 - val_loss: 0.0034 - val_mae: 0.0446\n",
            "Epoch 7/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0029 - mae: 0.0415 - val_loss: 0.0028 - val_mae: 0.0395\n",
            "Epoch 8/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0028 - mae: 0.0405 - val_loss: 0.0028 - val_mae: 0.0397\n",
            "Epoch 9/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0027 - mae: 0.0402 - val_loss: 0.0029 - val_mae: 0.0403\n",
            "Epoch 10/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0028 - mae: 0.0404 - val_loss: 0.0028 - val_mae: 0.0398\n",
            "Epoch 11/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0027 - mae: 0.0401 - val_loss: 0.0029 - val_mae: 0.0400\n",
            "Epoch 12/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0027 - mae: 0.0397 - val_loss: 0.0027 - val_mae: 0.0392\n",
            "Epoch 13/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0027 - mae: 0.0401 - val_loss: 0.0028 - val_mae: 0.0396\n",
            "Epoch 14/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0393 - val_loss: 0.0028 - val_mae: 0.0392\n",
            "Epoch 15/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0392 - val_loss: 0.0027 - val_mae: 0.0392\n",
            "Epoch 16/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0397 - val_loss: 0.0030 - val_mae: 0.0402\n",
            "Epoch 17/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0387 - val_loss: 0.0027 - val_mae: 0.0388\n",
            "Epoch 18/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0381 - val_loss: 0.0030 - val_mae: 0.0423\n",
            "Epoch 19/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0027 - mae: 0.0401 - val_loss: 0.0027 - val_mae: 0.0391\n",
            "Epoch 20/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0388 - val_loss: 0.0026 - val_mae: 0.0384\n",
            "Epoch 21/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0384 - val_loss: 0.0028 - val_mae: 0.0393\n",
            "Epoch 22/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0024 - mae: 0.0379 - val_loss: 0.0027 - val_mae: 0.0390\n",
            "Epoch 23/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0393 - val_loss: 0.0028 - val_mae: 0.0401\n",
            "Epoch 24/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0387 - val_loss: 0.0030 - val_mae: 0.0408\n",
            "Epoch 25/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0027 - mae: 0.0395 - val_loss: 0.0032 - val_mae: 0.0443\n",
            "Epoch 26/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0027 - mae: 0.0399 - val_loss: 0.0027 - val_mae: 0.0387\n",
            "Epoch 27/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0028 - mae: 0.0398 - val_loss: 0.0032 - val_mae: 0.0423\n",
            "Epoch 28/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0395 - val_loss: 0.0031 - val_mae: 0.0435\n",
            "Epoch 29/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0388 - val_loss: 0.0030 - val_mae: 0.0418\n",
            "Epoch 30/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0027 - mae: 0.0397 - val_loss: 0.0028 - val_mae: 0.0397\n",
            "Epoch 31/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0027 - mae: 0.0399 - val_loss: 0.0029 - val_mae: 0.0395\n",
            "Epoch 32/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0027 - mae: 0.0402 - val_loss: 0.0030 - val_mae: 0.0402\n",
            "Epoch 33/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0027 - mae: 0.0402 - val_loss: 0.0026 - val_mae: 0.0389\n",
            "Epoch 34/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0384 - val_loss: 0.0031 - val_mae: 0.0424\n",
            "Epoch 35/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0392 - val_loss: 0.0028 - val_mae: 0.0399\n",
            "Epoch 36/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0029 - mae: 0.0415 - val_loss: 0.0027 - val_mae: 0.0390\n",
            "Epoch 37/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0385 - val_loss: 0.0028 - val_mae: 0.0396\n",
            "Epoch 38/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0391 - val_loss: 0.0028 - val_mae: 0.0390\n",
            "Epoch 39/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0024 - mae: 0.0381 - val_loss: 0.0027 - val_mae: 0.0395\n",
            "Epoch 40/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0383 - val_loss: 0.0033 - val_mae: 0.0454\n",
            "Epoch 41/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0391 - val_loss: 0.0028 - val_mae: 0.0401\n",
            "Epoch 42/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0385 - val_loss: 0.0028 - val_mae: 0.0398\n",
            "Epoch 43/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0024 - mae: 0.0379 - val_loss: 0.0027 - val_mae: 0.0388\n",
            "Epoch 44/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0024 - mae: 0.0378 - val_loss: 0.0027 - val_mae: 0.0389\n",
            "Epoch 45/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0024 - mae: 0.0379 - val_loss: 0.0028 - val_mae: 0.0398\n",
            "Epoch 46/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0023 - mae: 0.0372 - val_loss: 0.0029 - val_mae: 0.0410\n",
            "Epoch 47/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0023 - mae: 0.0373 - val_loss: 0.0028 - val_mae: 0.0392\n",
            "Epoch 48/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0023 - mae: 0.0371 - val_loss: 0.0029 - val_mae: 0.0402\n",
            "Epoch 49/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0024 - mae: 0.0378 - val_loss: 0.0028 - val_mae: 0.0398\n",
            "Epoch 50/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0024 - mae: 0.0378 - val_loss: 0.0030 - val_mae: 0.0413\n",
            "Epoch 51/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0024 - mae: 0.0377 - val_loss: 0.0030 - val_mae: 0.0427\n",
            "Epoch 52/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0023 - mae: 0.0373 - val_loss: 0.0028 - val_mae: 0.0393\n",
            "Epoch 53/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0023 - mae: 0.0373 - val_loss: 0.0029 - val_mae: 0.0404\n",
            "Epoch 54/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0023 - mae: 0.0374 - val_loss: 0.0030 - val_mae: 0.0411\n",
            "Epoch 55/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0023 - mae: 0.0380 - val_loss: 0.0028 - val_mae: 0.0398\n",
            "Epoch 56/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0023 - mae: 0.0368 - val_loss: 0.0030 - val_mae: 0.0409\n",
            "Epoch 57/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0023 - mae: 0.0368 - val_loss: 0.0027 - val_mae: 0.0390\n",
            "Epoch 58/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0023 - mae: 0.0370 - val_loss: 0.0028 - val_mae: 0.0396\n",
            "Epoch 59/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0023 - mae: 0.0370 - val_loss: 0.0028 - val_mae: 0.0399\n",
            "Epoch 60/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0023 - mae: 0.0369 - val_loss: 0.0029 - val_mae: 0.0395\n",
            "Epoch 61/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0023 - mae: 0.0369 - val_loss: 0.0028 - val_mae: 0.0406\n",
            "Epoch 62/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0024 - mae: 0.0376 - val_loss: 0.0028 - val_mae: 0.0398\n",
            "Epoch 63/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0023 - mae: 0.0368 - val_loss: 0.0029 - val_mae: 0.0408\n",
            "Epoch 64/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0022 - mae: 0.0368 - val_loss: 0.0026 - val_mae: 0.0388\n",
            "Epoch 65/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0023 - mae: 0.0368 - val_loss: 0.0029 - val_mae: 0.0409\n",
            "Epoch 66/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0022 - mae: 0.0364 - val_loss: 0.0034 - val_mae: 0.0450\n",
            "Epoch 67/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0023 - mae: 0.0372 - val_loss: 0.0031 - val_mae: 0.0425\n",
            "Epoch 68/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0023 - mae: 0.0378 - val_loss: 0.0035 - val_mae: 0.0465\n",
            "Epoch 69/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0022 - mae: 0.0363 - val_loss: 0.0030 - val_mae: 0.0423\n",
            "Epoch 70/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0021 - mae: 0.0357 - val_loss: 0.0028 - val_mae: 0.0398\n",
            "Epoch 71/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0022 - mae: 0.0364 - val_loss: 0.0029 - val_mae: 0.0411\n",
            "Epoch 72/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0021 - mae: 0.0358 - val_loss: 0.0029 - val_mae: 0.0403\n",
            "Epoch 73/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0022 - mae: 0.0362 - val_loss: 0.0030 - val_mae: 0.0421\n",
            "Epoch 74/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0022 - mae: 0.0362 - val_loss: 0.0033 - val_mae: 0.0419\n",
            "Epoch 75/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0024 - mae: 0.0380 - val_loss: 0.0028 - val_mae: 0.0398\n",
            "Epoch 76/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0022 - mae: 0.0363 - val_loss: 0.0028 - val_mae: 0.0398\n",
            "Epoch 77/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0022 - mae: 0.0363 - val_loss: 0.0033 - val_mae: 0.0419\n",
            "Epoch 78/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0022 - mae: 0.0371 - val_loss: 0.0029 - val_mae: 0.0413\n",
            "Epoch 79/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0022 - mae: 0.0362 - val_loss: 0.0029 - val_mae: 0.0400\n",
            "Epoch 80/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0021 - mae: 0.0359 - val_loss: 0.0029 - val_mae: 0.0401\n",
            "Epoch 81/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0021 - mae: 0.0358 - val_loss: 0.0029 - val_mae: 0.0401\n",
            "Epoch 82/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0021 - mae: 0.0360 - val_loss: 0.0028 - val_mae: 0.0400\n",
            "Epoch 83/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0022 - mae: 0.0362 - val_loss: 0.0030 - val_mae: 0.0414\n",
            "Epoch 84/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0021 - mae: 0.0356 - val_loss: 0.0031 - val_mae: 0.0420\n",
            "Epoch 85/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0021 - mae: 0.0354 - val_loss: 0.0030 - val_mae: 0.0406\n",
            "Epoch 86/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0021 - mae: 0.0358 - val_loss: 0.0032 - val_mae: 0.0412\n",
            "Epoch 87/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0021 - mae: 0.0359 - val_loss: 0.0029 - val_mae: 0.0407\n",
            "Epoch 88/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0021 - mae: 0.0355 - val_loss: 0.0031 - val_mae: 0.0415\n",
            "Epoch 89/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0020 - mae: 0.0349 - val_loss: 0.0032 - val_mae: 0.0418\n",
            "Epoch 90/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0023 - mae: 0.0376 - val_loss: 0.0030 - val_mae: 0.0410\n",
            "Epoch 91/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0021 - mae: 0.0355 - val_loss: 0.0030 - val_mae: 0.0411\n",
            "Epoch 92/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0020 - mae: 0.0346 - val_loss: 0.0029 - val_mae: 0.0401\n",
            "Epoch 93/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0021 - mae: 0.0358 - val_loss: 0.0028 - val_mae: 0.0403\n",
            "Epoch 94/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0020 - mae: 0.0346 - val_loss: 0.0030 - val_mae: 0.0414\n",
            "Epoch 95/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0020 - mae: 0.0347 - val_loss: 0.0029 - val_mae: 0.0406\n",
            "Epoch 96/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0020 - mae: 0.0345 - val_loss: 0.0029 - val_mae: 0.0403\n",
            "Epoch 97/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0021 - mae: 0.0352 - val_loss: 0.0029 - val_mae: 0.0405\n",
            "Epoch 98/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0020 - mae: 0.0345 - val_loss: 0.0031 - val_mae: 0.0409\n",
            "Epoch 99/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0020 - mae: 0.0347 - val_loss: 0.0032 - val_mae: 0.0427\n",
            "Epoch 100/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0022 - mae: 0.0363 - val_loss: 0.0029 - val_mae: 0.0399\n",
            "Epoch 1/100\n",
            "129/129 [==============================] - 0s 3ms/step - loss: 0.0088 - mae: 0.0597 - val_loss: 0.0030 - val_mae: 0.0414\n",
            "Epoch 2/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0032 - mae: 0.0429 - val_loss: 0.0028 - val_mae: 0.0411\n",
            "Epoch 3/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0029 - mae: 0.0413 - val_loss: 0.0026 - val_mae: 0.0395\n",
            "Epoch 4/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0028 - mae: 0.0409 - val_loss: 0.0027 - val_mae: 0.0397\n",
            "Epoch 5/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0027 - mae: 0.0405 - val_loss: 0.0025 - val_mae: 0.0387\n",
            "Epoch 6/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0027 - mae: 0.0400 - val_loss: 0.0026 - val_mae: 0.0387\n",
            "Epoch 7/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0028 - mae: 0.0404 - val_loss: 0.0027 - val_mae: 0.0405\n",
            "Epoch 8/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0027 - mae: 0.0402 - val_loss: 0.0026 - val_mae: 0.0388\n",
            "Epoch 9/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0393 - val_loss: 0.0026 - val_mae: 0.0395\n",
            "Epoch 10/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0027 - mae: 0.0400 - val_loss: 0.0029 - val_mae: 0.0422\n",
            "Epoch 11/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0391 - val_loss: 0.0027 - val_mae: 0.0406\n",
            "Epoch 12/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0389 - val_loss: 0.0028 - val_mae: 0.0403\n",
            "Epoch 13/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0389 - val_loss: 0.0028 - val_mae: 0.0402\n",
            "Epoch 14/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0024 - mae: 0.0381 - val_loss: 0.0026 - val_mae: 0.0390\n",
            "Epoch 15/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0027 - mae: 0.0399 - val_loss: 0.0027 - val_mae: 0.0392\n",
            "Epoch 16/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0391 - val_loss: 0.0028 - val_mae: 0.0405\n",
            "Epoch 17/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0384 - val_loss: 0.0026 - val_mae: 0.0386\n",
            "Epoch 18/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0388 - val_loss: 0.0027 - val_mae: 0.0399\n",
            "Epoch 19/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0382 - val_loss: 0.0028 - val_mae: 0.0408\n",
            "Epoch 20/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0389 - val_loss: 0.0027 - val_mae: 0.0388\n",
            "Epoch 21/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0024 - mae: 0.0379 - val_loss: 0.0027 - val_mae: 0.0408\n",
            "Epoch 22/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0023 - mae: 0.0374 - val_loss: 0.0027 - val_mae: 0.0400\n",
            "Epoch 23/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0036 - mae: 0.0439 - val_loss: 0.0043 - val_mae: 0.0446\n",
            "Epoch 24/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0034 - mae: 0.0406 - val_loss: 0.0026 - val_mae: 0.0387\n",
            "Epoch 25/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0399 - val_loss: 0.0027 - val_mae: 0.0399\n",
            "Epoch 26/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0394 - val_loss: 0.0027 - val_mae: 0.0397\n",
            "Epoch 27/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0386 - val_loss: 0.0026 - val_mae: 0.0394\n",
            "Epoch 28/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0384 - val_loss: 0.0027 - val_mae: 0.0398\n",
            "Epoch 29/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0382 - val_loss: 0.0030 - val_mae: 0.0423\n",
            "Epoch 30/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0023 - mae: 0.0375 - val_loss: 0.0029 - val_mae: 0.0410\n",
            "Epoch 31/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0023 - mae: 0.0374 - val_loss: 0.0029 - val_mae: 0.0405\n",
            "Epoch 32/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0024 - mae: 0.0376 - val_loss: 0.0031 - val_mae: 0.0421\n",
            "Epoch 33/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0023 - mae: 0.0369 - val_loss: 0.0027 - val_mae: 0.0396\n",
            "Epoch 34/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0382 - val_loss: 0.0029 - val_mae: 0.0407\n",
            "Epoch 35/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0023 - mae: 0.0367 - val_loss: 0.0027 - val_mae: 0.0400\n",
            "Epoch 36/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0023 - mae: 0.0369 - val_loss: 0.0031 - val_mae: 0.0419\n",
            "Epoch 37/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0023 - mae: 0.0373 - val_loss: 0.0031 - val_mae: 0.0415\n",
            "Epoch 38/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0024 - mae: 0.0377 - val_loss: 0.0032 - val_mae: 0.0423\n",
            "Epoch 39/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0023 - mae: 0.0372 - val_loss: 0.0028 - val_mae: 0.0399\n",
            "Epoch 40/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0023 - mae: 0.0375 - val_loss: 0.0029 - val_mae: 0.0405\n",
            "Epoch 41/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0023 - mae: 0.0370 - val_loss: 0.0031 - val_mae: 0.0421\n",
            "Epoch 42/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0023 - mae: 0.0370 - val_loss: 0.0027 - val_mae: 0.0393\n",
            "Epoch 43/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0023 - mae: 0.0370 - val_loss: 0.0028 - val_mae: 0.0397\n",
            "Epoch 44/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0023 - mae: 0.0369 - val_loss: 0.0029 - val_mae: 0.0410\n",
            "Epoch 45/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0023 - mae: 0.0373 - val_loss: 0.0029 - val_mae: 0.0402\n",
            "Epoch 46/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0022 - mae: 0.0363 - val_loss: 0.0030 - val_mae: 0.0412\n",
            "Epoch 47/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0022 - mae: 0.0367 - val_loss: 0.0031 - val_mae: 0.0415\n",
            "Epoch 48/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0022 - mae: 0.0366 - val_loss: 0.0029 - val_mae: 0.0400\n",
            "Epoch 49/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0027 - mae: 0.0376 - val_loss: 0.0032 - val_mae: 0.0424\n",
            "Epoch 50/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0037 - mae: 0.0419 - val_loss: 0.0036 - val_mae: 0.0421\n",
            "Epoch 51/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0040 - mae: 0.0429 - val_loss: 0.0028 - val_mae: 0.0412\n",
            "Epoch 52/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0023 - mae: 0.0378 - val_loss: 0.0028 - val_mae: 0.0398\n",
            "Epoch 53/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0023 - mae: 0.0371 - val_loss: 0.0028 - val_mae: 0.0396\n",
            "Epoch 54/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0022 - mae: 0.0362 - val_loss: 0.0031 - val_mae: 0.0428\n",
            "Epoch 55/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0022 - mae: 0.0363 - val_loss: 0.0028 - val_mae: 0.0408\n",
            "Epoch 56/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0021 - mae: 0.0357 - val_loss: 0.0029 - val_mae: 0.0403\n",
            "Epoch 57/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0022 - mae: 0.0366 - val_loss: 0.0028 - val_mae: 0.0399\n",
            "Epoch 58/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0022 - mae: 0.0360 - val_loss: 0.0029 - val_mae: 0.0402\n",
            "Epoch 59/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0021 - mae: 0.0353 - val_loss: 0.0030 - val_mae: 0.0410\n",
            "Epoch 60/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0023 - mae: 0.0356 - val_loss: 0.0030 - val_mae: 0.0407\n",
            "Epoch 61/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0021 - mae: 0.0352 - val_loss: 0.0030 - val_mae: 0.0408\n",
            "Epoch 62/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0020 - mae: 0.0353 - val_loss: 0.0030 - val_mae: 0.0410\n",
            "Epoch 63/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0021 - mae: 0.0356 - val_loss: 0.0031 - val_mae: 0.0412\n",
            "Epoch 64/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0021 - mae: 0.0358 - val_loss: 0.0029 - val_mae: 0.0405\n",
            "Epoch 65/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0020 - mae: 0.0351 - val_loss: 0.0031 - val_mae: 0.0411\n",
            "Epoch 66/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0022 - mae: 0.0369 - val_loss: 0.0034 - val_mae: 0.0444\n",
            "Epoch 67/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0021 - mae: 0.0358 - val_loss: 0.0030 - val_mae: 0.0410\n",
            "Epoch 68/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0021 - mae: 0.0353 - val_loss: 0.0033 - val_mae: 0.0421\n",
            "Epoch 69/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0020 - mae: 0.0353 - val_loss: 0.0034 - val_mae: 0.0431\n",
            "Epoch 70/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0020 - mae: 0.0353 - val_loss: 0.0036 - val_mae: 0.0436\n",
            "Epoch 71/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0020 - mae: 0.0354 - val_loss: 0.0035 - val_mae: 0.0432\n",
            "Epoch 72/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0020 - mae: 0.0347 - val_loss: 0.0031 - val_mae: 0.0413\n",
            "Epoch 73/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0019 - mae: 0.0344 - val_loss: 0.0035 - val_mae: 0.0436\n",
            "Epoch 74/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0020 - mae: 0.0349 - val_loss: 0.0031 - val_mae: 0.0408\n",
            "Epoch 75/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0020 - mae: 0.0349 - val_loss: 0.0031 - val_mae: 0.0416\n",
            "Epoch 76/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0020 - mae: 0.0351 - val_loss: 0.0033 - val_mae: 0.0425\n",
            "Epoch 77/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0020 - mae: 0.0349 - val_loss: 0.0035 - val_mae: 0.0432\n",
            "Epoch 78/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0019 - mae: 0.0345 - val_loss: 0.0032 - val_mae: 0.0415\n",
            "Epoch 79/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0020 - mae: 0.0355 - val_loss: 0.0032 - val_mae: 0.0424\n",
            "Epoch 80/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0032 - mae: 0.0352 - val_loss: 0.0033 - val_mae: 0.0420\n",
            "Epoch 81/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0019 - mae: 0.0340 - val_loss: 0.0034 - val_mae: 0.0437\n",
            "Epoch 82/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0019 - mae: 0.0343 - val_loss: 0.0033 - val_mae: 0.0422\n",
            "Epoch 83/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0019 - mae: 0.0338 - val_loss: 0.0032 - val_mae: 0.0424\n",
            "Epoch 84/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0019 - mae: 0.0339 - val_loss: 0.0030 - val_mae: 0.0411\n",
            "Epoch 85/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0020 - mae: 0.0348 - val_loss: 0.0037 - val_mae: 0.0442\n",
            "Epoch 86/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0020 - mae: 0.0348 - val_loss: 0.0035 - val_mae: 0.0430\n",
            "Epoch 87/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0019 - mae: 0.0345 - val_loss: 0.0033 - val_mae: 0.0420\n",
            "Epoch 88/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0019 - mae: 0.0340 - val_loss: 0.0033 - val_mae: 0.0429\n",
            "Epoch 89/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0019 - mae: 0.0344 - val_loss: 0.0034 - val_mae: 0.0430\n",
            "Epoch 90/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0020 - mae: 0.0345 - val_loss: 0.0034 - val_mae: 0.0431\n",
            "Epoch 91/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0019 - mae: 0.0336 - val_loss: 0.0032 - val_mae: 0.0419\n",
            "Epoch 92/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0018 - mae: 0.0337 - val_loss: 0.0033 - val_mae: 0.0427\n",
            "Epoch 93/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0019 - mae: 0.0339 - val_loss: 0.0033 - val_mae: 0.0417\n",
            "Epoch 94/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0019 - mae: 0.0341 - val_loss: 0.0032 - val_mae: 0.0422\n",
            "Epoch 95/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0018 - mae: 0.0337 - val_loss: 0.0036 - val_mae: 0.0447\n",
            "Epoch 96/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0019 - mae: 0.0338 - val_loss: 0.0032 - val_mae: 0.0416\n",
            "Epoch 97/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0018 - mae: 0.0335 - val_loss: 0.0034 - val_mae: 0.0421\n",
            "Epoch 98/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0018 - mae: 0.0334 - val_loss: 0.0032 - val_mae: 0.0423\n",
            "Epoch 99/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0018 - mae: 0.0333 - val_loss: 0.0033 - val_mae: 0.0435\n",
            "Epoch 100/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0018 - mae: 0.0334 - val_loss: 0.0034 - val_mae: 0.0427\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span style=\"color:cyan\"> |-Trial ID: 7941e801a5e9060959b3f9167a21d036</span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span style=\"color:cyan\"> |-Score: 0.038478387519717216</span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span style=\"color:cyan\"> |-Best step: 0</span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span style=\"color:cyan\"> |-learning_rate: 0.01</span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span style=\"color:blue\"> |-num_layers: 4</span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span style=\"color:cyan\"> |-units_0: 24</span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span style=\"color:blue\"> |-units_1: 16</span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span style=\"color:cyan\"> |-units_2: 8</span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span style=\"color:blue\"> |-units_3: 8</span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "129/129 [==============================] - 0s 3ms/step - loss: 0.1790 - mae: 0.2851 - val_loss: 0.0525 - val_mae: 0.1720\n",
            "Epoch 2/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0398 - mae: 0.1480 - val_loss: 0.0328 - val_mae: 0.1332\n",
            "Epoch 3/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0249 - mae: 0.1182 - val_loss: 0.0227 - val_mae: 0.1122\n",
            "Epoch 4/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0183 - mae: 0.1010 - val_loss: 0.0210 - val_mae: 0.1010\n",
            "Epoch 5/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0159 - mae: 0.0913 - val_loss: 0.0215 - val_mae: 0.0917\n",
            "Epoch 6/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0203 - mae: 0.0835 - val_loss: 0.0225 - val_mae: 0.0985\n",
            "Epoch 7/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0232 - mae: 0.0802 - val_loss: 0.0135 - val_mae: 0.0785\n",
            "Epoch 8/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0112 - mae: 0.0700 - val_loss: 0.0106 - val_mae: 0.0769\n",
            "Epoch 9/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0078 - mae: 0.0666 - val_loss: 0.0091 - val_mae: 0.0712\n",
            "Epoch 10/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0065 - mae: 0.0623 - val_loss: 0.0080 - val_mae: 0.0677\n",
            "Epoch 11/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0059 - mae: 0.0600 - val_loss: 0.0076 - val_mae: 0.0665\n",
            "Epoch 12/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0055 - mae: 0.0575 - val_loss: 0.0070 - val_mae: 0.0632\n",
            "Epoch 13/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0051 - mae: 0.0556 - val_loss: 0.0066 - val_mae: 0.0615\n",
            "Epoch 14/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0048 - mae: 0.0541 - val_loss: 0.0063 - val_mae: 0.0599\n",
            "Epoch 15/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0045 - mae: 0.0524 - val_loss: 0.0060 - val_mae: 0.0590\n",
            "Epoch 16/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0043 - mae: 0.0511 - val_loss: 0.0057 - val_mae: 0.0573\n",
            "Epoch 17/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0041 - mae: 0.0499 - val_loss: 0.0056 - val_mae: 0.0568\n",
            "Epoch 18/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0040 - mae: 0.0490 - val_loss: 0.0054 - val_mae: 0.0555\n",
            "Epoch 19/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0039 - mae: 0.0485 - val_loss: 0.0056 - val_mae: 0.0551\n",
            "Epoch 20/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0042 - mae: 0.0487 - val_loss: 0.0060 - val_mae: 0.0542\n",
            "Epoch 21/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0047 - mae: 0.0484 - val_loss: 0.0096 - val_mae: 0.0544\n",
            "Epoch 22/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0079 - mae: 0.0471 - val_loss: 0.0060 - val_mae: 0.0574\n",
            "Epoch 23/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0159 - mae: 0.0527 - val_loss: 0.0117 - val_mae: 0.0528\n",
            "Epoch 24/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0115 - mae: 0.0484 - val_loss: 0.0058 - val_mae: 0.0504\n",
            "Epoch 25/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0049 - mae: 0.0450 - val_loss: 0.0048 - val_mae: 0.0492\n",
            "Epoch 26/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0035 - mae: 0.0440 - val_loss: 0.0043 - val_mae: 0.0486\n",
            "Epoch 27/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0032 - mae: 0.0434 - val_loss: 0.0043 - val_mae: 0.0483\n",
            "Epoch 28/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0030 - mae: 0.0426 - val_loss: 0.0040 - val_mae: 0.0477\n",
            "Epoch 29/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0029 - mae: 0.0420 - val_loss: 0.0040 - val_mae: 0.0471\n",
            "Epoch 30/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0029 - mae: 0.0416 - val_loss: 0.0039 - val_mae: 0.0474\n",
            "Epoch 31/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0028 - mae: 0.0413 - val_loss: 0.0039 - val_mae: 0.0471\n",
            "Epoch 32/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0028 - mae: 0.0411 - val_loss: 0.0039 - val_mae: 0.0466\n",
            "Epoch 33/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0028 - mae: 0.0410 - val_loss: 0.0039 - val_mae: 0.0462\n",
            "Epoch 34/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0028 - mae: 0.0407 - val_loss: 0.0039 - val_mae: 0.0464\n",
            "Epoch 35/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0028 - mae: 0.0405 - val_loss: 0.0042 - val_mae: 0.0466\n",
            "Epoch 36/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0029 - mae: 0.0406 - val_loss: 0.0043 - val_mae: 0.0463\n",
            "Epoch 37/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0034 - mae: 0.0408 - val_loss: 0.0056 - val_mae: 0.0463\n",
            "Epoch 38/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0046 - mae: 0.0419 - val_loss: 0.0074 - val_mae: 0.0476\n",
            "Epoch 39/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0068 - mae: 0.0428 - val_loss: 0.0080 - val_mae: 0.0465\n",
            "Epoch 40/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0075 - mae: 0.0424 - val_loss: 0.0061 - val_mae: 0.0477\n",
            "Epoch 41/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0053 - mae: 0.0411 - val_loss: 0.0045 - val_mae: 0.0446\n",
            "Epoch 42/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0036 - mae: 0.0398 - val_loss: 0.0036 - val_mae: 0.0443\n",
            "Epoch 43/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0027 - mae: 0.0389 - val_loss: 0.0036 - val_mae: 0.0446\n",
            "Epoch 44/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0390 - val_loss: 0.0034 - val_mae: 0.0437\n",
            "Epoch 45/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0385 - val_loss: 0.0035 - val_mae: 0.0437\n",
            "Epoch 46/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0024 - mae: 0.0384 - val_loss: 0.0034 - val_mae: 0.0436\n",
            "Epoch 47/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0024 - mae: 0.0382 - val_loss: 0.0036 - val_mae: 0.0439\n",
            "Epoch 48/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0386 - val_loss: 0.0036 - val_mae: 0.0441\n",
            "Epoch 49/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0027 - mae: 0.0388 - val_loss: 0.0040 - val_mae: 0.0448\n",
            "Epoch 50/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0030 - mae: 0.0386 - val_loss: 0.0043 - val_mae: 0.0443\n",
            "Epoch 51/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0034 - mae: 0.0388 - val_loss: 0.0050 - val_mae: 0.0447\n",
            "Epoch 52/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0043 - mae: 0.0392 - val_loss: 0.0052 - val_mae: 0.0449\n",
            "Epoch 53/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0046 - mae: 0.0398 - val_loss: 0.0045 - val_mae: 0.0434\n",
            "Epoch 54/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0038 - mae: 0.0388 - val_loss: 0.0038 - val_mae: 0.0434\n",
            "Epoch 55/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0032 - mae: 0.0383 - val_loss: 0.0032 - val_mae: 0.0425\n",
            "Epoch 56/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0024 - mae: 0.0375 - val_loss: 0.0032 - val_mae: 0.0425\n",
            "Epoch 57/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0023 - mae: 0.0371 - val_loss: 0.0032 - val_mae: 0.0424\n",
            "Epoch 58/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0023 - mae: 0.0369 - val_loss: 0.0033 - val_mae: 0.0436\n",
            "Epoch 59/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0023 - mae: 0.0371 - val_loss: 0.0032 - val_mae: 0.0427\n",
            "Epoch 60/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0023 - mae: 0.0370 - val_loss: 0.0032 - val_mae: 0.0428\n",
            "Epoch 61/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0022 - mae: 0.0368 - val_loss: 0.0032 - val_mae: 0.0428\n",
            "Epoch 62/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0022 - mae: 0.0366 - val_loss: 0.0032 - val_mae: 0.0430\n",
            "Epoch 63/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0022 - mae: 0.0364 - val_loss: 0.0032 - val_mae: 0.0423\n",
            "Epoch 64/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0022 - mae: 0.0364 - val_loss: 0.0033 - val_mae: 0.0430\n",
            "Epoch 65/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0023 - mae: 0.0367 - val_loss: 0.0033 - val_mae: 0.0429\n",
            "Epoch 66/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0023 - mae: 0.0368 - val_loss: 0.0033 - val_mae: 0.0428\n",
            "Epoch 67/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0370 - val_loss: 0.0036 - val_mae: 0.0434\n",
            "Epoch 68/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0372 - val_loss: 0.0033 - val_mae: 0.0429\n",
            "Epoch 69/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0023 - mae: 0.0366 - val_loss: 0.0033 - val_mae: 0.0430\n",
            "Epoch 70/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0022 - mae: 0.0360 - val_loss: 0.0032 - val_mae: 0.0428\n",
            "Epoch 71/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0021 - mae: 0.0361 - val_loss: 0.0032 - val_mae: 0.0425\n",
            "Epoch 72/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0021 - mae: 0.0357 - val_loss: 0.0032 - val_mae: 0.0426\n",
            "Epoch 73/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0021 - mae: 0.0359 - val_loss: 0.0033 - val_mae: 0.0431\n",
            "Epoch 74/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0021 - mae: 0.0359 - val_loss: 0.0032 - val_mae: 0.0426\n",
            "Epoch 75/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0021 - mae: 0.0355 - val_loss: 0.0032 - val_mae: 0.0429\n",
            "Epoch 76/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0021 - mae: 0.0356 - val_loss: 0.0032 - val_mae: 0.0427\n",
            "Epoch 77/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0021 - mae: 0.0355 - val_loss: 0.0033 - val_mae: 0.0430\n",
            "Epoch 78/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0021 - mae: 0.0356 - val_loss: 0.0032 - val_mae: 0.0428\n",
            "Epoch 79/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0021 - mae: 0.0356 - val_loss: 0.0033 - val_mae: 0.0435\n",
            "Epoch 80/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0021 - mae: 0.0355 - val_loss: 0.0032 - val_mae: 0.0424\n",
            "Epoch 81/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0021 - mae: 0.0351 - val_loss: 0.0033 - val_mae: 0.0429\n",
            "Epoch 82/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0022 - mae: 0.0360 - val_loss: 0.0034 - val_mae: 0.0432\n",
            "Epoch 83/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0022 - mae: 0.0354 - val_loss: 0.0033 - val_mae: 0.0432\n",
            "Epoch 84/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0024 - mae: 0.0361 - val_loss: 0.0037 - val_mae: 0.0433\n",
            "Epoch 85/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0360 - val_loss: 0.0034 - val_mae: 0.0437\n",
            "Epoch 86/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0023 - mae: 0.0355 - val_loss: 0.0034 - val_mae: 0.0433\n",
            "Epoch 87/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0021 - mae: 0.0350 - val_loss: 0.0033 - val_mae: 0.0433\n",
            "Epoch 88/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0021 - mae: 0.0355 - val_loss: 0.0032 - val_mae: 0.0428\n",
            "Epoch 89/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0020 - mae: 0.0349 - val_loss: 0.0032 - val_mae: 0.0426\n",
            "Epoch 90/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0021 - mae: 0.0353 - val_loss: 0.0033 - val_mae: 0.0434\n",
            "Epoch 91/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0020 - mae: 0.0350 - val_loss: 0.0033 - val_mae: 0.0430\n",
            "Epoch 92/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0020 - mae: 0.0347 - val_loss: 0.0035 - val_mae: 0.0444\n",
            "Epoch 93/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0020 - mae: 0.0347 - val_loss: 0.0032 - val_mae: 0.0431\n",
            "Epoch 94/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0020 - mae: 0.0347 - val_loss: 0.0032 - val_mae: 0.0429\n",
            "Epoch 95/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0020 - mae: 0.0346 - val_loss: 0.0032 - val_mae: 0.0430\n",
            "Epoch 96/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0020 - mae: 0.0343 - val_loss: 0.0035 - val_mae: 0.0441\n",
            "Epoch 97/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0020 - mae: 0.0350 - val_loss: 0.0034 - val_mae: 0.0438\n",
            "Epoch 98/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0020 - mae: 0.0347 - val_loss: 0.0036 - val_mae: 0.0445\n",
            "Epoch 99/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0022 - mae: 0.0350 - val_loss: 0.0034 - val_mae: 0.0434\n",
            "Epoch 100/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0022 - mae: 0.0350 - val_loss: 0.0052 - val_mae: 0.0457\n",
            "Epoch 1/100\n",
            "129/129 [==============================] - 0s 3ms/step - loss: 0.0774 - mae: 0.1866 - val_loss: 0.0265 - val_mae: 0.1170\n",
            "Epoch 2/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0180 - mae: 0.0936 - val_loss: 0.0133 - val_mae: 0.0865\n",
            "Epoch 3/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0098 - mae: 0.0737 - val_loss: 0.0090 - val_mae: 0.0718\n",
            "Epoch 4/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0068 - mae: 0.0629 - val_loss: 0.0069 - val_mae: 0.0634\n",
            "Epoch 5/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0054 - mae: 0.0565 - val_loss: 0.0057 - val_mae: 0.0577\n",
            "Epoch 6/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0047 - mae: 0.0524 - val_loss: 0.0050 - val_mae: 0.0541\n",
            "Epoch 7/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0041 - mae: 0.0497 - val_loss: 0.0045 - val_mae: 0.0516\n",
            "Epoch 8/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0038 - mae: 0.0475 - val_loss: 0.0042 - val_mae: 0.0493\n",
            "Epoch 9/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0036 - mae: 0.0461 - val_loss: 0.0040 - val_mae: 0.0481\n",
            "Epoch 10/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0034 - mae: 0.0450 - val_loss: 0.0038 - val_mae: 0.0467\n",
            "Epoch 11/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0032 - mae: 0.0441 - val_loss: 0.0037 - val_mae: 0.0462\n",
            "Epoch 12/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0031 - mae: 0.0435 - val_loss: 0.0036 - val_mae: 0.0452\n",
            "Epoch 13/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0030 - mae: 0.0427 - val_loss: 0.0035 - val_mae: 0.0447\n",
            "Epoch 14/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0030 - mae: 0.0422 - val_loss: 0.0034 - val_mae: 0.0444\n",
            "Epoch 15/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0029 - mae: 0.0419 - val_loss: 0.0034 - val_mae: 0.0439\n",
            "Epoch 16/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0029 - mae: 0.0414 - val_loss: 0.0033 - val_mae: 0.0435\n",
            "Epoch 17/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0028 - mae: 0.0410 - val_loss: 0.0033 - val_mae: 0.0429\n",
            "Epoch 18/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0028 - mae: 0.0409 - val_loss: 0.0032 - val_mae: 0.0427\n",
            "Epoch 19/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0028 - mae: 0.0406 - val_loss: 0.0036 - val_mae: 0.0428\n",
            "Epoch 20/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0031 - mae: 0.0409 - val_loss: 0.0036 - val_mae: 0.0432\n",
            "Epoch 21/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0036 - mae: 0.0410 - val_loss: 0.0046 - val_mae: 0.0425\n",
            "Epoch 22/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0040 - mae: 0.0405 - val_loss: 0.0036 - val_mae: 0.0449\n",
            "Epoch 23/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0039 - mae: 0.0408 - val_loss: 0.0038 - val_mae: 0.0422\n",
            "Epoch 24/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0032 - mae: 0.0397 - val_loss: 0.0030 - val_mae: 0.0417\n",
            "Epoch 25/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0028 - mae: 0.0394 - val_loss: 0.0032 - val_mae: 0.0414\n",
            "Epoch 26/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0391 - val_loss: 0.0030 - val_mae: 0.0412\n",
            "Epoch 27/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0387 - val_loss: 0.0031 - val_mae: 0.0413\n",
            "Epoch 28/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0384 - val_loss: 0.0030 - val_mae: 0.0412\n",
            "Epoch 29/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0384 - val_loss: 0.0030 - val_mae: 0.0413\n",
            "Epoch 30/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0382 - val_loss: 0.0029 - val_mae: 0.0411\n",
            "Epoch 31/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0024 - mae: 0.0380 - val_loss: 0.0030 - val_mae: 0.0412\n",
            "Epoch 32/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0381 - val_loss: 0.0029 - val_mae: 0.0407\n",
            "Epoch 33/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0024 - mae: 0.0379 - val_loss: 0.0031 - val_mae: 0.0415\n",
            "Epoch 34/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0382 - val_loss: 0.0030 - val_mae: 0.0410\n",
            "Epoch 35/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0027 - mae: 0.0380 - val_loss: 0.0035 - val_mae: 0.0412\n",
            "Epoch 36/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0029 - mae: 0.0383 - val_loss: 0.0033 - val_mae: 0.0413\n",
            "Epoch 37/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0029 - mae: 0.0379 - val_loss: 0.0035 - val_mae: 0.0431\n",
            "Epoch 38/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0031 - mae: 0.0384 - val_loss: 0.0033 - val_mae: 0.0415\n",
            "Epoch 39/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0386 - val_loss: 0.0037 - val_mae: 0.0415\n",
            "Epoch 40/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0038 - mae: 0.0389 - val_loss: 0.0031 - val_mae: 0.0405\n",
            "Epoch 41/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0373 - val_loss: 0.0029 - val_mae: 0.0408\n",
            "Epoch 42/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0024 - mae: 0.0372 - val_loss: 0.0028 - val_mae: 0.0401\n",
            "Epoch 43/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0023 - mae: 0.0368 - val_loss: 0.0028 - val_mae: 0.0403\n",
            "Epoch 44/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0023 - mae: 0.0368 - val_loss: 0.0028 - val_mae: 0.0402\n",
            "Epoch 45/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0023 - mae: 0.0367 - val_loss: 0.0028 - val_mae: 0.0402\n",
            "Epoch 46/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0022 - mae: 0.0364 - val_loss: 0.0028 - val_mae: 0.0403\n",
            "Epoch 47/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0022 - mae: 0.0365 - val_loss: 0.0028 - val_mae: 0.0402\n",
            "Epoch 48/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0022 - mae: 0.0364 - val_loss: 0.0028 - val_mae: 0.0401\n",
            "Epoch 49/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0022 - mae: 0.0363 - val_loss: 0.0028 - val_mae: 0.0405\n",
            "Epoch 50/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0022 - mae: 0.0363 - val_loss: 0.0029 - val_mae: 0.0404\n",
            "Epoch 51/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0022 - mae: 0.0363 - val_loss: 0.0028 - val_mae: 0.0403\n",
            "Epoch 52/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0022 - mae: 0.0362 - val_loss: 0.0029 - val_mae: 0.0404\n",
            "Epoch 53/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0023 - mae: 0.0363 - val_loss: 0.0029 - val_mae: 0.0406\n",
            "Epoch 54/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0367 - val_loss: 0.0035 - val_mae: 0.0412\n",
            "Epoch 55/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0027 - mae: 0.0370 - val_loss: 0.0033 - val_mae: 0.0414\n",
            "Epoch 56/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0029 - mae: 0.0368 - val_loss: 0.0034 - val_mae: 0.0433\n",
            "Epoch 57/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0028 - mae: 0.0370 - val_loss: 0.0031 - val_mae: 0.0426\n",
            "Epoch 58/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0022 - mae: 0.0363 - val_loss: 0.0028 - val_mae: 0.0401\n",
            "Epoch 59/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0023 - mae: 0.0359 - val_loss: 0.0030 - val_mae: 0.0413\n",
            "Epoch 60/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0022 - mae: 0.0360 - val_loss: 0.0028 - val_mae: 0.0401\n",
            "Epoch 61/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0021 - mae: 0.0356 - val_loss: 0.0028 - val_mae: 0.0400\n",
            "Epoch 62/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0021 - mae: 0.0352 - val_loss: 0.0028 - val_mae: 0.0405\n",
            "Epoch 63/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0021 - mae: 0.0355 - val_loss: 0.0029 - val_mae: 0.0406\n",
            "Epoch 64/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0021 - mae: 0.0353 - val_loss: 0.0028 - val_mae: 0.0406\n",
            "Epoch 65/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0021 - mae: 0.0353 - val_loss: 0.0028 - val_mae: 0.0407\n",
            "Epoch 66/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0021 - mae: 0.0353 - val_loss: 0.0028 - val_mae: 0.0406\n",
            "Epoch 67/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0021 - mae: 0.0352 - val_loss: 0.0028 - val_mae: 0.0406\n",
            "Epoch 68/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0021 - mae: 0.0351 - val_loss: 0.0028 - val_mae: 0.0407\n",
            "Epoch 69/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0021 - mae: 0.0349 - val_loss: 0.0029 - val_mae: 0.0407\n",
            "Epoch 70/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0021 - mae: 0.0351 - val_loss: 0.0029 - val_mae: 0.0411\n",
            "Epoch 71/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0022 - mae: 0.0351 - val_loss: 0.0033 - val_mae: 0.0417\n",
            "Epoch 72/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0358 - val_loss: 0.0035 - val_mae: 0.0425\n",
            "Epoch 73/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0028 - mae: 0.0362 - val_loss: 0.0030 - val_mae: 0.0415\n",
            "Epoch 74/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0021 - mae: 0.0353 - val_loss: 0.0028 - val_mae: 0.0404\n",
            "Epoch 75/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0021 - mae: 0.0350 - val_loss: 0.0028 - val_mae: 0.0407\n",
            "Epoch 76/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0020 - mae: 0.0348 - val_loss: 0.0029 - val_mae: 0.0411\n",
            "Epoch 77/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0020 - mae: 0.0346 - val_loss: 0.0029 - val_mae: 0.0410\n",
            "Epoch 78/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0020 - mae: 0.0345 - val_loss: 0.0029 - val_mae: 0.0411\n",
            "Epoch 79/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0020 - mae: 0.0347 - val_loss: 0.0030 - val_mae: 0.0413\n",
            "Epoch 80/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0020 - mae: 0.0346 - val_loss: 0.0029 - val_mae: 0.0414\n",
            "Epoch 81/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0020 - mae: 0.0345 - val_loss: 0.0029 - val_mae: 0.0409\n",
            "Epoch 82/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0020 - mae: 0.0344 - val_loss: 0.0029 - val_mae: 0.0409\n",
            "Epoch 83/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0020 - mae: 0.0343 - val_loss: 0.0029 - val_mae: 0.0410\n",
            "Epoch 84/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0020 - mae: 0.0343 - val_loss: 0.0029 - val_mae: 0.0413\n",
            "Epoch 85/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0020 - mae: 0.0342 - val_loss: 0.0029 - val_mae: 0.0406\n",
            "Epoch 86/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0020 - mae: 0.0343 - val_loss: 0.0029 - val_mae: 0.0413\n",
            "Epoch 87/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0020 - mae: 0.0341 - val_loss: 0.0029 - val_mae: 0.0410\n",
            "Epoch 88/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0020 - mae: 0.0341 - val_loss: 0.0030 - val_mae: 0.0414\n",
            "Epoch 89/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0019 - mae: 0.0339 - val_loss: 0.0029 - val_mae: 0.0414\n",
            "Epoch 90/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0020 - mae: 0.0341 - val_loss: 0.0030 - val_mae: 0.0416\n",
            "Epoch 91/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0020 - mae: 0.0341 - val_loss: 0.0030 - val_mae: 0.0417\n",
            "Epoch 92/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0019 - mae: 0.0337 - val_loss: 0.0030 - val_mae: 0.0417\n",
            "Epoch 93/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0019 - mae: 0.0340 - val_loss: 0.0030 - val_mae: 0.0416\n",
            "Epoch 94/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0019 - mae: 0.0339 - val_loss: 0.0030 - val_mae: 0.0417\n",
            "Epoch 95/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0019 - mae: 0.0339 - val_loss: 0.0030 - val_mae: 0.0420\n",
            "Epoch 96/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0019 - mae: 0.0337 - val_loss: 0.0030 - val_mae: 0.0414\n",
            "Epoch 97/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0019 - mae: 0.0336 - val_loss: 0.0031 - val_mae: 0.0423\n",
            "Epoch 98/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0019 - mae: 0.0340 - val_loss: 0.0031 - val_mae: 0.0423\n",
            "Epoch 99/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0020 - mae: 0.0341 - val_loss: 0.0041 - val_mae: 0.0426\n",
            "Epoch 100/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0033 - mae: 0.0361 - val_loss: 0.0033 - val_mae: 0.0423\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span style=\"color:cyan\"> |-Trial ID: 7e2f1e970fefc9b9c37e1d1c962d10de</span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span style=\"color:cyan\"> |-Score: 0.041163088753819466</span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span style=\"color:cyan\"> |-Best step: 0</span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span style=\"color:cyan\"> |-learning_rate: 0.001</span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span style=\"color:blue\"> |-num_layers: 2</span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span style=\"color:cyan\"> |-units_0: 32</span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span style=\"color:blue\"> |-units_1: 16</span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span style=\"color:cyan\"> |-units_2: 24</span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span style=\"color:blue\"> |-units_3: 24</span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Oracle triggered exit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhXWWLvEJGtB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "outputId": "31f3def0-298d-4854-ef47-24b130d4bc30"
      },
      "source": [
        "# Choosing thr model with the best hyperparameters\n",
        "best_model=tuner.get_best_models(num_models=1)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-0._inbound_nodes\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-0.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-0.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-1._inbound_nodes\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-1.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-1.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-2._inbound_nodes\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-2.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-2.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.bias\n",
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojhWrUtBKvmS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_hp=tuner.get_best_hyperparameters(num_trials=1)[0]"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXD5CPkmNNer",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_hp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2mc7TLFNtT7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a22ba6d5-cef2-4e00-8f8e-2f15c99e3fc4"
      },
      "source": [
        "# Using the tuned model \n",
        "tuner_model = tuner.hypermodel.build(best_hp)\n",
        "tune_model=tuner_model.fit(train_x, train_y, epochs = 50, validation_data = (valid_x,valid_y))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n",
            "129/129 [==============================] - 0s 3ms/step - loss: 0.0372 - mae: 0.1060 - val_loss: 0.0223 - val_mae: 0.0611\n",
            "Epoch 2/50\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0251 - mae: 0.0618 - val_loss: 0.0210 - val_mae: 0.0607\n",
            "Epoch 3/50\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0181 - mae: 0.0511 - val_loss: 0.0036 - val_mae: 0.0454\n",
            "Epoch 4/50\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0033 - mae: 0.0438 - val_loss: 0.0032 - val_mae: 0.0428\n",
            "Epoch 5/50\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0032 - mae: 0.0421 - val_loss: 0.0030 - val_mae: 0.0414\n",
            "Epoch 6/50\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0028 - mae: 0.0411 - val_loss: 0.0029 - val_mae: 0.0407\n",
            "Epoch 7/50\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0028 - mae: 0.0412 - val_loss: 0.0028 - val_mae: 0.0405\n",
            "Epoch 8/50\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0028 - mae: 0.0405 - val_loss: 0.0030 - val_mae: 0.0421\n",
            "Epoch 9/50\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0029 - mae: 0.0413 - val_loss: 0.0030 - val_mae: 0.0420\n",
            "Epoch 10/50\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0027 - mae: 0.0402 - val_loss: 0.0028 - val_mae: 0.0397\n",
            "Epoch 11/50\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0027 - mae: 0.0397 - val_loss: 0.0029 - val_mae: 0.0404\n",
            "Epoch 12/50\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0027 - mae: 0.0398 - val_loss: 0.0027 - val_mae: 0.0392\n",
            "Epoch 13/50\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0027 - mae: 0.0398 - val_loss: 0.0029 - val_mae: 0.0414\n",
            "Epoch 14/50\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0027 - mae: 0.0397 - val_loss: 0.0027 - val_mae: 0.0397\n",
            "Epoch 15/50\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0027 - mae: 0.0396 - val_loss: 0.0028 - val_mae: 0.0403\n",
            "Epoch 16/50\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0394 - val_loss: 0.0026 - val_mae: 0.0391\n",
            "Epoch 17/50\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0027 - mae: 0.0397 - val_loss: 0.0028 - val_mae: 0.0409\n",
            "Epoch 18/50\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0395 - val_loss: 0.0029 - val_mae: 0.0417\n",
            "Epoch 19/50\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0027 - mae: 0.0400 - val_loss: 0.0036 - val_mae: 0.0448\n",
            "Epoch 20/50\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0393 - val_loss: 0.0026 - val_mae: 0.0392\n",
            "Epoch 21/50\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0387 - val_loss: 0.0026 - val_mae: 0.0392\n",
            "Epoch 22/50\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0393 - val_loss: 0.0028 - val_mae: 0.0413\n",
            "Epoch 23/50\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0394 - val_loss: 0.0027 - val_mae: 0.0396\n",
            "Epoch 24/50\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0391 - val_loss: 0.0026 - val_mae: 0.0389\n",
            "Epoch 25/50\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0390 - val_loss: 0.0027 - val_mae: 0.0400\n",
            "Epoch 26/50\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0394 - val_loss: 0.0028 - val_mae: 0.0408\n",
            "Epoch 27/50\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0390 - val_loss: 0.0028 - val_mae: 0.0404\n",
            "Epoch 28/50\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0388 - val_loss: 0.0028 - val_mae: 0.0407\n",
            "Epoch 29/50\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0389 - val_loss: 0.0027 - val_mae: 0.0393\n",
            "Epoch 30/50\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0389 - val_loss: 0.0028 - val_mae: 0.0405\n",
            "Epoch 31/50\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0390 - val_loss: 0.0027 - val_mae: 0.0405\n",
            "Epoch 32/50\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0390 - val_loss: 0.0027 - val_mae: 0.0393\n",
            "Epoch 33/50\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0385 - val_loss: 0.0028 - val_mae: 0.0401\n",
            "Epoch 34/50\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0390 - val_loss: 0.0028 - val_mae: 0.0400\n",
            "Epoch 35/50\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0390 - val_loss: 0.0028 - val_mae: 0.0397\n",
            "Epoch 36/50\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0027 - mae: 0.0395 - val_loss: 0.0027 - val_mae: 0.0394\n",
            "Epoch 37/50\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0391 - val_loss: 0.0028 - val_mae: 0.0406\n",
            "Epoch 38/50\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0386 - val_loss: 0.0027 - val_mae: 0.0399\n",
            "Epoch 39/50\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0385 - val_loss: 0.0029 - val_mae: 0.0406\n",
            "Epoch 40/50\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0391 - val_loss: 0.0027 - val_mae: 0.0391\n",
            "Epoch 41/50\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0389 - val_loss: 0.0027 - val_mae: 0.0401\n",
            "Epoch 42/50\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0027 - mae: 0.0400 - val_loss: 0.0028 - val_mae: 0.0400\n",
            "Epoch 43/50\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0390 - val_loss: 0.0030 - val_mae: 0.0421\n",
            "Epoch 44/50\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0386 - val_loss: 0.0029 - val_mae: 0.0407\n",
            "Epoch 45/50\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0385 - val_loss: 0.0026 - val_mae: 0.0391\n",
            "Epoch 46/50\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0389 - val_loss: 0.0029 - val_mae: 0.0411\n",
            "Epoch 47/50\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0387 - val_loss: 0.0027 - val_mae: 0.0408\n",
            "Epoch 48/50\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0389 - val_loss: 0.0027 - val_mae: 0.0395\n",
            "Epoch 49/50\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0385 - val_loss: 0.0026 - val_mae: 0.0388\n",
            "Epoch 50/50\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0389 - val_loss: 0.0026 - val_mae: 0.0387\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9DpIHPSOgyl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tuner_model."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esB0G53m10c7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "d5c92462-9016-44ca-81dc-0e8fa7521af5"
      },
      "source": [
        "baseline_model = keras.Sequential([\n",
        "    keras.layers.Dense(32, activation=tf.nn.relu,\n",
        "                       input_shape=(train_x.shape[1],)),\n",
        "    keras.layers.Dense(32,activation=tf.nn.relu,),\n",
        "    keras.layers.Dense(32,activation=tf.nn.relu,),\n",
        "    # keras.layers.Dense(32,activation=tf.nn.relu,),\n",
        "\n",
        "    keras.layers.Dense(1)\n",
        "\n",
        "  ])\n",
        "\n",
        "baseline_model.compile(loss='mse',\n",
        "                optimizer='adam',\n",
        "                metrics=['mae'])\n",
        "baseline_model.summary()\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_8 (Dense)              (None, 32)                832       \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 2,977\n",
            "Trainable params: 2,977\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vojmSRPq1_Ex",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9e368fed-0d9a-43e6-e766-ee3adba7ca8b"
      },
      "source": [
        "EPOCHS = 100\n",
        "model1 = baseline_model.fit(train_x, train_y, epochs=EPOCHS,\n",
        "                    validation_data= (valid_x, valid_y), verbose=1)\n",
        "                    # callbacks=[PrintDot(), tensorboard_callback])"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "129/129 [==============================] - 0s 3ms/step - loss: 0.0263 - mae: 0.1106 - val_loss: 0.0093 - val_mae: 0.0739\n",
            "Epoch 2/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0072 - mae: 0.0648 - val_loss: 0.0066 - val_mae: 0.0600\n",
            "Epoch 3/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0062 - mae: 0.0574 - val_loss: 0.0056 - val_mae: 0.0546\n",
            "Epoch 4/100\n",
            "129/129 [==============================] - 0s 3ms/step - loss: 0.0054 - mae: 0.0532 - val_loss: 0.0047 - val_mae: 0.0512\n",
            "Epoch 5/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0044 - mae: 0.0494 - val_loss: 0.0045 - val_mae: 0.0499\n",
            "Epoch 6/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0042 - mae: 0.0479 - val_loss: 0.0042 - val_mae: 0.0477\n",
            "Epoch 7/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0038 - mae: 0.0458 - val_loss: 0.0037 - val_mae: 0.0472\n",
            "Epoch 8/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0032 - mae: 0.0431 - val_loss: 0.0037 - val_mae: 0.0463\n",
            "Epoch 9/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0030 - mae: 0.0421 - val_loss: 0.0034 - val_mae: 0.0444\n",
            "Epoch 10/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0028 - mae: 0.0411 - val_loss: 0.0033 - val_mae: 0.0439\n",
            "Epoch 11/100\n",
            "129/129 [==============================] - 0s 3ms/step - loss: 0.0027 - mae: 0.0402 - val_loss: 0.0034 - val_mae: 0.0447\n",
            "Epoch 12/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0027 - mae: 0.0402 - val_loss: 0.0034 - val_mae: 0.0447\n",
            "Epoch 13/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0396 - val_loss: 0.0032 - val_mae: 0.0430\n",
            "Epoch 14/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0391 - val_loss: 0.0033 - val_mae: 0.0437\n",
            "Epoch 15/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0390 - val_loss: 0.0033 - val_mae: 0.0439\n",
            "Epoch 16/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0390 - val_loss: 0.0031 - val_mae: 0.0427\n",
            "Epoch 17/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0024 - mae: 0.0375 - val_loss: 0.0031 - val_mae: 0.0422\n",
            "Epoch 18/100\n",
            "129/129 [==============================] - 0s 3ms/step - loss: 0.0023 - mae: 0.0375 - val_loss: 0.0030 - val_mae: 0.0423\n",
            "Epoch 19/100\n",
            "129/129 [==============================] - 0s 3ms/step - loss: 0.0023 - mae: 0.0370 - val_loss: 0.0030 - val_mae: 0.0419\n",
            "Epoch 20/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0022 - mae: 0.0367 - val_loss: 0.0030 - val_mae: 0.0416\n",
            "Epoch 21/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0022 - mae: 0.0361 - val_loss: 0.0030 - val_mae: 0.0417\n",
            "Epoch 22/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0022 - mae: 0.0360 - val_loss: 0.0031 - val_mae: 0.0426\n",
            "Epoch 23/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0022 - mae: 0.0362 - val_loss: 0.0031 - val_mae: 0.0422\n",
            "Epoch 24/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0021 - mae: 0.0359 - val_loss: 0.0030 - val_mae: 0.0415\n",
            "Epoch 25/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0021 - mae: 0.0355 - val_loss: 0.0030 - val_mae: 0.0418\n",
            "Epoch 26/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0021 - mae: 0.0352 - val_loss: 0.0030 - val_mae: 0.0417\n",
            "Epoch 27/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0021 - mae: 0.0352 - val_loss: 0.0031 - val_mae: 0.0421\n",
            "Epoch 28/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0021 - mae: 0.0355 - val_loss: 0.0031 - val_mae: 0.0417\n",
            "Epoch 29/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0020 - mae: 0.0348 - val_loss: 0.0031 - val_mae: 0.0418\n",
            "Epoch 30/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0020 - mae: 0.0350 - val_loss: 0.0031 - val_mae: 0.0420\n",
            "Epoch 31/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0020 - mae: 0.0346 - val_loss: 0.0031 - val_mae: 0.0420\n",
            "Epoch 32/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0020 - mae: 0.0345 - val_loss: 0.0031 - val_mae: 0.0420\n",
            "Epoch 33/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0019 - mae: 0.0338 - val_loss: 0.0032 - val_mae: 0.0437\n",
            "Epoch 34/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0020 - mae: 0.0344 - val_loss: 0.0031 - val_mae: 0.0422\n",
            "Epoch 35/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0021 - mae: 0.0349 - val_loss: 0.0031 - val_mae: 0.0422\n",
            "Epoch 36/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0020 - mae: 0.0344 - val_loss: 0.0033 - val_mae: 0.0431\n",
            "Epoch 37/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0021 - mae: 0.0348 - val_loss: 0.0031 - val_mae: 0.0420\n",
            "Epoch 38/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0019 - mae: 0.0341 - val_loss: 0.0031 - val_mae: 0.0423\n",
            "Epoch 39/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0019 - mae: 0.0336 - val_loss: 0.0031 - val_mae: 0.0421\n",
            "Epoch 40/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0018 - mae: 0.0333 - val_loss: 0.0033 - val_mae: 0.0436\n",
            "Epoch 41/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0018 - mae: 0.0329 - val_loss: 0.0035 - val_mae: 0.0453\n",
            "Epoch 42/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0018 - mae: 0.0328 - val_loss: 0.0033 - val_mae: 0.0439\n",
            "Epoch 43/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0018 - mae: 0.0328 - val_loss: 0.0032 - val_mae: 0.0429\n",
            "Epoch 44/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0017 - mae: 0.0327 - val_loss: 0.0033 - val_mae: 0.0438\n",
            "Epoch 45/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0017 - mae: 0.0328 - val_loss: 0.0032 - val_mae: 0.0433\n",
            "Epoch 46/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0018 - mae: 0.0327 - val_loss: 0.0033 - val_mae: 0.0433\n",
            "Epoch 47/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0017 - mae: 0.0326 - val_loss: 0.0033 - val_mae: 0.0433\n",
            "Epoch 48/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0017 - mae: 0.0320 - val_loss: 0.0033 - val_mae: 0.0430\n",
            "Epoch 49/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0017 - mae: 0.0318 - val_loss: 0.0034 - val_mae: 0.0442\n",
            "Epoch 50/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0017 - mae: 0.0320 - val_loss: 0.0033 - val_mae: 0.0435\n",
            "Epoch 51/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0017 - mae: 0.0319 - val_loss: 0.0034 - val_mae: 0.0439\n",
            "Epoch 52/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0016 - mae: 0.0312 - val_loss: 0.0034 - val_mae: 0.0447\n",
            "Epoch 53/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0018 - mae: 0.0327 - val_loss: 0.0041 - val_mae: 0.0492\n",
            "Epoch 54/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0021 - mae: 0.0350 - val_loss: 0.0034 - val_mae: 0.0439\n",
            "Epoch 55/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0019 - mae: 0.0339 - val_loss: 0.0036 - val_mae: 0.0443\n",
            "Epoch 56/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0021 - mae: 0.0341 - val_loss: 0.0033 - val_mae: 0.0432\n",
            "Epoch 57/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0016 - mae: 0.0317 - val_loss: 0.0033 - val_mae: 0.0435\n",
            "Epoch 58/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0308 - val_loss: 0.0033 - val_mae: 0.0437\n",
            "Epoch 59/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0305 - val_loss: 0.0034 - val_mae: 0.0439\n",
            "Epoch 60/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0304 - val_loss: 0.0035 - val_mae: 0.0444\n",
            "Epoch 61/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0302 - val_loss: 0.0034 - val_mae: 0.0441\n",
            "Epoch 62/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0299 - val_loss: 0.0035 - val_mae: 0.0448\n",
            "Epoch 63/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0305 - val_loss: 0.0036 - val_mae: 0.0447\n",
            "Epoch 64/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0306 - val_loss: 0.0036 - val_mae: 0.0452\n",
            "Epoch 65/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0296 - val_loss: 0.0034 - val_mae: 0.0444\n",
            "Epoch 66/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0292 - val_loss: 0.0038 - val_mae: 0.0458\n",
            "Epoch 67/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0294 - val_loss: 0.0035 - val_mae: 0.0446\n",
            "Epoch 68/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0294 - val_loss: 0.0036 - val_mae: 0.0449\n",
            "Epoch 69/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0292 - val_loss: 0.0036 - val_mae: 0.0453\n",
            "Epoch 70/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0295 - val_loss: 0.0037 - val_mae: 0.0458\n",
            "Epoch 71/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0290 - val_loss: 0.0037 - val_mae: 0.0458\n",
            "Epoch 72/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0296 - val_loss: 0.0036 - val_mae: 0.0450\n",
            "Epoch 73/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0290 - val_loss: 0.0036 - val_mae: 0.0451\n",
            "Epoch 74/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0286 - val_loss: 0.0039 - val_mae: 0.0464\n",
            "Epoch 75/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0287 - val_loss: 0.0038 - val_mae: 0.0456\n",
            "Epoch 76/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0291 - val_loss: 0.0038 - val_mae: 0.0458\n",
            "Epoch 77/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0283 - val_loss: 0.0037 - val_mae: 0.0454\n",
            "Epoch 78/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0282 - val_loss: 0.0043 - val_mae: 0.0486\n",
            "Epoch 79/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0284 - val_loss: 0.0036 - val_mae: 0.0454\n",
            "Epoch 80/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0286 - val_loss: 0.0039 - val_mae: 0.0464\n",
            "Epoch 81/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0281 - val_loss: 0.0039 - val_mae: 0.0466\n",
            "Epoch 82/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0280 - val_loss: 0.0040 - val_mae: 0.0466\n",
            "Epoch 83/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0279 - val_loss: 0.0038 - val_mae: 0.0458\n",
            "Epoch 84/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0282 - val_loss: 0.0039 - val_mae: 0.0463\n",
            "Epoch 85/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0278 - val_loss: 0.0038 - val_mae: 0.0458\n",
            "Epoch 86/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0012 - mae: 0.0273 - val_loss: 0.0040 - val_mae: 0.0471\n",
            "Epoch 87/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0012 - mae: 0.0268 - val_loss: 0.0040 - val_mae: 0.0469\n",
            "Epoch 88/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0012 - mae: 0.0269 - val_loss: 0.0040 - val_mae: 0.0470\n",
            "Epoch 89/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0012 - mae: 0.0268 - val_loss: 0.0038 - val_mae: 0.0461\n",
            "Epoch 90/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0012 - mae: 0.0272 - val_loss: 0.0040 - val_mae: 0.0469\n",
            "Epoch 91/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0012 - mae: 0.0273 - val_loss: 0.0039 - val_mae: 0.0466\n",
            "Epoch 92/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0012 - mae: 0.0268 - val_loss: 0.0040 - val_mae: 0.0467\n",
            "Epoch 93/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0012 - mae: 0.0267 - val_loss: 0.0041 - val_mae: 0.0478\n",
            "Epoch 94/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0012 - mae: 0.0270 - val_loss: 0.0040 - val_mae: 0.0470\n",
            "Epoch 95/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0012 - mae: 0.0269 - val_loss: 0.0041 - val_mae: 0.0471\n",
            "Epoch 96/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0011 - mae: 0.0263 - val_loss: 0.0039 - val_mae: 0.0463\n",
            "Epoch 97/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0011 - mae: 0.0264 - val_loss: 0.0040 - val_mae: 0.0471\n",
            "Epoch 98/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0012 - mae: 0.0266 - val_loss: 0.0039 - val_mae: 0.0466\n",
            "Epoch 99/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0011 - mae: 0.0259 - val_loss: 0.0039 - val_mae: 0.0467\n",
            "Epoch 100/100\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.0011 - mae: 0.0259 - val_loss: 0.0040 - val_mae: 0.0470\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQfK6T5QfGmL",
        "colab_type": "text"
      },
      "source": [
        "### Plot Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVNkEROM4yM5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 549
        },
        "outputId": "4530247b-9a56-4c2b-9a2f-173c3de36d53"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_history(histories, key='mae'):\n",
        "  plt.figure(figsize=(16,10))\n",
        "  for name, history in histories:\n",
        "    val = plt.plot(history.epoch, history.history['val_'+key],\n",
        "                   '--', label=name.title()+' Val')\n",
        "    plt.plot(history.epoch, history.history[key], color=val[0].get_color(),\n",
        "             label=name.title()+' Train')\n",
        "\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel(key.replace('_',' ').title())\n",
        "  plt.legend()\n",
        "\n",
        "  plt.xlim([0,max(history.epoch)])\n",
        "  plt.ylim([0,0.2])\n",
        "\n",
        "plot_history([('baseline', model1),('tuner_model', tune_model)])"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA7wAAAJRCAYAAABm7rvfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdeXyW1Z3//9fJRtiXJCKCFhUQgZAIYRsXsI7gfF0rWqBYix112orbOH6xxanWfn1MO1/n11Z/HW21ou10hMqM4kyZOmoVO4qWRLGyWEWIyCINQdmXLOf7R0Ia9gC5Cbnu1/PxuB/e97nOda7PFf56e67rnBBjRJIkSZKkpMlo6QIkSZIkSUoFA68kSZIkKZEMvJIkSZKkRDLwSpIkSZISycArSZIkSUokA68kSZIkKZFSGnhDCBeFEP4YQlgWQrhrP8f/NoSwJITwhxDCSyGEzzU69pUQwgf1n680ah8aQni3fswHQwghlfcgSZIkSWqdQqr24Q0hZALvAxcCq4AFwKQY45JGfc4H3owxbgshfB0YE2OcEELoBpQCJUAEyoChMcZPQwi/B24B3gTmAg/GGP8rJTchSZIkSWq1UjnDOxxYFmNcHmPcBcwELm/cIcb4coxxW/3PN4Be9d/HAS/EGDfEGD8FXgAuCiH0ADrFGN+IdUn958AVKbwHSZIkSVIrlcrA2xP4uNHvVfVtB/LXwO6Z2gOd27P+e1PHlCRJkiSlqayWLgAghHANdY8vj27GMW8EbgRo37790P79+zfX0JIkSZKk40hZWdn6GGPB3u2pDLyrgZMb/e5V37aHEMJfAtOB0THGnY3OHbPXua/Ut/faq32fMQFijD8FfgpQUlISS0tLj+QeJEmSJEnHuRDCR/trT+UjzQuAviGEU0MIOcBE4Lm9ijoL+AlwWYzxT40OPQ+MDSF0DSF0BcYCz8cY1wKbQggj61dnvhaYk8J7kCRJkiS1Uimb4Y0xVocQplIXXjOBx2OMi0MI9wGlMcbngP8LdACert9daGWM8bIY44YQwnepC80A98UYN9R//wbwBNCWund+XaFZkiRJkrSPlG1LdDzxkWZJkiRJSq4QQlmMsWTv9uNi0SpJkiRJOlxVVVWsWrWKHTt2tHQpOkZyc3Pp1asX2dnZTepv4JUkSZLUKq1atYqOHTvSu3dv6l+RVILFGKmsrGTVqlWceuqpTTonlYtWSZIkSVLK7Nixg7y8PMNumgghkJeXd1gz+gZeSZIkSa2WYTe9HO6/t4FXkiRJko5QZmYmxcXFFBUVMWTIEF5//fVmHX/KlCnMnj0bgOuvv54lS5Yc1Xjbtm0jLy+PTZs27dF+xRVXMGvWrAOe16FDh6O6bksx8EqSJEnSEWrbti0LFy7knXfe4R/+4R/45je/mbJrPfbYYwwYMOCoxmjXrh3jxo3jmWeeaWjbuHEj//M//8Oll156tCUedwy8kiRJktQMNm3aRNeuXQHYsmULF1xwAUOGDKGwsJA5c+YAsHXrVi6++GKKiooYNGhQw6xqWVkZo0ePZujQoYwbN461a9fuM/6YMWPYvd1qhw4dmD59OkVFRYwcOZJ169YBUFFRwfjx4xk2bBjDhg3jtdde22ecSZMmMXPmzIbfzzzzDOPGjaO2tna/NbdmrtIsSZIkSUdo+/btFBcXs2PHDtauXctvf/tboG77nGeeeYZOnTqxfv16Ro4cyWWXXcZvfvMbTjrpJH79618DdbOrVVVV3HzzzcyZM4eCggJmzZrF9OnTefzxxw943a1btzJy5Ejuv/9+/vf//t88+uij3H333dx6663cfvvtnHPOOaxcuZJx48axdOnSPc4dN24c119/PZWVleTl5TFz5kymTp16wJpb83vSBl5JkiRJiTDhJ/P3abtkcA++PKo323fVMGXG7/c5ftXQXlxdcjIbtu7i6/9StsexWX8z6pDX3P1IM8D8+fO59tprWbRoETFGvvWtb/Hqq6+SkZHB6tWrWbduHYWFhdxxxx1MmzaNSy65hHPPPZdFixaxaNEiLrzwQgBqamro0aPHQa+bk5PDJZdcAsDQoUN54YUXAHjxxRf3eM9306ZNbNmyZY93cHNycrjsssuYPXs248eP5+2332bcuHEHrPnEE0885N/heGXglSRJkqRmMGrUKNavX09FRQVz586loqKCsrIysrOz6d27Nzt27KBfv3689dZbzJ07l7vvvpsLLriAL3zhCwwcOJD58/cN7AeSnZ3dMPOamZlJdXU1ALW1tbzxxhvk5uYe9PxJkybx3e9+lxgjl19+OdnZ2TzxxBP7rbk1M/BKkiRJSoSDzci2zck86PFu7XOaNKN7MO+99x41NTXk5eWxceNGTjjhBLKzs3n55Zf56KOPAFizZg3dunXjmmuuoUuXLjz22GPcddddVFRUMH/+fEaNGkVVVRXvv/8+AwcOPOwaxo4dy0MPPcSdd94JwMKFCykuLt6n35gxY7j22mv58Y9/zIMPPghwwJpbMwOvJEmSJB2h3e/wAsQYefLJJ8nMzGTy5MlceumlFBYWUlJSQv/+/QF49913ufPOO8nIyCA7O5uHH36YnJwcZs+ezS233MLGjRuprq7mtttuO6LA++CDD3LTTTcxePBgqqurOe+883jkkUf26ZeRkcFVV13Fr371K0aPHg1wwJpbsxBjbOkaUq6kpCTuXs1MkiRJUjIsXbqUM888s6XL0DG2v3/3EEJZjLFk775uSyRJkiRJSiQDryRJkiQpkQy8kiRJkqREMvBKkiRJkhLJwCtJkiRJSiQDryRJkiQpkQy8kiRJknSEMjMzKS4upqioiCFDhvD666836/hTpkxh9uzZAFx//fUsWbLkqMZ7/vnnKS4upri4mA4dOnDGGWdQXFzMtdde26TzH3nkEX7+858fVQ3HUlZLFyBJkiRJrVXbtm1ZuHAhUBcmv/nNbzJv3ryUXOuxxx476jHGjRvHuHHjABgzZgwPPPAAJSV7bl9bU1NDZmbmfs//2te+dtQ1HEvO8EqSJElSM9i0aRNdu3YFYMuWLVxwwQUMGTKEwsJC5syZA8DWrVu5+OKLKSoqYtCgQcyaNQuAsrIyRo8ezdChQxk3bhxr167dZ/wxY8ZQWloKQIcOHZg+fTpFRUWMHDmSdevWAVBRUcH48eMZNmwYw4YN47XXXmtS7b1792batGkMGTKEp59+mkcffZRhw4ZRVFTE+PHj2bZtGwD33nsvDzzwQEM906ZNY/jw4fTr14/f/e53R/HXSw0DryRJkiQdoe3bt1NcXEz//v25/vrr+fu//3sAcnNzeeaZZ3jrrbd4+eWXueOOO4gx8pvf/IaTTjqJd955h0WLFnHRRRdRVVXFzTffzOzZsykrK+OrX/0q06dPP+h1t27dysiRI3nnnXc477zzePTRRwG49dZbuf3221mwYAH/9m//xvXXX9/ke8nLy+Ott95i4sSJXHnllSxYsIB33nmHM888k5/97Gf7Pae6uprf//73/PCHP+Q73/lOk691rPhIsyRJkqRW7zv/sZglazY165gDTurEPZcOPGifxo80z58/n2uvvZZFixYRY+Rb3/oWr776KhkZGaxevZp169ZRWFjIHXfcwbRp07jkkks499xzWbRoEYsWLeLCCy8E6h4p7tGjx0Gvm5OTwyWXXALA0KFDeeGFFwB48cUX93jPd9OmTWzZsoUOHToc8n4nTJjQ8H3RokXcfffdfPbZZ2zZsqXhMei9XXnllQ01lJeXH/Iax5qBV5IkSZKawahRo1i/fj0VFRXMnTuXiooKysrKyM7Opnfv3uzYsYN+/frx1ltvMXfuXO6++24uuOACvvCFLzBw4EDmz5/f5GtlZ2cTQgDqFs6qrq4GoLa2ljfeeIPc3NzDrr99+/YN36dMmcKzzz5LUVERTzzxBK+88sp+z2nTps0+NRxPDLySJEmSWr1DzcQeC++99x41NTXk5eWxceNGTjjhBLKzs3n55Zf56KOPAFizZg3dunXjmmuuoUuXLjz22GPcddddVFRUMH/+fEaNGkVVVRXvv/8+Awce/j2NHTuWhx56iDvvvBOAhQsXUlxcfNjjbN68mR49elBVVcUvf/lLevbsedhjHA8MvJIkSZJ0hHa/wwsQY+TJJ58kMzOTyZMnc+mll1JYWEhJSQn9+/cH4N133+XOO+8kIyOD7OxsHn74YXJycpg9eza33HILGzdupLq6mttuu+2IAu+DDz7ITTfdxODBg6murua8887jkUceOexxvvvd7zJixAgKCgoYMWIEmzdvPuwxjgchxtjSNaRcSUlJ3L2amSRJkqRkWLp0KWeeeWZLl6FjbH//7iGEshhjyd59XaVZkiRJkpRIBl5JkiRJUiIZeCVJkiRJiWTglSRJkiQlkoFXkiRJkpRIBl5JkiRJUiIZeCVJkiRJiWTglSRJkqQjUFlZSXFxMcXFxZx44on07Nmz4feuXbuOSQ333nsvIQSWLVvW0PbDH/6QEAKlpaVNHueJJ55g6tSph92nvLycXr16UVtbu0d7cXExb7755n7HKS8vZ9CgQU2u7WgYeCVJkiTpCOTl5bFw4UIWLlzI1772NW6//faG3zk5Oc16rerq6gMeKywsZObMmQ2/n376aQYOHNis1z+Q3r17c8opp/C73/2uoe29995j8+bNjBgx4pjUcDAGXkmSJElqJlOmTGH27NkNvzt06ADAK6+8wpgxY7jqqqvo378/kydPJsYIQFlZGaNHj2bo0KGMGzeOtWvXAjBmzBhuu+02SkpK+NGPfnTAa15xxRXMmTMHgA8//JDOnTuTn5/fcPypp56isLCQQYMGMW3atIb2GTNm0K9fP4YPH85rr73W0F5RUcH48eMZNmwYw4YN2+PY/kyaNGmPwD1z5kwmTpxIeXk55557LkOGDGHIkCG8/vrrh/z7NbesY35FSZIkSUqFGRfv2zbwChh+A+zaBr+8et/jxV+CsybD1kr41bV7Hrvu181a3ttvv83ixYs56aSTOPvss3nttdcYMWIEN998M3PmzKGgoIBZs2Yxffp0Hn/8cQB27dp1yEeTO3XqxMknn8yiRYuYM2cOEyZMYMaMGQCsWbOGadOmUVZWRteuXRk7dizPPvssI0aM4J577qGsrIzOnTtz/vnnc9ZZZwFw6623cvvtt3POOeewcuVKxo0bx9KlSw94/S9+8YsUFxfz0EMPkZWVxaxZs3j66ac54YQTeOGFF8jNzeWDDz5g0qRJh/WYdXMw8EqSJEnSMTB8+HB69eoF1L3jWl5eTpcuXVi0aBEXXnghADU1NfTo0aPhnAkTJjRp7IkTJzJz5kyef/55XnrppYbAu2DBAsaMGUNBQQEAkydP5tVXXwXYo33ChAm8//77ALz44ossWbKkYexNmzaxZcuWA167e/fuDBo0iJdeeonu3buTlZXFoEGD2LhxI1OnTmXhwoVkZmY2jH8sGXglSZIkJcPBZmRz2h38ePu8ZpnRzcrKaljAqba2do/Fq9q0adPwPTMzk+rqamKMDBw4kPnz5++/rPbtm3TdSy65hDvvvJOSkhI6dep0FHdQV/cbb7xBbm5uk8/Z/Vhz9+7dmTRpEgA/+MEP6N69O++88w61tbWHNV5z8R1eSZIkSWomvXv3pqysDIDnnnuOqqqqg/Y/44wzqKioaAi8VVVVLF68+LCv265dO77//e8zffr0PdqHDx/OvHnzWL9+PTU1NTz11FOMHj2aESNGMG/ePCorK6mqquLpp59uOGfs2LE89NBDDb8XLlx4yOtfeeWVzJ07l1mzZjFx4kQANm7cSI8ePcjIyOAXv/gFNTU1h31fR8vAK0mSJEnN5IYbbmDevHkUFRUxf/78Q87Q5uTkMHv2bKZNm0ZRURHFxcVHvLjTxIkTGTJkyB5tPXr04Hvf+x7nn38+RUVFDB06lMsvv5wePXpw7733MmrUKM4++2zOPPPMhnMefPBBSktLGTx4MAMGDOCRRx455LW7dOnCqFGj6N69O6eddhoA3/jGN3jyyScpKirivffea/JsdXMKu1cGS7KSkpJ4rF+OliRJkpRaS5cu3SOoKT3s7989hFAWYyzZu68zvJIkSZKkRHLRKkmSJEk6zt1///17vGcLcPXVV+/zzq72ZOCVJEmSpOPc9OnTDbdHwEeaJUmSJEmJZOCVJEmSJCWSgVeSJEmSlEgGXkmSJElSIhl4JUmSJOkIVFZWUlxcTHFxMSeeeCI9e/Zs+L1r165jUsO9995LCIFly5Y1tP3whz8khEBpaWmTx3niiSeYOnXqYfeZMWNGwz3n5ORQWFhIcXExd911V5Ou++1vf5sXX3yxyXUeLldpliRJkqQjkJeXx8KFC4G64NmhQwf+7u/+LiXXqq6uJitr//GtsLCQmTNncvfddwPw9NNPM3DgwJTUsbfrrruO6667DoDevXvz8ssvk5+fv0efmpoaMjMz93v+fffdl9L6nOGVJEmSpGYyZcoUZs+e3fC7Q4cOALzyyiuMGTOGq666iv79+zN58mRijACUlZUxevRohg4dyrhx41i7di0AY8aM4bbbbqOkpIQf/ehHB7zmFVdcwZw5cwD48MMP6dy58x6h86mnnqKwsJBBgwYxbdq0hvYZM2bQr18/hg8fzmuvvdbQXlFRwfjx4xk2bBjDhg3b41hTdejQgTvuuIOioiLmz5/Pfffdx7Bhwxg0aBA33nhjw703/nv17t2be+65hyFDhlBYWMh777132NfdmzO8kiRJklq//7oLPnm3ecc8sRD+6nvNNtzbb7/N4sWLOemkkzj77LN57bXXGDFiBDfffDNz5syhoKCAWbNmMX36dB5//HEAdu3adchHkzt16sTJJ5/MokWLmDNnDhMmTGDGjBkArFmzhmnTplFWVkbXrl0ZO3Yszz77LCNGjOCee+6hrKyMzp07c/7553PWWWcBcOutt3L77bdzzjnnsHLlSsaNG8fSpUsP6163bt3KiBEj+Kd/+icABgwYwLe//W0AvvzlL/Of//mfXHrppfucl5+fz1tvvcU///M/88ADD/DYY48d1nX3ZuCVJEmSpGNg+PDh9OrVC4Di4mLKy8vp0qULixYt4sILLwTqHv/t0aNHwzkTJkxo0tgTJ05k5syZPP/887z00ksNgXfBggWMGTOGgoICACZPnsyrr74KsEf7hAkTeP/99wF48cUXWbJkScPYmzZtYsuWLYd1r5mZmYwfP77h98svv8w//uM/sm3bNjZs2MDAgQP3G3ivvPJKAIYOHcq///u/H9Y198fAK0mSJKn1a8aZ2KORlZVFbW0tALW1tXssXtWmTZuG75mZmVRXVxNjZODAgcyfP3+/47Vv375J173kkku48847KSkpoVOnTkdxB3V1v/HGG+Tm5h7xGLm5uQ3v7e7YsYNvfOMblJaWcvLJJ3PvvfeyY8eO/Z63+2+0++9ztHyHV5IkSZKaSe/evSkrKwPgueeeo6qq6qD9zzjjDCoqKhoCb1VVFYsXLz7s67Zr147vf//7TJ8+fY/24cOHM2/ePNavX09NTQ1PPfUUo0ePZsSIEcybN4/Kykqqqqp4+umnG84ZO3YsDz30UMPv3QtzHand4TY/P58tW7bs8Y5zqjnDK0mSJEnN5IYbbuDyyy+nqKiIiy666JAztDk5OcyePZtbbrmFjRs3Ul1dzW233XZEqyxPnDhxn7YePXrwve99j/PPP58YIxdffDGXX345ULey9KhRo+jSpQvFxcUN5zz44IPcdNNNDB48mOrqas477zweeeSRw65nty5dunDDDTcwaNAgTjzxRIYNG3bEYx2usHt1rJQMHsJFwI+ATOCxGOP39jp+HvBDYDAwMcY4u779fOAHjbr2rz/+bAjhCWA0sLH+2JQY40H/l0NJSUk8nD2oJEmSJB3/li5dyplnntnSZegY29+/ewihLMZYsnfflM3whhAygR8DFwKrgAUhhOdijEsadVsJTAH22KwqxvgyUFw/TjdgGfDfjbrcuTscS5IkSZK0P6l8pHk4sCzGuBwghDATuBxoCLwxxvL6Y7UHGecq4L9ijNtSV6okSZIkHb/uv//+Pd6zBbj66qv3eWdXe0pl4O0JfNzo9ypgxBGMMxH4//Zquz+E8G3gJeCuGOPOIytRkiRJko5/06dPN9wegeN6leYQQg+gEHi+UfM3qXundxjQDZh2gHNvDCGUhhBKKyoqUl6rJEmSpGMvlWsS6fhzuP/eqQy8q4GTG/3uVd92OL4IPBNjbFjLO8a4NtbZCcyg7tHpfcQYfxpjLIkxluzeTFmSJElScuTm5lJZWWnoTRMxRiorKw9rf+BUPtK8AOgbQjiVuqA7EfjSYY4xiboZ3QYhhB4xxrUhhABcASxqjmIlSZIktS69evVi1apV+ERn+sjNzaVXr15N7p+ywBtjrA4hTKXuceRM4PEY4+IQwn1AaYzxuRDCMOAZoCtwaQjhOzHGgQAhhN7UzRDP22voX4YQCoAALAS+lqp7kCRJknT8ys7O5tRTT23pMnQcS+k+vMcL9+GVJEmSpOQ60D68x/WiVZIkSZIkHSkDryRJkiQpkQy8kiRJkqREMvBKkiRJkhLJwCtJkiRJSiQDryRJkiQpkQy8kiRJkqREMvBKkiRJkhLJwCtJkiRJSiQDryRJkiQpkQy8kiRJkqREMvBKkiRJkhLJwCtJkiRJSiQDryRJkiQpkQy8kiRJkqREMvBKkiRJkhLJwCtJkiRJSiQDryRJkiQpkQy8kiRJkqREMvBKkiRJkhLJwCtJkiRJSiQDryRJkiQpkQy8kiRJkqREMvBKkiRJkhLJwCtJkiRJSiQDryRJkiQpkQy8kiRJkqREMvBKkiRJkhLJwCtJkiRJSiQDryRJkiQpkQy8kiRJkqREMvBKkiRJkhLJwCtJkiRJSiQDryRJkiQpkQy8kiRJkqREMvBKkiRJkhLJwCtJkiRJSiQDryRJkiQpkQy8kiRJkqREMvBKkiRJkhLJwCtJkiRJSiQDryRJkiQpkQy8kiRJkqREMvBKkiRJkhLJwCtJkiRJSiQDryRJkiQpkQy8kiRJkqREMvBKkiRJkhLJwCtJkiRJSiQDryRJkiQpkQy8kiRJkqREMvBKkiRJkhLJwCtJkiRJSiQDryRJkiQpkQy8kiRJkqREMvBKkiRJkhLJwCtJkiRJSiQDryRJkiQpkQy8kiRJkqRESmngDSFcFEL4YwhhWQjhrv0cPy+E8FYIoTqEcNVex2pCCAvrP881aj81hPBm/ZizQgg5qbwHSZIkSVLrlLLAG0LIBH4M/BUwAJgUQhiwV7eVwBTgX/czxPYYY3H957JG7d8HfhBj7AN8Cvx1sxcvSZIkSWr1UjnDOxxYFmNcHmPcBcwELm/cIcZYHmP8A1DblAFDCAH4PDC7vulJ4IrmK1mSJEmSlBSpDLw9gY8b/V5V39ZUuSGE0hDCGyGE3aE2D/gsxlh9hGNKkiRJktJEVksXcBCfizGuDiGcBvw2hPAusLGpJ4cQbgRuBDjllFNSVKIkSZIk6XiVyhne1cDJjX73qm9rkhjj6vr/LgdeAc4CKoEuIYTdQf2AY8YYfxpjLIkxlhQUFBx+9ZIkSZKkVi2VgXcB0Ld+VeUcYCLw3CHOASCE0DWE0Kb+ez5wNrAkxhiBl4HdKzp/BZjT7JVLkiRJklq9lAXe+vdspwLPA0uBX8UYF4cQ7gshXAYQQhgWQlgFXA38JISwuP70M4HSEMI71AXc78UYl9Qfmwb8bQhhGXXv9P4sVfcgSZIkSWq9Qt2kabKVlJTE0tLSli5DkiRJkpQCIYSyGGPJ3u2pfKRZkiRJkqQWY+CVJEmSJCWSgVeSJEmSlEgGXkmSJElSIhl4JUmSJEmJZOCVJEmSJCWSgVeSJEmSlEgGXkmSJElSIhl4JUmSJEmJZOCVJEmSJCWSgVeSJEmSlEgGXkmSJElSIhl4JUmSJEmJZOCVJEmSJCWSgVeSJEmSlEgGXkmSJElSIhl4JUmSJEmJZOCVJEmSJCWSgVeSJEmSlEgGXkmSJElSIhl4JUmSJEmJZOCVJEmSJCWSgVeSJEmSlEgGXkmSJElSIhl4JUmSJEmJZOCVJEmSJCWSgVeSJEmSlEgGXkmSJElSIhl4JUmSJEmJZOCVJEmSJCWSgVeSJEmSlEgGXkmSJElSIhl4JUmSJEmJZOCVJEmSJCWSgVeSJEmSlEgGXkmSJElSIhl4JUmSJEmJZOCVJEmSJCWSgVeSJEmSlEgGXkmSJElSIhl4JUmSJEmJZOCVJEmSJCWSgVeSJEmSlEgGXkmSJElSIhl4JUmSJEmJZOCVJEmSJCWSgVeSJEmSlEgGXkmSJElSIhl4JUmSJEmJZOCVJEmSJCWSgVeSJEmSlEgGXkmSJElSIhl4JUmSJEmJZOCVJEmSJCWSgVeSJEmSlEgGXkmSJElSIhl4JUmSJEmJZOCVJEmSJCWSgVeSJEmSlEgpDbwhhItCCH8MISwLIdy1n+PnhRDeCiFUhxCuatReHEKYH0JYHEL4QwhhQqNjT4QQVoQQFtZ/ilN5D5IkSZKk1ikrVQOHEDKBHwMXAquABSGE52KMSxp1WwlMAf5ur9O3AdfGGD8IIZwElIUQno8xflZ//M4Y4+xU1S5JkiRJav1SFniB4cCyGONygBDCTOByoCHwxhjL64/VNj4xxvh+o+9rQgh/AgqAz5AkSZIkqQlS+UhzT+DjRr9X1bcdlhDCcCAH+LBR8/31jzr/IITQ5ujKlCRJkiQl0XG9aFUIoQfwC+C6GOPuWeBvAv2BYUA3YNoBzr0xhFAaQiitqKg4JvVKkiRJko4fqQy8q4GTG/3uVd/WJCGETsCvgekxxjd2t8cY18Y6O4EZ1D06vY8Y409jjCUxxpKCgoIjugFJkiRJUuuVysC7AOgbQjg1hJADTASea8qJ9f2fAX6+9+JU9bO+hBACcAWwqFmrliRJkiQlQsoCb4yxGpgKPA8sBX4VY1wcQrgvhHAZQAhhWAhhFXA18JMQwuL6078InAdM2c/2Q78MIbwLvAvkA/8nVfcgSZIkSWq9QoyxpWtIuZKSklhaWtrSZUiSJEmSUiCEUBZjLNm7/bhetDkB+nQAACAASURBVEqSJEmSpCNl4JUkSZIkJZKBV5IkSZKUSAZeSZIkSVIiGXglSZIkSYlk4JUkSZIkJZKBV5IkSZKUSAZeSZIkSVIiGXglSZIkSYlk4JUkSZIkJZKBV5IkSZKUSAZeSZIkSVIiGXglSZIkSYlk4JUkSZIkJZKBV5IkSZKUSAZeSZIkSVIiGXglSZIkSYlk4JUkSZIkJZKBV5IkSZKUSAZeSZIkSVIiGXglSZIkSYlk4JUkSZIkJZKBV5IkSZKUSAZeSZIkSVIiGXglSZIkSYlk4JUkSZIkJZKBV5IkSZKUSAZeSZIkSVIiGXglSZIkSYlk4JUkSZIkJZKBV5IkSZKUSAZeSZIkSVIiGXglSZIkSYmUFoF30/aqli5BkiRJknSMpUfg3VHd0iVIkiRJko6xtAi8O6trWroESZIkSdIxliaBt7alS5AkSZIkHWNpEXhraiMbt/keryRJkiSlk7QIvAArKre2dAmSJEmSpGMofQLv+i0tXYIkSZIk6RhKo8C7raVLkCRJkiQdQ2kReHMyM1ix3keaJUmSJCmdpEfgzcqg3MArSZIkSWklLQJvm6y6Gd4YY0uXIkmSJEk6RtIm8G7ZWc36LbtauhRJkiRJ0jGSFoE3N6MGwPd4JUmSJCmNpEXgbburEsD3eCVJkiQpjaRF4M2o2UV2ZmC5gVeSJEmS0kZaBF6qd3BKt3bO8EqSJElSGkmPwFtTRb9uWb7DK0mSJElpJD0CL5GzOmygvHIrtbVuTSRJkiRJ6SBNAi/0z17Hzupa1m7a0dKlSJIkSZKOgbQJvJ9jDeBKzZIkSZKULtIj8GbmULBzJYArNUuSJElSmmhy4A0hfC6E8Jf139uGEDqmrqxmltWGtpvKaZud6QyvJEmSJKWJJgXeEMINwGzgJ/VNvYBnU1VUs8tqQ6j8gM91a+tKzZIkSZKUJpo6w3sTcDawCSDG+AFwQqqKanZZubBjI4O7VTnDK0mSJElpoqmBd2eMcdfuHyGELKD17O+T1QaA4rYVrNywjeqa2hYuSJIkSZKUak0NvPNCCN8C2oYQLgSeBv4jdWU1s6xcAPplfUJ1bWTVp9tbuCBJkiRJUqo1NfDeBVQA7wJ/A8wF7j7USSGEi0IIfwwhLAsh3LWf4+eFEN4KIVSHEK7a69hXQggf1H++0qh9aAjh3foxHwwhhENWn5kDmW3oVVu3NdGKSh9rliRJkqSka1LgjTHWxhgfjTFeHWO8qv77QR9pDiFkAj8G/goYAEwKIQzYq9tKYArwr3ud2w24BxgBDAfuCSF0rT/8MHAD0Lf+c1FT7oFup9Fte93WRCsqDLySJEmSlHRNXaW5bwhhdghhSQhh+e7PIU4bDiyLMS6vf/93JnB54w4xxvIY4x+AvV+qHQe8EGPcEGP8FHgBuCiE0APoFGN8oz5w/xy4oin3QH4fsj/7kI65WZQ7wytJkiRJidfUR5pnUDezWg2cT13Q/JdDnNMT+LjR71X1bU1xoHN71n8//DHz+hI+XUHfvDZuTSRJkiRJaaCpgbdtjPElIMQYP4ox3gtcnLqyjl4I4cYQQmkIobSiogLy+0JtNUM6bzLwSpIkSVIaaPK2RCGEDOCDEMLUEMIXgA6HOGc1cHKj373q25riQOeurv9+yDFjjD+NMZbEGEsKCgogry8AhbkVrP5sOzuqappYiiRJkiSpNWpq4L0VaAfcAgwFvgx85aBnwAKgbwjh1BBCDjAReK6J13seGBtC6Fq/WNVY4PkY41pgUwhhZP3qzNcCc5o0Yt7pAJwe1hIjfLxhWxNLkSRJkiS1Rk1dpXlBjHFLjHFVjPG6GOOVMcY3DnFONTCVuvC6FPhVjHFxCOG+EMJlACGEYSGEVcDVwE9CCIvrz90AfJe60LwAuK++DeAbwGPAMuBD4L+adKftukG7PHpU170CvNzHmiVJkiQp0bIOdjCEcNAZ2RjjZYc4Ppe6PXsbt3270fcF7PmIcuN+jwOP76e9FBh0sOseUF5fOm8rB6DcwCtJkiRJiXbQwAuMom615KeAN4GQ8opSKb8PWe//N/kdcly4SpIkSZIS7lCPNJ8IfIu6GdUfARcC62OM82KM81JdXLPL6wtb/8SArhh4JUmSJCnhDhp4Y4w1McbfxBi/Aoyk7r3ZV0IIU49Jdc0trw8AQztWGnglSZIkKeEO9UgzIYQ21O25OwnoDTwIPJPaslIkv25rogE5f+JPmzuydWc17dsc8k8gSZIkSWqFDrVo1c+pe5x5LvCdGOOiY1JVqnQ9FUImp7IGOJ0V67cyqGfnlq5KkiRJkpQCh3qH9xqgL3X78L4eQthU/9kcQtiU+vKaWVYOdP0cJ+z6GIDySh9rliRJkqSkOugMb4yxSfv0tip5femwcQUAKyoMvJIkSZKUVMkLtIeS35eMDcs5qVMOK5zhlSRJkqTESr/Am3c6VG9naNftrtQsSZIkSQmWhoG3bqXms9qvp9zAK0mSJEmJlX6Bt35rov5Zn/Dptio+27arhQuSJEmSJKVC+gXeDt0hpyMn164G8LFmSZIkSUqo9Au8IUB+H/J21m1NZOCVJEmSpGRKv8ALkNeHtpuWkxHwPV5JkiRJSqg0Dbx9CRtXcXrXTJYbeCVJkiQpkdIz8Ob3ASIjOn1GuXvxSpIkSVIipWfgrd+aaHDbClZUbCXG2MIFSZIkSZKaW5oG3tMB6JP5CVt31VCxZWcLFyRJkiRJam7pGXhz2kOnnvSsXgXAigofa5YkSZKkpEnPwAuQ14cu2z8C8D1eSZIkSUqg9A28+X3J+exDcjKDKzVLkiRJUgKlb+DN60vYuYnBXXe5F68kSZIkJVD6Bt78PgAM67iBFQZeSZIkSUqc9A289VsTDWrzJ8ort1Fb69ZEkiRJkpQk6Rt4O/eCzDacGtayq7qWNRu3t3RFkiRJkqRmlL6BNyMT8k7nxKqPAShfv62FC5IkSZIkNaf0DbwAeX3otLUcgBXrt7RsLZIkSZKkZpXegTe/L5kbP6JjdmSFM7ySJEmSlCjpHXjz+hJqqxnRdYszvJIkSZKUMGkeeOu2Jhrafj3llc7wSpIkSVKSpHfgrd+L98ycT1i5YRtVNbUtXJAkSZIkqbmkd+Bt2xXa5XNKXENNbWTVp25NJEmSJElJkd6BFyC/LwU7VwKu1CxJkiRJSWLgzetD+83lAK7ULEmSJEkJYuDN60PGtgpOyt3lDK8kSZIkJYiBN78vAH/R5VPKneGVJEmSpMQw8ObVBd7ithWsWL+1hYuRJEmSJDUXA2/X3hAy6Zf1CWs2bmdHVU1LVyRJkiRJagYG3qwc6NqbnjWriRFWbvCxZkmSJElKAgMvQF4fuu2o25poeYWPNUuSJElSEhh4AfL7krupnEAt5ZUGXkmSJElKAgMvQF4fQvV2BrbfzApneCVJkiQpEQy80LA10YhOlaxwhleSJEmSEsHACw1bExXmujWRJEmSJCWFgRegwwnQphOnZXxCxeadbNlZ3dIVSZIkSZKOkoEXIATIO52Tqj4GoNxZXkmSJElq9Qy8u+X1pfO2jwB8rFmSJEmSEsDAu1t+X7K3rCaXnQZeSZIkSUoAA+9ueX0AGNbxUx9pliRJkqQEMPDuVr81UUnHSpYbeCVJkiSp1TPw7tbtNAAG5Kyj3L14JUmSJKnVM/DultMeOvWiN2v5bFsVn27d1dIVSZIkSZKOgoG3sfw+nLBrJQArnOWVJEmSpFbNwNtYXl86bikHIisqDLySJEmS1JoZeBvL70vGrs2ckLHJ93glSZIkqZUz8DaWdzoAIzttcKVmSZIkSWrlDLyN5dVtTXRWu/XuxStJkiRJrZyBt7HOJ0NWLmdkf8KK9VuJMbZ0RZIkSZKkI2TgbSwjA7qdzsm1q9m2q4aKzTtbuiJJkiRJ0hFKaeANIVwUQvhjCGFZCOGu/RxvE0KYVX/8zRBC7/r2ySGEhY0+tSGE4vpjr9SPufvYCc1adH4f8nZ8DOB7vJIkSZLUiqUs8IYQMoEfA38FDAAmhRAG7NXtr4FPY4x9gB8A3weIMf4yxlgcYywGvgysiDEubHTe5N3HY4x/atbC8/rSduvHZFHte7ySJEmS1IqlcoZ3OLAsxrg8xrgLmAlcvlefy4En67/PBi4IIYS9+kyqP/fYyOtDqK3m9Kz1rDDwSpIkSVKrlcrA2xP4uNHvVfVt++0TY6wGNgJ5e/WZADy1V9uM+seZ/34/Afno5Net1DyiY6WBV5IkSZJaseN60aoQwghgW4xxUaPmyTHGQuDc+s+XD3DujSGE0hBCaUVFRdMvmtcHgMFtKwy8kiRJktSKpTLwrgZObvS7V33bfvuEELKAzkBlo+MT2Wt2N8a4uv6/m4F/pe7R6X3EGH8aYyyJMZYUFBQ0veq2XaB9AX0yP+GjDduoqXVrIkmSJElqjVIZeBcAfUMIp4YQcqgLr8/t1ec54Cv1368CfhvrN78NIWQAX6TR+7shhKwQQn7992zgEmARh7CzuvbwKs/ry0nVq9hVXcuaz7Yf3rmSJEmSpONCygJv/Tu5U4HngaXAr2KMi0MI94UQLqvv9jMgL4SwDPhboPHWRecBH8cYlzdqawM8H0L4A7CQuhniRw9Vy/KKLeysrml68Xmn02X7RwCUV/pYsyRJkiS1RlmpHDzGOBeYu1fbtxt93wFcfYBzXwFG7tW2FRh6uHVU10Zml61i8ojPNe2E/L7k7KikE1tZsX4r5/Y9jEeiJUmSJEnHheN60arm0jY7k0fmfUh1TRMfbc6rW6l5QM46F66SJEmSpFYqLQLvCZ3a8PGG7Tz3zpqmnVC/NVFJxw0GXkmSJElqpdIi8HbKzab/iR15Ycm6pp3QtTeETAa2+RPlBl5JkiRJapVS+g7v8eTnXx1Ofoc2TeucmQ1de3NaWMPHn26nqqaW7My0+H8DkiRJkpQYaZPiTuiUS0ZGYPOOKup3Pjq4/L6cuGsVNbWRjzdsS32BkiRJkqRmlTaBF2Dxmo38xfd+y2/f+9OhO+f1oeO2jwjU+h6vJEmSJLVCaRV4+3XvSKfcbP7/l5cdepY3vy8ZNTvpGSoNvJIkSZLUCqVV4M3OzOBrY07n7ZWfMf/DyoN3rt+aqDC3wsArSZIkSa1QWgVegKuH9uKEjm146LfLDt6xfmuiIe3XU15p4JUkSZKk1ibtAm9udiY3nnca85dX8v66zQfu2L4A2nTizOx1rKgw8EqSJElSa5M22xI19qURpzCsdzf6de944E4hQF4fTtm2mjUbd7Cjqobc7MxjV6QkSZIk6aik3QwvQLucLIpO7gJw8MWr8vuSv/NjAB9rliRJkqRWJi0D727f/c8l3Dpz4YE75PWl3fa15LKTcheukiRJkqRWJa0Db9vsTP7jD2v44EDv8ub3AeDU8AnLDbySJEmS1KqkdeD96jmnkpuVyT+/8uH+O+TVBd7idhXO8EqSJElSK5PWgbdb+xwmjziF595Zw8rKbfvpcDoAxW3di1eSJEmSWpu0DrwAN5x3Gpkh8PC8/czy5rSDzifTN/MTVqzfTyCWJEmSJB230nJbosa6d8rlgS8WMeSULvvvkNeHnhVrWL9lJ5t3VNExN/vYFihJkiRJOiJpP8MLcFnRSfTq2m7/B/P70m37R0Ck3FleSZIkSWo1DLz1Pli3meufLGX9lp17HsjrQ3b1FgrYyAr34pUkSZKkVsPAWy8jI/DSe+v42f+s2PNA/UrNp2esZUWFgVeSJEmSWgsDb73TCzpwcWEPfjH/IzZuq/rzgfy+AJzVbj3lzvBKkiRJUqth4G3kpvP7sGVnNU+8Xv7nxk69IKsthW0reP3D9fxp844Wq0+SJEmS1HQG3kbO7NGJvzyzOzNeX8GWndV1jRkZkHc6Z3f5lE3bq7luxgI276g6+ECSJEmSpBZn4N3LLRf04fpzTiU0bszrQ+et5Tx8zRD++Mlm/uYXZeysrmmpEiVJkiRJTWDg3cvgXl2Y+vm+tG/TaIvivD7waTljTu/CP141mNc/rORvZ71DTW1suUIlSZIkSQeVdegu6SfGyK/fXUsgcPHgHnULV8Ua+LScK4f0o3LLLu6fu5S8Djl857KBhBAOPagkSZIk6Zgy8B7AE6+Vs+az7Vw4oDs5eXUrNVO5DAr6ccN5p1GxZSc/fXU5J3Rsw9TP923ZYiVJkiRJ+/CR5v0IITD1831Ys3EHz7y9CvLr9uKl8oOGPndd1J8rz+rJA//9PjN/v7KFKpUkSZIkHYiB9wBG9yugsGdnHn7lQ6qzO0L7E2D9nwNvRkbg+1cNZswZBXzrmXf578WftGC1kiRJkqS9GXgPIITATef3obxyG79+d23de7xr3oba2oY+2ZkZ/PPkIRT26sLNT73NgvINLVixJEmSJKmx9Ai8tTV1n8M0dkB3xg7oTrucLCj+EqxbBAv/ZY8+7XKymDFlGD27tuWvn1jAHz/Z3FxVS5IkSZKOQogx+VvrlJyUGUtv7ABtOkFu57rP5++GM/4KNqyANx/5c/vuT6/h0LE7VO+Equ2Q0xF+fimsWwxTS6FDwR7XWPXpNsY//DoA//b1v6BX13YtcauSJEmSlHZCCGUxxpK929NjlebOvWDM12HHxj9/2nSsO7Z5LSx8CnZu3POcL/0KOo6DD1+GpyYQCXD65wm7tsLz34Lxj+7RvVfXdjz51eFc/ch8rn3898z+2l/QrX3OMbpBSZIkSdLe0mOGt6QklpaWHrxTbQ3s3PznQNz1c3UzvRuW8/aLM1nz7stcnPl7GHodlM2ALz8Dp39+n2F+v2ID1/zsTQb06MS/3jCi7nFoSZIkSVLKHGiGNz3e4W2KjExo26Uu6PYYXBd2AbqdxsAr7+LBtjexnTZs2boV8vrAf/5t3aPOexl+ajcemnQWf1j1Gd/45VtU1dTu00eSJEmSlHoG3ibIycrgngnn8G9cwPPvVfJu8T3w6Qp49f/ut/+4gSdy/xcKeeWPFUyb/Qdqa5M/iy5JkiRJxxuft22ivzg9n5NuepQb/6WM6jcjLxZ9iYzXfgSDroLuA/bpP2n4KazfvJN/euF9Cjq24Zv/68wWqFqSJEmS0pczvIehd0EH/v0bZ/OLS9qRccHfE9t0ouY/bt1jb97Gpn6+D9eO+hw/eXU5j/1u+TGuVpIkSZLSm4H3MHWoWEjPmRfCileZnf91Mlf9nk//59H99g0hcM+lA/lfhSfyf369lGffXn2Mq5UkSZKk9GXgPVw9h0L+GTD/x3Qb+WXejAPJ+u29lC56b7/dMzMCP5hQzKjT8vi7p99h3vsVx7hgSZIkSUpPBt7DFQKM/Dp88gcu6LCCHtc8TBuqWDvrNma8toL9bfPUJiuTn1w7lH7dO/4/9s47zo6q7OPfub3svdtrdjfJpnfSG5CE3rsg0i0g2FFUUF+R14avoiJFpIjSpChopJdAgPQe0pPdZFu299vvnXn/eG7bZBPSG+f7+cxn5p4zd+7M3Cnn9zzPeQ63Pr2cVTUdR2HHFQqFQqFQKBQKheKzhRK8B8LYq8CZDYseonzIOIxTvsuF5oUsevN5GrqCfX7F67Dy5Bcnk5th47rHF/P80mqVvVmhUCgUCoVCoVAoDiNK8B4INhdMvAmq5kOwC/us2zHyhvJA5tMUOw0Mw6DDH97tawUeB89+eRrDizz84J9r+fxfFrGlsfsoHIBCoVAoFAqFQqFQnPgowXugzPwmfHstOLxgsaNd8Aes3TXwwa95enE1Z9z3AUu3t+32tbIcF8/fPJ17Lx/DpsZuzrv/Q3775iaCkdhROAiFQqFQKBQKhUKhOHFRgvdAcWaDIxMMA2JRGDATxl8HCx7gVG8DHoeVq/+yiKcW7ditX6/JpHHV5HLe/e4sLhxbwgPztnL2H+bz4RaV0EqhUCgUCoVCoVAoDhVK8B4MgQ54eCYsfUw+n3kPOLPp//FdvHLbdE4eksdPXvmEO/+1llB0dw9uXoad+646iWe/PBWTpnHd40v41j9W0twdOsIHolAoFAqFQqFQKBQnHkrwHgzOLLC5YfGfQY+BKwfO+RXULSPzk7/z+A2T+dqcQTy/rIblO9r3uJkZg/N4/Vun8M3Th/D62gZO/937PLtYJbVSKBQKhUKhUCgUioNBCd6DZdqt0F4Fm9+Uz2M+BxWz4d17MPc0cMfZw3n7O7OYMSgPgNaevr23DquZ288cymvfOoURxV7uenktn3tkIZsaVFIrhUKhUCgUCoVCoTgQlOA9WEZcBN5SWPSQfNY0OP8+iIXhjR8AMLggA4DFla2cfO88nl60Y4/e28EFGfzj5mn89nPjqGzu4fz7P+TeNzYSCKukVgqFQqFQKBQKhUKxPyjBe7CYLTD1Ztj+ITSslbLcQXDqHbD+37DpjeSqw4o8TBqQzY9f+YRz//ghb3yys0/hq2kaV0ws5d3vzuaS8f14+P1tnPWHD3h/U9OROiqFQqFQKBQKhUKhOO7Rds0gfCIyadIkY9myZYfvBwLtImxHXwYWu5RFw/DIKRD2wW2LwC5eXl03mLumnj++s4XKFh/TK3J59itT0TRtj5tfuK2VH72ylspmHxeMLeZ/LhhJgddx+I5HoVAoFAqFQqFQKI4jNE1bbhjGpN3KleA9jOxYCH89B6Z/Hc7+Ra+qaEznP6vrCUV1rp5Sjq4bLKpsZfqg3D7Fbyga45EPKnlg3lbsZhPfP3c410wpx2Tas1BWKBQKhUKhUCgUis8CSvAebsFrGLDwAbA4YMpXUuVzvwUrnoKb50HxuD1+/Y1PdvLVp1cwvjyL288cysmD8/oUvpXNPfz4lU9YsK2Vk8qy+NlFoxhXlnU4jkihUCgUCoVCoVAojgv2JHhVH95DhaZB5QfwwW8gmpaJ+Yy7wZUrwlffc+Kp04YX8stLx9DYGeS6x5dw5SMLWbCtZbf1KvIzeObLU/n9VeOobfdz8YMf890XVtPYFTz0x6RQKBQKhUKhUCgUxzFK8B5Kpt8Gvib45F+pMme2jM1bvxKWPrbHr9osJr4wtZx5d8zmfy8eRXWbnzv/tZZoTN9tXU3TuHR8KfO+N5tbZlUwd3U9c377Pg+8t4VgRGVzVigUCoVCoVAoFApQIc2HFsOAh6aB2Qa3zBevb6L86cuhZjF8bQlk9vvUTQUjMWrb/Qwu8BAIx7jjpdV88eSBTCjP3m3dHa0+fvXaRt5Y10C/LCd3njec88cU7zURlkKhUCgUCoVCoVCcKKiQ5iOBpsG0W6FhDez4uHf5+b+TkObXv79Pm3JYzQwu8ACwpambBdtaueyhBdz01yWsqe3otW7/XDd/vm4iz35lKl6nla8/u5IrH1nI2trOQ3ZoCoVCoVAoFAqFQnG8oQTvoWbsVTDiQrA6e5fnDITZP4CN/4WNr+7fJkuz+PD7c/jBOcNZWdPBRQ98zJf/tgxfKNprvRmD8vjvN07mV5eNobLZx0UPfsQdL66mSfXvVSgUCoVCoVAoFJ9BDmtIs6Zp5wB/BMzAY4Zh/HqXejvwd2Ai0ApcZRjGdk3TBgAbgE3xVRcZhvHV+HcmAk8CTuA14FvGpxzEURuWaFdiEXhklozb+5X3wFu835voDkb424LtrKrp5NHrJ6JpGvfMXY/VojGqJJNRJV4G5rrpCUd58L2tPPFxFTazidvmDOZLJw/EYTUfhgNTKBQKhUKhUCgUiqPHER+WSNM0M7AZOBOoBZYCVxuGsT5tnduAsYZhfFXTtM8DlxqGcVVc8P7XMIzRfWx3CfBNYDEieO83DOP1ve3LURG8nXXQsBaGndO7fOdq+Ot5kFUON70mSa0OAsMwuOqRRaysaScSk//SZTNz/fQB/PDc4Wxv8XHXy2tZsK2V0mwnd503gnNHF6n+vQqFQqFQKBQKheKEYU+C13IYf3MKsNUwjMr4DvwDuBhYn7bOxcDd8eWXgAe0vSgxTdOKAa9hGIvin/8OXALsVfAeFd65Gza9DrevB4c3VV48Dj7/DDzzOXj2KrjuFbC5DvhnNE3jha9OJxzV2dLUzbr6LtbXdzG4IAOALJeVBdtaMZuguTvEbc+soH+Oix+dP4KzRhUd5EEqFAqFQqFQKBQKxbHL4ezD2w+oSftcGy/rcx3DMKJAJ5AbrxuoadpKTdM+0DTtlLT1az9lm8cGU78K4W5Y9czudRWz4fLHoHYpvHC9hDofJDaLiVElmVw5qYy7LxrFFRNLk+UPfGE8N586iMkDsnHbzOxo83PLU8v5/kurWb6jja8/u4IXltbQ4Q8f9H4oFAqFQqFQKBQKxbHC4fTwHgw7gXLDMFrjfXZf0TRt1P5sQNO0m4GbAcrLyw/DLn4KpROhbCos/jNMuRlMu/SdHXkxXPB7mPsteOU2uPQRMB16+4PLZuGCsSVcMLYEkBDobU0+nlq0nWeXVPOf1fVYTCb+u2Ynd72sMWNwHueNLuKCcSVk2I/Vy0OhUCgUCoVCoVAoPp3D6eGtA8rSPpfGy/pcR9M0C5AJtBqGETIMoxXAMIzlwDZgaHz90k/ZJvHv/cUwjEmGYUzKz88/BIdzAEy7Fdq3w+Y3+q6feCOc/j+w9gV4804Zr/cwo2kagwsz+NnFo3n7O7M4ZUg+PfFsz267hWXb2/jhv9aypraDmG6wtamH1p7QYd8vhUKhUCgUCoVCoTjUHE4X3lJgiKZpAxFR+nngC7us8x/gBmAhcAXwnmEYhqZp+UCbYRgxTdMqgCFApWEYbZqmdWmaNg1JWnU98KfDeAwHx/ALIXsANK2H4ef3vc7Jt4OvFRY9CK48mHXHEdu9AXluHr1+EmtqO1iwrZXlO9pZUd2OPxzjC48uxmO3YDZrdPojjCjxcPmEUi4+qR95GfYjto8KhUKhUCgUCoVCcaAc7mGJzgP+gAxL9IRhGL/QNO0eYJlhGP/RNM0BPAWMB9qAzxuGUalp2uXAPUAE0IGfGoYxN77NSaSGJXod+MYxvIBTgwAAIABJREFUPSxRJAhWx97X0XV45VZY8w84/3cw+ctHZt/6wDAMqtv8SfG7YGsrVS0+0k/wkIIMbj61gon9sxmY51YZnxUKhUKhUCgUCsVR5YgPS3QscUyMw9vdCJ7CPdfHIvD8tbD5TbjiCRh92ZHbt0+hJxRlVXU7b65rZP7mZpq6QwQiMQAsJo2KfDdnjCjk1KH5nFSWpcb6VSgUCoVCoVAoFEcUJXiPpuBd/Ai89RP4zieQUbDn9SIBeOpSqF0G17wAg047cvu4H+i6wbbmHl5bu5PHP6qiKxhN1llMGjMH53HJ+BJGlXhp7g6T6bSS5bKS5bLhtpmVR1ihUCgUCoVCoVAcUpTgPZqCt2ULPDAJZt8Js3+493UDHfDk+dBWBTf8B0p3+8+OObY0dvOvFXXMXV1PfWeALJeNNl8Ykwb6LpeXxaTx5ndOZVB+Bq+t3ck/l9eS6bKKKHbayHJZ+dykUlw2C/5wFJvZhMV8OHOrKRQKhUKhUCgUiuOdPQleNe7MkSBvCAw5C5Y+Bid/Byx7SfrkzIJr/wlPnA3PXAFffBPyhx25fT0AhhR6+MG5w/nBucMBiMV0Vtd18u9Vdby1rpGdnUEASrIclOe46Il7hP3hGA1dQTY2dNMZiCSzRV8yXoZW/tN7W3l0fiX9sp2U57goz3HRP9fFF2cOxGI2EdMNzCblLVYoFAqFQqFQKBR9ozy8R4pt70m48iUPw0m7Jqvug7ZKePxsMFtF9GaVffp3jkEMQ4Y2emt9I2+ua2BNbScAgwsyOHtUIWeNLGJsaSaaphGJ6XQFIuS4bWiaxoKtLXy0tYUdbX5q2vzsaPWj6wZr7j4LTdP4zvOr+HBLc1IMl+e6GVyQwUXjSo7yUSsUCoVCoVAoFIojiQppPtqC1zDgoeng8MKX3tq37zSshb+eL/1+v/gGuPMO7z4eAeo7Ary1roG31jeyuKqNmG5QnOngrJGFnDWqiCkDc7DuJYS5JxQlwy6BCS+vrGVJVRs7WkUM7+wMMLggg7e+MwuAG/+6hMauEDMH5TJrWD5TBuZgt6iEWgqFQqFQKBQKxYmGErxHW/CCCNjMUnBm7/t3diwQz3DBCLhhLtg9h2//jjDtvjDvbmzirXUNzN/STDCik+m0cvqIAs4bXczsYfn71X83HNXp8Icp8MowUA/O28rCba0sqWojHNNxWs3cNHMA3z9n+OE6JIVCoVAoFAqFQnEUUIL3WBC8CULdYHGCeR+7UG96A/7xBRgwE655ae99gI9T/OEo8ze38Na6Bt7d2ERnIEKh185Vk8q4cnIZpdmug9r2ospW3t/UzIhiL1dPKac7GOGyhxYwc3Aes4flM60iVw2npFAoFAqFQrGPBMIxHFaTGn1DccygBO+xInibNsLLN8u4vOOvgQnXQ/aAT//equfgla/CiIvgc0+C6cQVZ5GYzryNTTy3pJr3NzcDMGtoPldPKee04QV7DXneV2ra/Pzk35+wcFsroaiO3WJiWkUu3ztrGGNKMw96+wqFQqFQKBQnGp/UdfLishoWVbaxqbGbbJeVsaVZ/PZz48j32InGdDW6huKooQTvsSJ4dR02vw7L/wZb3wZDh4rZMOsH0H/G3r+78EF48y6YcANc+Ef4DFjUatv9vLCslheW1tDQFaTAY+fKSWVcNbmMspwD9/omCEZiLK5q4/1NTXywuZmHr5nIsCIP729q4t0NTcwams+Mwbm4bMdOQvNITOejrS10B6PoukFMN4gZBsOLPIwtzSIYifHckmp0Q8ZMjhmyztSBOUwakEOnP8Ij87cRMwx03cBttzBlYA4TyrOVl/uzRNMG6KiBoWcd7T1RKBQKxTFIS0+IxZVtLK5q5dpp/Rla6OG1tTv57gurmTQgm/Hl2TR0Btiws5t/3TYDq9nEz+au4611jYwry2RsaRbjSrMYU5qZzL+iUBxOlOA9VgRvOp21sPIZWPkUnHsvDD9fPL+hLhnKqC/e+Rl8dB+c8l04/X+O7P4eRaIxnfc3NfPckmrmbWrCAE4Zks8XppRx+ojCQ+L1TeeJj6r4vzc3EYjEsJlNTOyfzcB8N7+8dAwAiytbafOFyXRZyXbJ+MHZLtsBC0ZdNwhEYrjjL4SnFu2gviNAQ2eQnZ0BGrtCnDWykDvPG0E4qjP0x6/vto1bZlVw57kj6PRHGHfP7onRvnvmUL5x+hDqOgLM+s08TCYNkyZ9n3UD7r5wJDfOHEinP8K2lh7G9stUVtoTDcOA7R/Cx/eLwS2nAr6xQoxnhvGZMKIpFAqFYs80d4f447ubWVTZxtamHgBcNjO//dw4zhtTTDiqo2nssd01d3U9b65rYHVtBzVtAQAKvXYW33UGAPM3N+N1WhlR7FGJRBWHHCV4j0XBm0CPydxkhvd+DvP/D8pnwMQbYOTFYHWm1jUMmPstWPE3OOsXMOPrR2efjyL1HQFeWFbD80tr2NkZJC/DzpWTSvn85HLKcw/e65sgFI2xbHs7729qYmFlKzEdXv/WKQB85e/LeHt9Y6/1+2U5+fiHpwHwk1c+YXNjd1IMZ7qsVOS5uWpyOQD3v7uFTY3dNHQGaegM0tgV5LThBfzlerlHJ//iHdp9YQq9DoozHRRlOpg1NJ/PTZLhqVZWt+NxWDBpGmaThknT8DqtZDqt6LpBZyCSFLSJeqvZ1Oe4xV3BCEur2hhZ4qU408krK+v49vOryLBbmDowh+mDcpk5OI9hhR5MR3ncY8MwCEZ0gpEYwWgsuVzgsZObYaczEImHqccIRmLkuO2MKvFSnOlQfYyqPoS3fgQ7V4M7H6beApO+BK4cWP08bHkLLn6g9/NGoVAoFCcsTV1BFlW1saiylRHFXq6b1h9fKMqMX7/H+PIspg7MZVpFDqP7ZR6QY6HNF2Z1bQddgQgXn9QPgFn/N48drX6sZo0RxV7GlmYye2gBZ4wsPNSHpzjG6AlF2dTQzaaGbjY2dLFxZzcPXzuB3Aw7T35cxV/mV1LgdVDotVPodVDgsXPTzIG47RbafGEAsl3WvbbnlOA9lgVvOj1NsOpZEbRtleDIhPHXwVk/T3lf9Bi8eCNs+A/kDIKKWRIWPeAUabx+RojpBh9sbuLZxTW8t7ER3YBThuRx9ZRyzhhRiM1y+LyTzd0hWnpCtPvDdPojtPsjmE0kBe2vXtvAyuoOOgJh2v0ROvxhRvfL5OXbZgJw5SMLaekO9RK0o/tlct6YYgA6/GG8DutREZgd/jAfb21lwbYWFmxrparFB8BHP5hDabaLLY3dWMwmBuS6DlhERmI6zd0hGrpE8JdluxhTmkmbL8xtzyzHF4olRW0oonPr7EHcNHMgO1p9zPq/93fb3j0Xj+L66QPYsLOLc//44W71v/vcOC6fWEpDZ5DFVa2MKslkYJ67TwPACUWoB2JheS5smwev3QEzvgFjrwKrI7Xegj/BWz+BkvHw+WfBW3z09lmhUBwWDMNgc2MPlc09nBt/1yzY1oLDamZUiVd52z5D/OLV9by7oYnK+Ps9w27huun9+UF8FAtdNw5b+6OuI8Camg5W13ayuqaDtXWdXDC2mF9fPhZdNzjlN/MoynQwINfNwDwXA/LcjCvNOiTd2NIxDIOWnjC17X7GxCPa5q6u56XltdS2++nwR5KOhGe/MhWXzcLb6xv5pK6TLJdVJqeNTJeV8WVZaJqGYRjKuI60z3e0+tjY0M2E8myKMh3MXV3PN55bmVwnw25hWJGH31wxlkH5Gczb2MTcNfU0d4do7ArS2BWiMxBh4/+eg8Nq5p6563ni4yqsZo0Cj4N8j50ir4OHrpmAyaSxorqdDn+Y00cUKcF7XGEYsP0jEb6aCS77i5RvfA0Gngpmq/QD3vaurBfuATQoHifit2IWlE//zHhrdnYGeHFZLc8vraGuI0Beho0rJpZxxcRSKvLcx4RnMhzTj8sGRX1HgBXV7VwwtgSArz27glfX7KQk08H0QXnMGJTLjMG5FGfKtdYdjIjnuivlvR6Q5+aCsSVEYzoz732Ppu4Q6Y+eG2cM4O6LRhGKxrj2scVk2C04rGbsFhMOq5mzRxcxZ1gB3cEITy+qxmGVcofVhMNiZlRJJuW5LoKRGNuae5LfbegMsq6+i9OGF1CW4+JfK2q5/YXVADitZkYUexhZ4uVrcwYn9/+EoLsBFj8Cyx6Hk66Fc34pzxTDANMeDEEbX4V/fkWMbFc/ByUnHdl9VigUh5yGziDzNzfz0VYxYLb0hLBbTKy5+yzsFjMXPfARa2o7sZlNjCzxMr48i5MH53H6COVtO96J6QYbdnaxbHsby3a00xOK8uRNUwD42jMrCEZiTK3IYVpFLiOLvUetC5OuG/jCUTwOK/5wlHvmrqeqxcf2Vh+NXSEAvn3GEL59xlDafWFueWo5/XNFCA/Mc8eFsRunrXf7yjAMmntC1LYHGFroIcNuYf7mZh77qIradj917QFCUR2A+XfMoTzXxdOLdvD80hpKs51ku210B6N0+MP87aYpmEwad/9nHU8u2N7rd+wWE5t+fi4At7+winfWN5KViO5zWinPcfGLeHe4Rz7YxoadXUlRrAEFXgc/PFcMDQ+9v5WqZl/Sv6WhUZbj5OunSTfHP76zhboOP4YBugEGBoMLMrht9mAAfjZ3Hc3dIYz48RsGjC3N4tbZg2T/nl9FZyCC1WzCZTfjtlmY0D+LS8eXAvCvFbVYzSbcdjMum4UMu4UCjz053GdfhpBEWV1HgPvf2cLGhi42NXYTjMi5ve/KcVw2oZTtLT7mrq5neLGX4UUeSrOdn2ocCEZiya6CK6rbWV3TQVNcEDd3hwiEY7x0q+Q/uu2Z5by2toEd916gBO9xS6JvXVsl3D8ebBkw+jIYf714ZDCgbgVUvg9VH0DNEtAjYLZD2ZS4AJ4NxSft+1BIxykx3WD+lmaeW1zNuxubiOkGDquJirwMBhVkUJHnTs4r8t3HVDKq44Udrb5k42nhNunLPLzIwxvfPhWAST9/h5aeUK/vXDa+H/ddJQLqZ3PX4XVYKcp0UOR1UOh10C/bSabTetj3PRzV2drUw/qdXayr72RdfRcb6rt493uzKPA4eOzDSl5cVsuoEi8jS7yMKslkZIn3iOzbIaFpo3hr174AsQiMuBBmfhtKJ+7b9xvWwnNXg68Fvr4UssoO7/4qFIpDSrsvzMLKVk4dmk+G3cL9727hvrc3k5dhZ+bgXGYOymPG4NzkUH8NnUFW1bSzsrqDldUdrKnrYM6wAh6+Vp4ZP/znGvrnuhlfnsXY0sxj6p25tambNbWd1LUHqOuQqcMfYe43TgbghaU1VLb4ku/7gXlucty2E9YDFwjHkqLvwXlbefj9bfSEogAUZzqYOjCH+6486ag7APYHfzjKjlY/XqeVfllOqlv9fO/F1VS1+mjuTrUz7r18DFdNLqeyuYe7567fTdA+f/M0plbk8s76Ru5/bwul2U76ZTkpzXZRmu1kWkVuMofKpxHTDboCEToCEr3nD8eYOTgPgH+vqpPoPn+YjoBE/9ktJl64ZToA339pNYsq2wARq4YB/XNdPPPlaQDc+vRyVtV0SH1c0I4o9iYNFdc9vpgtjT2YNJLX8cT+2dx/9XgArnlsETs7gmjxeg2YMSiXn108Olnf4Y8QjRn0hKL4w1HOGV3Ery4bC8DQH71OOKb3Ot7rpvXnfy8ZTSSmM+RHr+OyiRh2280EwjFumjmQW2cPoqkryDl//JARxR6GF4moHV7kZUhhxhFJiNrQGaS+M8DE/jlK8B73GIaI2RV/g0/+BdGAjOd77Usw4GTx6vhbwdsPapeKAK78ABrXyvftmTDwFBgYD4HOG3JCJ6lp7Ary7oYmtjX3sK25h8pmHzXt/l6exX5ZTiry3QzKz0jOB+VnUOi1n7AvxUOJrhtsauymMxBhWkUuAC8tr8Vq1ijySqh2oddxTGd/TrdY/ndNPf9cXsu6+i6a4i9Tm9nEJz87G5vFxIPztrKmtiPeN9tGtkuEe6JvUmNXELNJI8tpPTrW8n/dDOv/I0OeTbsNcgft/zZ6mmDDXJj8pUO/f4eR7S0+PqnvpLrNT02bn8auEC6bmfs/Px6TSWP+5maq26Th5HVYkqFqg/IzjvauKxQHTDASY0lVGx9vbeHjbS2sq+/CMODxGyZx+ohCGjqDdAYiDC3M2Kd3WiSm0xWIkJthxxeKcsGfPkp2azGbNIYVerhlVgUXn9SPRPvxcL0rK5t7WFLVJmK2PUBtXNi++Z1TybBb+NVrG3hkfiUAeRk2+mU56Zft5A9XjcdmMfHDf67hnytqicRSL/3BBRm8c/ssAF5dsxMDg4q8jD49hMc6zd0hlu9oY+n2dpZtb+OT+i7mf38O/bKc/HdNPYsqW5k8QEZn6Jd1AkUwxekJRdne4mNHq59xZZmUZrv4eGsL976xkdLslJgtzXYyoTybLJftaO/yMU99RwB/OEpPKIY/FKUnFKUky8nofpmEozoPztuKPxzFF47hC0WxmEycP7aI04YXHvbnwb6i+vCeCII3nWAnbH4T6lfCzG+BpwgWPCBJaSwOKBwtIYnFJ4nArVsq4rfyfejYIdvwFIvwHTgLisZIxlbboe0jcVSJBFP9FOtXQthHKGbQ0BWirjNEVZeJZYFitjX3QNNGYpEgBho6Ei6bm5NLRlEFg/IzGOnuoqh0EP3z3Hgcx4m3T3FQNHeHWL+zi50dAT4/Jd43+/UNzNvYlOyXHYkZ9M918cEdcwCxnn68tRUAj8NCtsvGuLIs/hS3vj46v5KqVh/RmE5UN4jGJBzpm6dLuNLtz69iZ2eQqK4TiclwUlMG5vCTC0YCcMGfPqSlO0zMMPBY4UxtCTdpcym65hEoHst9L81DszowufPIcFjIsJsZXuRlXFkWhiHGCbfNgsdhwW23fHoSkrrlsOQxuOC+o949orUnxMaGbqrb/OxoFVFb3ebnhVum47SZ+cWr63n0wyoAct02Cr0ODFKJ5r71j5X8e1V9r23mum0s/8mZAHzjuZUs3NaK12nB67DidVoZmOtKWsbnbWoiFImR75FEGvke+zFtyDnURGM6XcEoXYEInfFpbGkmWS4bVS0+PtzSjMch587jsOJxWBiY5z4uzlEongAvHNUJx3RCEUkkWRE3hlQ29+APxzCbtOTksJqTIqLdF0Y3DEkQaNKwmDQsJtNhySMRjemsqeskw25haKEnmbfAataYUJ7NzMF5zBycx9jSA0sy1BdtvjCrazpYWd3OiuoOrppcxoXjStjU0M2VjyxkQJ6b9CbunecOZ2pFLst3tPGLVzckyxMN4bsvHMWY0kw+3trCH97ZnKwPRXXqOwI895VpDCn08LcF2/npf9Zh0qA405kUtD8+fwS5GXbqOwIEIjH6ZTn3eJ1FYzp1HQEqW3xUNfvQDYMvn1IBwNm/n8+mxu7kuiWZDs4cWZi855dubyMU0ZPXRygaoyzHxeQBORiGwR/f3ZIsD0UlgeIpQ/K4dHwpgXCM659YnCwHsFvMXDW5jGun9ac7GOGOF9dgs5iwW0zxuZkzRhYwY1AeXcEILy6rTauT+skDssnNsPPmugZueWp5fLsmxpVlMXlANtdNG0BRpoPPLDvXQPMmaf/mDNpzFx7FCcueBO+xE5ui2D8cmTD2SpkSjLoEMgph5yqoXyWZV5f9Fe6qg9GXg78NTBbILIWIH1o2S2bW1c+ltuEpEa9QTkV8Pkjm2QN7J7k51jAMEfI7FkL1AplbHfDVj6T+1e9B3TLsQP/4NKNsKtd8SYbvMR78Llrzxl6bXBOcxK3bf8S/V9Xzsf0bGGg8G5vKh7ZT8OWOoX+uOz65kvPcEzhc6rNGvsfOLE9+r7I7zx3BneeOAKR/jC8sVtAEXzmlgjNHFMZDnSK0+8MUeOzJ+jfWNbCj1YfFZMJijjeMzanrJRiNEdV1LCYTDqtk1s52pQwsE8uzKWlfSmloC1Nb/kVeZCcdznIItGMYBv/YFKMr2EEw0pb8zo0zBjCuLItQVOecP/RO6GW3mPjanMF88/QhdPoj3PjkEtw2C06bGbfNzJzuV7mo7jm05g10X/p3Xtlm4LKak/17XDYzA/Lc5GXYCUVjNHWF0A1DxoCOj/NcmOnA67DSHYywvcUv4z8bBoZhENNhaGEGWS4bLT0h1tV34QtFqY6L2Zo2P7+8dAxlOS7+vaqee/67HgCrWaMs20VZjovuUASnzcz10wdw6fhSynNdfY73eO/lY/nR+SPoCkTpDEToCkaIpXl+ZgzKxeOwSF1c0NV1BJL1f3hnC6vjoWYJpg7M4fl4qNq9b2yMZwyXZBr5HjvlOS4G5rn7vL503SCqG9gsJgzDYEtTD4FwDH9YErYFIjHKc1yM7pdJMBLjkQ8qCUSkLhLTsZg0Zg8vYM6wAnpCUf4yvzJ5PSUE15SBkl21OxjhrXWN8TrJ1m41a4zul0mh10F9R4CXltcmhWzi+G8/cyhTK3KZt7GJm55cutsxPPPlqcwcnMfK6nb+59/rdqv/z9dnMrY0i+eXVvPz/24QQewUMexxWPnlpWMoynSwfEcbi6va8DismDQJGYzGDL4wtRyH1cxHW1pYsr2NmK4Tjcl5i+kG/3PBSEwmjX8ur+WjrS1EYjox3SASMzBpJLPe/9+bG3l7fSOhaFzURnU8Dgvvxw1VX3tmBe9saOq17+U5LuZ/X+p//MonLNjW2qt+RLE3aUy58a9LWF3b2at+8oBsXvyq9C07+/fzqWzpQUuThacOzeOxGybL8m/m0dAZ7PX9c8cU8cfPi6Fs0s/fpisoz5lY/NivnlLGry4by7BCD3//4hQmDcg+bOHGOW4bc4YXMGd4Qa9yi1nj3NFF1O+y74noFpOmJcNE030ryf6Juwxt47RZGFnsTRoKLj6phNNHFFDkdfQZMVOyD15Li9mUfFfPGda77uWvzWB7i5+qFh+VzT1UtfjIdtvi+2vwxSeX0h2M9vrO5RNKmTwgB03TeHDeVjQ0EaPxvBEDct3Jc2M1m8iwW5K5O8IxHVfcixyK6lS1+JJiORzVCUV1SrOdzBiUR3N3iP+NP+/SSSRoHF+WxV3nDWfSgBxGl2Qe1iSdxzShHtj6DvSbAFnl0vXvX1+WOrtXhG/JeJjxTXDnHd19VRxVlIf3REbXobMGsvvL5zd/BCv+LuP8Apht0P9kOPvnYhHb+i507wRfM3TVSXh0Ek2E8q5COHcwZPUHyyEMFUlck3sTjroOrVsgP/4Gm/stWP6kLDsyJWHXgJNh+tdlO/Wr5LgNPT4Z8jAskwYHVfMh1B3vNBFfJ6MAisYQbN5Oz5Kn0OqXk9WyEjMxGszFPGi6hqd7JvR6kbtt5t1EcP8cF/3z3BR7HcdV35n9oqdZvOhFo8FbcrT35vhmxwJo2SL3bke1TAUjxcsK8JtB4G+BsqnyEh923m5W7GhMxxeK0ROOYreYyMuwE4npvL2+kZ6ghCklpumDcpkzrIA2X5hv/WMl/niokj8cwx+O8psxdZz2yV1EbRlc2vYN1hoVvX7rV5eN4eop5ayq6eCSBz/e7XD+dPV4LhxXwsdbW7jmscW71T9x4yROG17Yy2MBMvRAea6bX106hpElXmrbRQT3z3VT5HXsPcN22A/NGyB/xCGLWklkjmzuCdHcLVOWy8o1U+X5+rk/L2B9fRe+cCz5nbNGFiZF17l//JBOf5hARERtKKpz9ZRyfnXZGHTdoOKu13b7zS+dPJCfXDASXyjKqJ++ic1iwmk1YzFpxAyDr84axFdnDWJnZ4Dpv3pvt+//+PwRfPmUCrY2dXPGffN3q0/8d2tqO7jogY9x28zJUG+v08q3zxjCjEF57Gj18fLKOjLjdYlpaJEHr8NKJKbT4RcjQncwSnd8PnNwHplOK8u2t/Hq2p10BVJ13aEIT9w4mQKPI9nPdFdW/uRMst027n1jIw+/vw1L3LuaGGJt6Y/OwGYxcd/bm3llZV1S8JtNJhxWUzIr/hMfVbGkqg271YTNbMJuNZHptHLH2ZIo5o1PGqht9ye9aDaLCa/TwmnDJXHT8h3ttPaIMSchtj2OVP3ra3fS3BMiGjOS6xR5HVwyXro6PP5R1W55DSry3Mlh5h56f+tuwmp4kSfZVeL3b29O9kU0aTCqJJPpg3LJcasQzcOFYRisqG4npoPDKteFw2rC67AmRXFMNw5bpv+YbtATjBKKxeJeZhHFOW7bZ9uDC+K82fQ6bPyvtFtjITjzHol2jIaheaMMv1e/Qtoljevhe5vBmQULH4LKeVAyQURyyQTIyP/031QcN6iQ5s+i4O0LXYf2KnkI7FwtYvDMe6TuoenQlGZRdGRB+TQYfQW0bZMQan+rNLbDvtR6mkksazmDwO4BPSoJc/RIfJ7+OboP5fGEW54iEU+eYlk2DPn9jmrZz1AXfGedCPGt70BblQjdgpH7FsZiGBBol+111kBHTdpyXGgEe3t0MNvAlSf7PmgOkWEX0BQwE678kJXe01kTLmF7q4/qVj817f5efYdsZhNlOU4GxjMLDszLYECei4q847TPcGctrP83bPgv1CwSI4HFCT+sFgNIYrxXJYB7U7MEGtelrrGOanDlwhf+IfWPnBq/N83SHz+rXDKzz/6B1Nctl2swYcg6EjR8gvHc58HXQsPlL9OeOSrZj2dIQQYlWU7afGHe3dCYHBta08TDM6F/Nv3i9St2tGMySXliGlniJcdto90XprJFMmyX5bjwHkjXgW3zYMmjsO09yXFgtskz7NqXj1jCPl8oKoK4J4TTamZ0v0wMw+Cul9cSiRm4bGacVjMOq5lxZZlJ0fTa2p3J7OMumwWn1Uxeho3cDDtG3Gu+t8a1eMxFbEm4vGSFd9rMhKM6OzsDyTD6aNxTWpzpoMDrIKaLUDtUIbD7S2J87e5gBJDjtJhMMta4SUPXjWQSFoVC8RkkGgKLXYyZv6mQ57u3FEazw7MxAAAgAElEQVRcIMkZy6bt+RkfDaecMoselhFOmjcC8fZZ/nC4bZG0h1u2igB2ZB6Rw9pnWrfFc/cEpXuR1SXzEReK57qrXtrA6XVWlwxJaDr2u5UcSpTgVYL302nZCp3V0LVTbp7uegllnvlNqf91ufQdTqdsmiTCat0m/YMNXW4ukwVMVnB4pTFvMkOgQxqgFrvMrQ6wecDmjq9vkYeSySrDLDWuk9Brf6uIq1h493125YmgSkyeEhlD1FMsYsHhlePp2JEmZGtSy+Ge3tuzukVgZJXJPDM+zyiU7zStl0y4zRvk+7ti90K/iTDiImIVc6gnn+r2YFIEb2/1SQhVq49wNJUJz2UzS2r9fDcVSUHspiIvg0zXMdJn2DCgaYOcG7sn1We8YJS8dPrPFAPCqEtk/UdmSXh9ziDxtg84ReafpTFe23fIfdG0Ac79tZQ99wXY9Gqqe0FmGRSNlaGDQNa1ueVaPpayqvc0w6IHYc6Pj5396qiWodpGXyYRGcuegPm/g+HnQ/lUiezoaYLLHpH1X7heGk4VsyV3QcGIEzpxX5KwT85TZ/rzr0YSk029RZ73r9wmHhBHVmo+5CwoHClhg61bpRHozJIEiPvTNy7sh55GeZ6HfTJF/PJMcHjFALv5LSnLLBWvS+GoY7sbzb7SVyyv4vhg0xtpz+l+8t470hiGtLu66qQdlDv4wJIRHo+0bIWNcyWJotkGX3xDylc+LY6NkvEHfk+FelJe4FAPzLlTyv8yW55HuUOkLdd/ujynjvQ597eJ97p0EuQMlHPw/HVyvEZaFuVbPoTisbD0MXj1u7tv5xsrZN8XPgQf3JsSw94SCfc+5bvgzD5yx3UEUIJXCd6DJxqSkOeunSKGu+rl4TvsXEkQ9cAkeTBHAuKlBQkxOfMeEbv39uGRmn2XeK466+D3I0XsWl0Q8YkX9bzfwpSvQHcTbH0TMsvFC5z4/a763vvkb939N9JxZMaFbB+iNqtcbvx9fYCGuqF5s4jguuUSitpe1VuYW92SDbtwlDSu80eAtwTdkU19xElVe4TtLT5JqBGfatr86PHb0kaE8a5mRnmDDHX7KLf1UGzugjFX4B44BSPQgbl+KeGcYUTcJcTiY7PpcW+P9KOMf473qdQNCZcy4mUWk4l8j51Cr50Mu6W3F0XX5dgSL522SrjsUek77msVD/ieXgQ7V0PVh7D9Qzk3oS6xRl71tNRvfFVeKJ6ifTvfeyMWEWHjKRLjSvViEZoOb/zcjwJ37sH/zr5QtxxWPiNhU22SQRRPMXxtsVx/bVUyjran+Pi1vHY3wPzfyr19JBPdGYYYwja+KuFsDWuk/PLHYcwVch2YLHu+h9+4Cza/nvpf3AUw9WY49Y4js/+HmsSQdXoMVj2TErOdtWIMGHe1NOQC7XDvAPmOKzf+zCuDqV8VI1TrNsnwHeyQZ3igQ57hl/wZTroaqhfBE2en/bAm99Zlj8LQs+X+fvuncSEbF7RhP1z7T2kwrv4HvHzL7vt/y3wZO37p4/Dq7fLf6fHQXpMFvrlSnsvNm8SzUTBS7p3DQSwi56FpvRidxl4pz+5Nb8CLN8SFq5ESsDf8B/rPgDUvxo/NSK0DcPMH0qBMP7bcwXIMhaNg0hfF+6I4ugQ7JeS18ROZLA44916pe3Bq3BMYx54pBt2L7pfPSx4VEeztJ4LY208M+vtDJJgSs521EpGSOwhql8G/vyZl6Yb5G1+Ve7bqQ+miVjZFvlMw8ui8T8J+wBBBurdn7/6w6ln4+H5xLIAI25EXyxB7h9toVPm+jHJSt1Lmvqbe7ZblfxOBWTjm0Bp+dR0aVsOWt2WqWybCNhGqHQmKQdCZLW3MiF/a2q48cRR11UtenkggVRcJwNir5Fm9bR5sei1udPRD+3ZZ/45tYlh8/9dyTRWPSyW8zR10XLZRlOBVgvfIEotKyIlmEm9VLCovk0hAyhM3Y8EIefkH2mHxX1J1Nrd4j8un7l9oSSQoArg77qUOdsRfRvEG3pEIU2naKA8WZ7a8LFc9EzcCRHdf1+aWF4XZJufKMDD0CL6CidRmTaazq4uplX/q9ZVuw8GPI1/k3/rJzDat4knbbwDoMpxsMUrZrPfjydjZ1Bn5WIlgQcdKDKsWxUJMlonGpxgmTayFumHCajGT5baR5XJQ5IxwU8Mvccc60THTkzeWaPnJ2IbOwZ1ViGYyA5rst2aSF5HJLKG4ybkl5Qlq3ihlJRPE2/N7yTxM7hCJEhhwMgyc3VuYhnpk3Z5GEVmDThMP0+a3YPHD0N0IPQ0pQ8e3P5H/+cPfwbv39D7XnmL4+jKwZ0gDB0N++2D6n0dD8lLcNg8mXC9hxquehdfukOOpmA0Vc6Sv+ZH07ui6XPuB9ng3hDYItKWW/a3yOdAh43onGmuZpanGm6dkz+dmzQsikIrHwdXPHd6wdT0mx5EI27pvBKBJH+bh58u0v9b3jmrJWl/1AZROFi9nqAcenSMipmI2DDj1yBlJ9oXNb8oztHmTCLPOGhh8BlzykAitX5bIc8ZbknreDTtPvN+GIY2bzFJ55nwahiENI80sjSF/G1QvlOslXRRPvFE8wLXL4L3/lWvJ6pLfsLlF1OUOEgND9aJ4XYYYSWxuuf9sLhGbxJ8fnbXidWlYKwZRk0ka/iuflq4uRaOlAVwyAU76wv7fV7oOHdtlXzxFYrR88QbpN58w1GomuPwxSfbYtBFWPxv/shb/PQ0m3gDZA2Q/170Sr9ZS60y8SaJZ6lZIcshIQP67pnh3hu9XieD9+I/SLaRwpBjmCkeKeDkUYtgw4obhuvhUL8blsiki3MJ+eOKstC/Ez+XEG8X772uFpy9Lq47XT70Vxl0lBpYXbxBDtd0jDWu7VxrZ/afLdbP5zbQ6j9R7io/8iBB6TIyN7VUwRDKz88+vyJjlCRxZ0n3kqqfkc1ulGFITYrSrTq7ZqTfLuf1FkRhi0plyC5z3G6l/80dyjWX2i/8Xdal2TcsWeOIc6SKWzgV/gEk3Sf07d8u9nP58Lj5JnstrXpDt++LJ1mwZYkC+4q/y3NL1Q5ehWNehq1buGXee3DOv3i772NOQWu+yx2Ds58QA9o8vpNo2ZqvMz/+dvBerF8O7d8t9Hw2lIj9KJsj131knhs3Rl8k9eLTGgzcMuQZiYWmrdjfC74ZKnc0j91H/6TDiYsgfuv/bD7TL9ZU/TI7/3gFyTvpNkOiawWfKs+5wZZrWYylBu+jPsPZFecckrums/vDtuFF5x0Jp0+YNOeZFsBK8SvAqjgaGAQsfgE/+KWEyCSriDevuBlj2uJSZrKlwFT1G0lOw6yY1M2FrJlGTDbMexqyHMOkRND2Khs4RlFUHh2ZCGlhGKkTH6paGUELk7HoOCkeLhyrYIRZKi1PWt7pEyOYNlRe/poHZAUYsJfSCXTD+WnnxLn5Y+tJqZmm05g6WsOIJ14mF32KPh97b5WVjGPJiDnVLo2XNC9JvuWFt3KOviYcxSzImJ8M2Q929p0S/0uRvOHb/bEn7bLanlafVmSziMfe37S5mA22yD+lhT+mYLODMkQa1M1v2q7N29/7qaBImnPRelPZueLVsEWFv98DVz0pj61ARCYilfeN/JTlJ2VQR1iDiov8M2bdDSUc1vPZ92P4RhONDlRSNgXN+LY00X6t0jfAUiWf4UId16zG5pps2iHGoeaOIg0Sysgcmi2j19hMRmVUuSQdPulrqO+vknOyPBzThIT7W6agWw1L9SglTr18FnkL4RjzJ2dv/I/30SsbLlDs41UiMBGHpo3Jem9aL6Iz4U9FF/jZ4+asp42vBCBE1hzOcOtSdCo9d8XfxgDeuS92DVhfcWSfHsP7fcj8UjpLn264exNrlEqreWZcStyUnwcnfEaHy84KUkAd5Zk67Fc74qWz3pS9KeXpbcPTlIlwS50ZWSNWPv1Y8bp11MPeb8gwMdcszNtQNZ/1cBHHNUnj8jN2P/4on5De2fwQv3JASxDaPPM/n3CX/Y8NaWPVcvD9i2jTsfPn/u3ZKfhGzTe6fWEie0wbg8Mgzqmq+hOM3b4ob4c1wV738v2teEMNR4Rg5v96S/bsfIoG4EaEmfv7r5B01/Dx5Bv9+TOpZkiBx3QU64J2fxo2Mpamw6f3xEidGp6hZAjWLoeETuOl1uW7+e7sYqcqmyPOzbKokHd3b8SWeB6EeWPAned60bJHzFw2kvI2dddI9JG8o5FbI+Y+GpWtTwQj5zpK/yH8R7BZRH2yX+8rfKqJq16SoJou8231Nvd9dGYXyvdxBIrZyh8g8q//R6VrTVS+CfscCOb9N6+GSh8X41lYJK56S7l1lU+SaTscw5Ni3vCVe3JolIm6//I7Ub3tPrsWjmUQrFpF7ZedqeU5O+YqUPzAFWjbJs6lojBheBs2RCE84tAaWg0QJXiV4FUebtippwMciklyr/3R5APqaU/2cE+gx8aIkxUyakEksR0Mpy6nJKg//RFhRwqraUQM7V4plsntnKtz6skdF8OxYKA2C+lWgh0UsDpwl1v14xupAJEqnL0iHP0ynP0SXP0xXIERXIBKfh+kORIhEIpgwMGsxzOiY0TGhi4dZ03FZNdxWcFvBZdFwWkhODrNOZrQVs8OD1WrFFmrD0bE1fnxWNJMFzWSWOTqaHkGLhdH0qBxTLJI2TyyH2ZPRYL8wWeLnIvbp62rmuBdjD5PFKfsVDYkVNRpKNdISnxPLsXBaWbBvAWtxyLXjzAFXdtpyzi7LOfHlXNmPvho9YV+80VabarwlvBqJz7v2edfi16yhi9cks0zW6WmShmnCIGFxwpk/k+VNb8COj0n+N4l30Nm/kPnbP5VQwYhPBN+Qs8QbNeLC1O8ahjSs/S3gS0zNvT8H2qWhr8dSRiQjljbXd/mcNo8G41M4tf9GTH4jeV1Y5RgHnCyNsnCPiGJXtoSZuXMho0giEqyulHEDQ+7Hzhq5l8deKff+f74pQ6ol8JRII/XC38s12L5DBK0zR/Yl1C1Gj4QxJSE20st6TX2UR4Nyz/e6TjNSnrj0clvG7mXpdRb7kRPPui7/hUcSfvGPa6SxGPHLZ1uGeCnP/oWs+8sSaXwWjBDvacEIeQbnDTky+7svGIY8oxvXy7ElDBlPnCMNa5D7LWcg9JuU6pf+hzGpfBJWlwimERfAGXdL2ern5Rr09hNBtz/ddg6WaEieG7teo/2ni8GmaYPc64lrM9gl99Gs78v+bnpdQltjIXo9y0unyD3QvmN3D+me0ExyfpxZYrByZknElyNTPLuJ/unJsuzeZQcaSh/slOenpqXyihwJlv9NuiHVLknlXikaB9e8IIb2HQvEwNa9U/6jjhqJoJpzF6DBnyZIhELeUMgdKh7M/jP79mQahrwrWjaJWGreJGK5eWPccB3HFjdK5w+TKS8+Txev0bB44Vu2yEgcLVtl3rq1t0g2WSTHTN4QMXAlxHDuYPFCH6lr3N8mz3V7hhhjX/qiXJuaSYwf/WfCyd8WQ2l6REHxOPHgDj1bxPGxTtMGaScmhj1tWAMjL4FLH5b//1dlcg48RfLe8xTJu3v4eVLfsEYiO1x5h10YK8GrBK/is46uiyegZUsqpOvNH4kIH3IWDI8nnjpAq2kgHKMjEKbDL2N4yjztcyBCZ3I5Vb7rcBz7Q2IYEqvJhNViwhIfssRq1rCYTViIYSOMVY9gJYKVMDYjgtUIp5aJYDPCuAw/GmAlzGmRDykxGsg2OrAiQrfBUcHOwVdTUlRAvsuKKbtsdxFgdR6+F20smhLAekR+90j3n00mT0kTxm3bJKTblSNei0DbHjagpZJjBDrEua8lXnwmuPTPYtne9IY0lvKGyvrBDhEASVHbKvO+ktiB/Ceu3Hh2SmtaeL1pl3B7s/z+bmH4u5RrJmlw+9sktD7UFe+rGpD/wpEp+xLs3PM+HQ1MljRR6u1bxFrs8UiELvHq7Eko74vhSDOJYSDhhbM4envlLE7xqiXEf6/1XFJnid8/ejRt0lPLRiy+3Mc8FpZro6dJvEQ2t2RfNXT5n0zmtGHp9N5D0O2xrK914oaSvdanfT9hcDlg45uROs5YFIxoKjdEwvjhyhFjk6ckFbmRNHLlyHne558z5NoOtPcxte3yuSMV0ZI4xuQ5MdLK+vrMLp91MVbuzbBo98YTYebJPW51ynatzlQyTJMFhp4j/7e/Va7rRBh+sLN3SH562afdu7aMlDjeNblbUijvoW5/+/XuL7GoXPPdDTL1NKSM3N0NEsLftVPu5wO6DrXeCUfNNok2MtvlPO+aANSZI/de/tCUqM0fJoL/YN6P/jYRvkkxvEW6d7Rt6/3/WRzyfySuF7s3/v+kL+9a702V2b0H7j0O9Ug0SvVCMSrULZfRRFw5sOUd+W8Gn5HKXZKIHAt29jZMJg2Ynb0/a1o88suWNrftfxn0dg7s73IiFN1sk8iM+hWpslC3PBdGXw6z75R7+z4ZAg7NLAZiTyFMu00MvqFuibaw2FPOG5NVjALZ/eXYa5dDLCjdMKIBmZvM8pu+Zpni7zHt+peV4FUoFMce0ZhOVzAqHuS4MA5HdSIxGTolEtOJxIdSicQ/R/ss23Vdvc8EqRrJhfRZfB2tV5mJGPnhOjp6Arze6KUnLBvMdFoZV5bFSWVZjI/Ps9WYmEIsKg1jX0tvj6u/ZRfRGi/zt7HXRlii35grT4a5cuelfY6XuXJTdfvTuD+U6Hoqo3x6iOmYK0SsrHpOQnITYYveklQI8m7e5mgfnum0Ms0Ub5zt6m1NE7QWx6ExviT68+7RU9wjZZGAGGQifgkjTs/VkMzdENxlOZ7w5oDQUtn9E7kCEssJQ4UpkV9gT1NaDoJe+Qj2sE76lDCM7Km+z984BBi6NAADbeBPE6IJ73ZfWBy9uzEkBLEeTYnWxORvi3tU94DZJttKbMeZLYa33c6jljr2XmV9fUbmZmvadtP3N0eE4+FKWGYYcu3uKoKTnxPLfcyDnbtHvuyKxRHP12GXY0gKR+teymxp5WlleiQuahtTAtfXzO73kSbPRE9hyuOWmNyFYhhy54lXLhaJRxolhE1iORQXN4nlSFpUUnxdPSrh2EnP7XDZ7pFEj4noTojh7vq4QOyK/3+J5fjnaODTt2l1y7lJGE1Nlj7maZNm2qXMnFZuFqNQX6J2b/daArM91QfeMFKCM/m/7RIBcThJjyA021LPo/RuE31+zxqPcrLKOTF08cYXjpTjWfXs7t/xxEf0+LRnUqJrHIAzB+2H25XgVSgUigMlphtsbephVU07q2o6WFndwebG7mRG7f65rqT4Pak8mxHFHuyWYzu5wzGBHov3RY6L4UhQwoETgnZfkiwpjk8Sjbd0kQy9hWuy8WjepfzY6C92zBAJ7t7txd+WEsT+9t3rTekCM6u3GE4XtOmT1XV89Ps+ksQiaV7jdFHcnvocCaREY1JE9jH1VZ4oS4TKuvN7h44mpowiEbieYlnncBkIjndikbgI7uxbECeWw929I0ySkSa7lu0p8iRtfVtG78Rt6Unekp7mvuo8nx4hYBjyO9FQbw9scp5urIiXaVpv4bovyyZr38/dRFRI0sDQyW4RFcG0c93XOrGwGKvtmaluNe4CifqyuqWLkyURCRTPcVI+PT5sXrckx4tFIG8o2tAzleBVKBSKQ4kvFGVNbSerajpYVdPOyuoOmrrFEmkzmxhZ4hUvcHkW48uyKctx9h72SaFQKBSKfUGPh3sf41lyFYr9Jj1j9EGi+vAqwatQKA4zhmGwszMYF8AdrKruYE1dB8GIJJxy28xYzCaJ5ENCqGUOoCXLiZdpaWXJcGsNzCaNDLuFLJeVTKeVTKeNTKc1+TnLGS93WclySZ3bZlZiW6FQKBQKxQnLngTvUcjprVAoFCcmmqZRkuWkJMvJeWOk/0k0prOpsZuV1R1sa+5B1w3J12KAgRGfJ5IVG8l+x33VG8iHmGHQHYzSGYjQ0NlNZyBKZyBMJLZnA6bFpCVFcLoo9jisuO0WMuxm3HZLfDk1l2VzssxqVqGkCoVCoVAojh+U4FUoFIrDiMVsYlRJJqNKMg/r7xiGQSASo8MfSWbA7gyE0zJm986U3dwTYktTDz2hKL5QdK9iOR2bxZQUwW6bBY9DhHCm00qO20au20Zuhn23Za/DojzMCoVCoVAojjhK8CoUCsUJgKZpuGwWXDYLJVn7n6k4FI3hC8XwhaJJESzzVFnv8ig98bo2X5iqFh+tPWF6Qn0PM2U1a+S4beS47eRl2OLLNvLigliWJfzaZjZjtcgQUzaLCZvZhNVswmw6NII5phv4w1ECkRiBcIxAJIY/HF8Ox/BHYgTDMfzhKMGojj0u8jPsFjIclt2W3TYLpkO0bwqFQqFQKA4tSvAqFAqFArvFjN1iJucgh1cKRmK0+cK0+cK0+sK09oRo84Vp6QnT5kst72j109oTwhfey7ibu2DSxMNsNadEsNWiJZfT68wmjWAklhS1/nBqORzTD+oY+8JtM5MR93Z74mLYbZO5Jx4O7nVa8TqseJ2W+FzCyr0OCx6HFZtFhYsrFAqFQnGoUYJXoVAoFIcMh9Wc7Me8LyQEcmtPmFZfiM5AJDm2ciSmx8dkNpJjM0diOuFd62I6kWS9rBuKxnBYzWS5rDisZlw2M06rGaft/9u71xi5zruO47//OXPbq71eX1rspA6KJeT0iqyq3KQqVUVKQ4PUiqYKEKFIgaqFIm51kbhV8KK8oKUQXqS0JYJCGhVaLJBaqiRAuSiN06YXpwp1QyFJE6/vO7vruZ4/L84zs2fWu47t7GR2Hn8/0uqc55kzJ8+sj6L9zXMraaJXDnWTlVS1SqrJcl43WUnDe0qqlhK1Olm/h3up2dFSY7DHu94onIfXl5sdnV5aUb3R0XIrr+tklx42PlFO+2F420Q5BOTVoJzXlTRdLRfamK45z9u8lXqcNxo9sNLq9svLzY6WW11NV0t62WxNe2Zretm2qnbP1jRTZTg8AODqEXgBACNzpQF5FKaq0tyL7Pl2dzXamRYb+RzqxQttLTbaWrzQWb/caGuh3tDxhbxcb7T1Anl5wER5MNBffF5StZzIVFw0TaGkgcXT8trV+t6lq9e4VlrdPNj3A2w3hNrLnx9eSmzdLwUmyqletq2m3TPVEIQHz/fM1LR7tqpaefO2a3F3dUNbSizUBgBjjcALAMCQmVkeNCup9szWrvj9WeZabnW02Oio3mj35xsPzD9uF4dudwaGcffOTy21+nWNdlerHafWPy9ujRVeWVPWQI+rmTRZSfsre++ZqWmysLJ3Ps85HVgBfKq3Knhlta5SSnSh1dWJxYZOLDb0/GJDC4tNPR/KC4tNPf70OZ041lCzc/Gw9O2T5X74naqU1MkytbquTjdTp+tqZ+HYzdTJ8vp2odwO13WybCCkT5Tz4eozYXj6TK2s6Wpeni7WhWumQ3mmUL5W53l3M9fZlVaYytDMR3IsNXW6MM0hH93R0nKzo+2TZc1N5nP656Yq2jEZjlOF+nCcfJFbrTU73TAao6t6sx2+pGnnozLCiITpWkl7ZqvaPZOPOpifqlyT/47AuCPwAgCwxSWJhRBVlrR1e8NfrIlKqv07p7R/59SG17i7zl9o68RAGG6E82Yels838jneqamUJiolpqlKqV8up6ZSkqiUmspJPhe8lKxeX07yo7u01GxrqZl/2bAUvnBYqDfyoeyNjpZanUIP+cZKiSlNbPUY5pr3yuU15dVjqE/za+YmKyGE5b3cu2fzHu/ds1VVS5vXy72eLMt/96fDPP0zy80wV39NqA1B9sxKa93fTWIKK7lXNT9d0Sv3btNkOdX5C22dWWnp+MKSzq60dHal3e9pX6tSStYNxLO1shrtfMRBvbE6hL44vWC5eXVz+dPEtGu6qj2zVe2aqWnPbPg36P9b5McdkwRjYCsxv5z/S4+5Q4cO+dGjR0fdDAAAEJle73tvLne9kfcSFstLza66Wd6T3O163sOcZepmrk43Hz7dyXrHrF/u9Tj3yu1upjNLLS3Um+sO/56bLGv3zGrw6gWxYkDbNbMajNvdTGeX82B6JvS09hac6y0yd3qp1e+lvVQAna2V+quuz0/nW5LtDFuTzU+vhtv5qYq2T1Yua9X1LMv3HD/T+++Htg4cl9t5OA515y+0NVFO+wvITa2zwvpU6KHPF5sra7qaarpa1lQ17W+1NlkpaanZ6X+hslBvhtEHTS3Um1oIX7acXWlf1O5SYto1U+1/GZH/XvJQvn0yD+jbJ0MP9mRFM7VrcwQAsNnM7DF3P3RRPYEXAABgfGSZ68xKSwuLTZ2oN0L4amqhHo4hoC3Um+sG1LnJsrqZa7Gx/jZiZtL2sLf26k9V8+F8fnp1aPF82GZs2L3Ll8vdX9JFzpqdrk7WmwO/9xMD/x6NF/yyIE1M2yfKmpuqaC4M656brGj7VLkfiuem8m3b+vPzC/Pyq6WEhd0AbRx4GdIMAAAwRpLEtHM67zk8qNkNr8sy1+nllhbq+RzoE4VAVkpMO6aq2jGd9zQWw+v2ifLYLtb1Uge/ainVvrlJ7ZubvOR1WeaqNzs6F3qrz620QxAO5yut/mv/d2ZFX3vmnM4uty976HV/xfk1q81PlAdXo58op5qqpJpZs0Vaccu0mVppbP/9gfUQeAEAACKUhKG1u2aquun7Rt2aa1uSmLaFvbdfMb/xHPWi3grovYB8/kI7X3Su3VWj1dVKq6ML7ay/UF1v4bpGYbG6cystPVeo7y1g90IDPKcq6bp7hxe3Spup5YvNldJEleK8+DBvvjhfvtyr3+B1Mylzl3t+zMLRs/zYdb/49axYzuvMpEphX/Zyavl58tJv1+aeT0VIzRiyPmIEXgAAAGCLMbOwqnlJ1+3YvPsWV31f7G+L1hnYHi0/5vPRFxttnag39O2FpX79lWyTtlWUwuJwvTBc6YXhNOnXV9J8EcE4cicAAA1WSURBVLs0SfJ59/059z5Q7oY59avz7bPVefhh7v3a31EpyYNvKTGlZkrTcEzW+SnU997XC86J5cPgEzOZmVKTkjWv5fV5OQnX5u+Xpiol7Q6rj/cWvNs1U9NsLd49zwm8AAAAwDWiuOr73qvYA93dtdzqavFCW61O1t/Kq7j9V6ebqbVmq6+B67rZwBZhUh7wk15Is3yrtGRNaFv39UT98OfuanVWtxxrdfJ29M7bYTuy1kC5d52r3cnUaGfqZF2VQ+CslZO893qdFdVLaR6Oe+f5NXnPci+4Zi51s0zd0OObZWuOHhavC/t/X/RTqM/clWVSp5uFXu/VfcN7vd69HnF3Fd6Tv951V73RVqN98VD5aikZDMJhdMjumZp2hZXhd8+M5/ZcBF4AAAAAl8XM+iteY/y45/PJF8LCaifrzf75Qjj/7xN1/fvxU6qvs7Bdmph2hoXrauVUlVKiailRtZSqWi6cl5JQDuelRNVy4TxcV0pNzd7Q/HZXzXZvD/m8rhF+LvSPmRqtYrl3zcbz3XlSAQAAAOAaYGb5nOxaWTfunr7ktY12VwuLTZ1caoRQ3Owvgnd2JV9Urdnuqt7o6FSnpWanq2Y7U7OTqdXpqtnJz69WJU1UKyeaKCzC1jvunK7062shSB/b4D4EXgAAAADAgFo51fXzk7p+/tKrkF+Kez6EvNnJQhju9s97w81rpVQTlbzHt7flVq2cXtZ+3UW/t0E9gRcAAAAAsOnMLAxrTqXaaNrAJlsAAAAAgCgReAEAAAAAUSLwAgAAAACiROAFAAAAAESJwAsAAAAAiBKBFwAAAAAQJQIvAAAAACBKBF4AAAAAQJQIvAAAAACAKBF4AQAAAABRIvACAAAAAKJE4AUAAAAARGmogdfMbjGzJ83suJkdXuf1qpl9Orz+iJntD/VvNrPHzOwb4Xhz4T3/Eu75ePjZPczPAAAAAAAYT6Vh3djMUkn3SHqzpGckPWpmR9z9icJld0k66+43mtntkj4k6Z2STkn6SXf/npm9UtIXJO0tvO8Odz86rLYDAAAAAMbfMHt4Xy/puLs/5e4tSfdLum3NNbdJui+cf0bSm8zM3P2r7v69UH9M0oSZVYfYVgAAAABAZIYZePdKerpQfkaDvbQD17h7R9J5SfNrrnm7pK+4e7NQ98kwnPm3zcw2t9kAAAAAgBhs6UWrzOwm5cOcf6FQfYe7v0rSj4Wfn93gvXeb2VEzO3ry5MnhNxYAAAAAsKUMM/A+K+m6QnlfqFv3GjMrSdom6XQo75P0WUk/5+7f6b3B3Z8Nx7qkv1E+dPoi7n6vux9y90O7du3alA8EAAAAABgfwwy8j0o6YGY3mFlF0u2Sjqy55oikO8P5OyQ95O5uZtsl/ZOkw+7+H72LzaxkZjvDeVnSrZK+OcTPAAAAAAAYU0MLvGFO7nuVr7D8LUkPuPsxM/ugmb0tXPZxSfNmdlzSr0rqbV30Xkk3SvqdNdsPVSV9wcy+Lulx5T3EHxvWZwAAAAAAjC9z91G3YegOHTrkR4+yixEAAAAAxMjMHnP3Q2vrt/SiVQAAAAAAXC0CLwAAAAAgSgReAAAAAECUCLwAAAAAgCgReAEAAAAAUSLwAgAAAACiROAFAAAAAESJwAsAAAAAiBKBFwAAAAAQJQIvAAAAACBKBF4AAAAAQJQIvAAAAACAKBF4AQAAAABRIvACAAAAAKJE4AUAAAAARInACwAAAACIEoEXAAAAABAlAi8AAAAAIEoEXgAAAABAlAi8AAAAAIAoEXgBAAAAAFEi8AIAAAAAokTgBQAAAABEicALAAAAAIgSgRcAAAAAECUCLwAAAAAgSgReAAAAAECUCLwAAAAAgCgReAEAAAAAUSLwAgAAAACiROAFAAAAAESJwAsAAAAAiBKBFwAAAAAQJQIvAAAAACBKBF4AAAAAQJQIvAAAAACAKBF4AQAAAABRIvACAAAAAKJE4AUAAAAARInACwAAAACIEoEXAAAAABAlAi8AAAAAIEoEXgAAAABAlAi8AAAAAIAoEXgBAAAAAFEi8AIAAAAAokTgBQAAAABEicALAAAAAIgSgRcAAAAAECUCLwAAAAAgSgReAAAAAECUCLwAAAAAgCgReAEAAAAAUSLwAgAAAACiROAFAAAAAESJwAsAAAAAiBKBFwAAAAAQJQIvAAAAACBKBF4AAAAAQJSGGnjN7BYze9LMjpvZ4XVer5rZp8Prj5jZ/sJrHwj1T5rZj1/uPQEAAAAAkIYYeM0slXSPpLdIOijpXWZ2cM1ld0k66+43SvqwpA+F9x6UdLukmyTdIunPzSy9zHsCAAAAADDUHt7XSzru7k+5e0vS/ZJuW3PNbZLuC+efkfQmM7NQf7+7N939fyQdD/e7nHsCAAAAADDUwLtX0tOF8jOhbt1r3L0j6byk+Uu893LuCQAAAACASqNuwLCY2d2S7g7Fppl9c5TtAYZgp6RTo24EsMl4rhEjnmvEimcbW8kr1qscZuB9VtJ1hfK+ULfeNc+YWUnSNkmnX+C9L3RPSZK73yvpXkkys6PufujqPgawNfFcI0Y814gRzzVixbONcTDMIc2PSjpgZjeYWUX5IlRH1lxzRNKd4fwdkh5ydw/1t4dVnG+QdEDSly/zngAAAAAADK+H1907ZvZeSV+QlEr6hLsfM7MPSjrq7kckfVzSX5nZcUlnlAdYhesekPSEpI6k97h7V5LWu+ewPgMAAAAAYHxZ3qEaNzO7OwxxBqLBc40Y8VwjRjzXiBXPNsbBNRF4AQAAAADXnmHO4QUAAAAAYGSiDrxmdouZPWlmx83s8KjbA1wtM/uEmS0Ut9cysx1m9kUz+3Y4zo2yjcCVMrPrzOxhM3vCzI6Z2ftCPc82xpaZ1czsy2b2tfBc/36ov8HMHgl/k3w6LL4JjBUzS83sq2b2j6HMc40tL9rAa2appHskvUXSQUnvMrODo20VcNX+UtIta+oOS3rQ3Q9IejCUgXHSkfRr7n5Q0hskvSf8f5pnG+OsKelmd3+NpNdKusXM3iDpQ5I+7O43Sjor6a4RthG4Wu+T9K1CmecaW160gVfS6yUdd/en3L0l6X5Jt424TcBVcfd/U76SedFtku4L5/dJ+qmXtFHAi+Tuz7n7V8J5XfkfUXvFs40x5rmlUCyHH5d0s6TPhHqea4wdM9sn6a2S/iKUTTzXGAMxB969kp4ulJ8JdUAs9rj7c+H8eUl7RtkY4MUws/2SXifpEfFsY8yFYZ+PS1qQ9EVJ35F0zt074RL+JsE4+oik35SUhfK8eK4xBmIOvMA1w/Pl1llyHWPJzKYl/Z2kX3H3xeJrPNsYR+7edffXStqnfMTZD4y4ScCLYma3Slpw98dG3RbgSpVG3YAhelbSdYXyvlAHxOKEmb3c3Z8zs5cr70kAxoqZlZWH3U+5+9+Hap5tRMHdz5nZw5J+SNJ2MyuF3jD+JsG4+RFJbzOzn5BUkzQr6U/Ec40xEHMP76OSDoTV4yqSbpd0ZMRtAjbTEUl3hvM7Jf3DCNsCXLEw/+vjkr7l7n9ceIlnG2PLzHaZ2fZwPiHpzcrnpz8s6R3hMp5rjBV3/4C773P3/cr/pn7I3e8QzzXGgOWjxeIUvoX6iKRU0ifc/Q9H3CTgqpjZ30p6o6Sdkk5I+l1Jn5P0gKTrJf2vpJ9297ULWwFblpn9qKQvSfqGVueE/Zbyebw82xhLZvZq5Yv3pMo7Fh5w9w+a2fcrX0Bzh6SvSvoZd2+OrqXA1TGzN0r6dXe/leca4yDqwAsAAAAAuHbFPKQZAAAAAHANI/ACAAAAAKJE4AUAAAAARInACwAAAACIEoEXAAAAABAlAi8AAFuImXXN7PHCz+FNvPd+M/vmZt0PAICtrjTqBgAAgAEX3P21o24EAAAxoIcXAIAxYGbfNbM/MrNvmNmXzezGUL/fzB4ys6+b2YNmdn2o32NmnzWzr4WfHw63Ss3sY2Z2zMz+2cwmwvW/bGZPhPvcP6KPCQDApiLwAgCwtUysGdL8zsJr5939VZL+TNJHQt2fSrrP3V8t6VOSPhrqPyrpX939NZJ+UNKxUH9A0j3ufpOkc5LeHuoPS3pduM8vDuvDAQDwUjJ3H3UbAABAYGZL7j69Tv13Jd3s7k+ZWVnS8+4+b2anJL3c3duh/jl332lmJyXtc/dm4R77JX3R3Q+E8vslld39D8zs85KWJH1O0ufcfWnIHxUAgKGjhxcAgPHhG5xfiWbhvKvV9TzeKuke5b3Bj5oZ63wAAMYegRcAgPHxzsLxv8L5f0q6PZzfIelL4fxBSe+WJDNLzWzbRjc1s0TSde7+sKT3S9om6aJeZgAAxg3f3gIAsLVMmNnjhfLn3b23NdGcmX1deS/tu0LdL0n6pJn9hqSTkn4+1L9P0r1mdpfyntx3S3pug/9mKumvQyg2SR9193Ob9okAABgR5vACADAGwhzeQ+5+atRtAQBgXDCkGQAAAAAQJXp4AQAAAABRoocXAAAAABAlAi8AAAAAIEoEXgAAAABAlAi8AAAAAIAoEXgBAAAAAFEi8AIAAAAAovT/3oMXGcmViswAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 1152x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lx9wDErBh6Qr",
        "colab_type": "text"
      },
      "source": [
        "### Explaination"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpV6vXxP5iYy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}